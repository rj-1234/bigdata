{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Part 2: Transformations and Loads\n",
    "\n",
    "In this course, Data Engineers apply data transformation and extraction best practices such as user-defined functions, efficient table joins, and parallel database writes.  \n",
    "By the end of this course, you will transform complex data with custom functions, load it into a target database, and navigate Spark documents to source solutions.\n",
    "\n",
    "**The part-2 course includes the following lessons:**\n",
    "1. Course Overview and Setup\n",
    "1. Common Transformations\n",
    "1. User Defined Functions\n",
    "1. Advanced UDFs\n",
    "1. Joins and Lookup Tables\n",
    "1. Database Writes\n",
    "1. Table Management\n",
    "1. Capstone Project: Custom Transformations, Aggregating and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw, Query, and Summary Tables\n",
    "\n",
    "A number of different terms describe the movement of data through an ETL pipeline. For course purposes, data begins in the pipeline with **raw tables.** This refers to data that arrives in the pipeline, conforms to a schema, and does not include any sort of parsing or aggregation.\n",
    "\n",
    "Raw tables are then parsed into query-ready tables, known as **query tables.**  Query tables might populate a relational data model for ad hoc (OLAP) or online (OLTP) queries, but generally do not include any sort of aggregation such as daily averages.  Put another way, query tables are cleaned and filtered data.\n",
    "\n",
    "Finally, **summary tables** are business level aggregates often used for reporting and dashboarding. This includes aggregations such as daily active website visitors.\n",
    "\n",
    "It is a good idea to preserve raw tables because it lets you adapt to future needs not considered in the original data model or correct parsing errors. This pipeline ensures data engineers always have access to the data they need, while reducing the barriers to common data access patterns. Data becomes more valuable and easier to access in each subsequent stage of the pipeline.  \n",
    "\n",
    "<div><img src=\"../../resources/gold-silver-bronze.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Transformations\n",
    "\n",
    "Apache Spark&trade; allow you to manipulate data with built-in functions that accommodate common design patterns.\n",
    "\n",
    "## In this lesson you:\n",
    "* Apply built-in functions to manipulate data\n",
    "* Define logic to handle null values\n",
    "* Deduplicate a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations in ETL\n",
    "\n",
    "The goal of transformations in ETL is to transform raw data in order to populate a data model.  The most common models are **relational models** and **snowflake (or star) schemas,** though other models such as query-first modeling also exist. Relational modeling entails distilling your data into efficient tables that you can join back together. A snowflake model is generally used in data warehousing where a fact table references any number of related dimension tables. Regardless of the model you use, the ETL approach is generally the same.\n",
    "\n",
    "Transforming data can range in complexity from simply parsing relevant fields to handling null values without affecting downstream operations and applying complex conditional logic.  Common transformations include:<br><br>\n",
    "\n",
    "* Normalizing values\n",
    "* Imputing null or missing data\n",
    "* Deduplicating data\n",
    "* Performing database rollups\n",
    "* Exploding arrays\n",
    "* Pivoting DataFrames\n",
    "\n",
    "<div><img src=\"../../resources/data-models.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In Functions\n",
    "\n",
    "Built-in functions offer a range of performant options to manipulate data. This includes options familiar to:<br><br>\n",
    "\n",
    "1. SQL users such as `.select()` and `.groupBy()`\n",
    "2. Python, Scala and R users such as `max()` and `sum()`\n",
    "3. Data warehousing options such as `rollup()` and `cube()`\n",
    "\n",
    "**Hint:** For more depth on built-in functions, see  <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\" target=\"_blank\">Spark Built-in functions API doc</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_06-transformations-and-loads-intro\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_06-transformations-and-loads-intro\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data\n",
    "\n",
    "Normalizing refers to different practices including restructuring data in normal form to reduce redundancy, and scaling data down to a small, specified range. For this case, bound a range of integers between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by taking a DataFrame of a range of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integerDF = spark.range(1000, 10000)\n",
    "\n",
    "display(integerDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** To normalize these values between 0 and 1, subtract the minimum and divide by the maximum, minus the minimum.\n",
    "\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=minmaxscaler#pyspark.ml.feature.MinMaxScaler\" target=\"_blank\">Also see the built-in class `MinMaxScaler`</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max, min\n",
    "\n",
    "colMin = integerDF.select(min(\"id\")).first()[0]\n",
    "colMax = integerDF.select(max(\"id\")).first()[0]\n",
    "\n",
    "normalizedIntegerDF = (integerDF\n",
    "  .withColumn(\"normalizedValue\", (col(\"id\") - colMin) / (colMax - colMin) )\n",
    ")\n",
    "\n",
    "display(normalizedIntegerDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Null or Missing Data\n",
    "\n",
    "Null values refer to unknown or missing data as well as irrelevant responses. Strategies for dealing with this scenario include:<br><br>\n",
    "\n",
    "* **Dropping these records:** Works when you do not need to use the information for downstream workloads\n",
    "* **Adding a placeholder (e.g. `-1`):** Allows you to see missing data later on without violating a schema\n",
    "* **Basic imputing:** Allows you to have a \"best guess\" of what the data could have been, often by using the mean of non-missing data\n",
    "* **Advanced imputing:** Determines the \"best guess\" of what data should be using more advanced strategies such as clustering machine learning algorithms or oversampling techniques \n",
    "\n",
    "**Hint:** <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=imputer#pyspark.ml.feature.Imputer\" target=\"_blank\">Also see the built-in class `Imputer`</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the following DataFrame, which has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptDF = spark.createDataFrame([\n",
    "  (11, 66, 5),\n",
    "  (12, 68, None),\n",
    "  (1, None, 6),\n",
    "  (2, 72, 7)], \n",
    "  [\"hour\", \"temperature\", \"wind\"]\n",
    ")\n",
    "\n",
    "display(corruptDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any records that have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptDroppedDF = corruptDF.dropna(\"any\")\n",
    "\n",
    "display(corruptDroppedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute values with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptImputedDF = corruptDF.na.fill({\"temperature\": 68, \"wind\": 6})\n",
    "\n",
    "display(corruptImputedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicating Data\n",
    "\n",
    "Duplicate data comes in many forms. The simple case involves records that are complete duplicates of another record. The more complex cases involve duplicates that are not complete matches, such as matches on one or two columns or \"fuzzy\" matches that account for formatting differences or other non-exact matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the following DataFrame that has duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateDF = spark.createDataFrame([\n",
    "  (15342, \"Conor\", \"red\"),\n",
    "  (15342, \"conor\", \"red\"),\n",
    "  (12512, \"Dorothy\", \"blue\"),\n",
    "  (5234, \"Doug\", \"aqua\")], \n",
    "  [\"id\", \"name\", \"favorite_color\"]\n",
    ")\n",
    "\n",
    "display(duplicateDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates on `id` and `favorite_color`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateDedupedDF = duplicateDF.dropDuplicates([\"id\", \"favorite_color\"])\n",
    "\n",
    "display(duplicateDedupedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Helpful Data Manipulation Functions\n",
    "\n",
    "| Function    | Use                                                                                                                        |\n",
    "|:------------|:---------------------------------------------------------------------------------------------------------------------------|\n",
    "| `explode()` | Returns a new row for each element in the given array or map                                                               |\n",
    "| `pivot()`   | Pivots a column of the current DataFrame and perform the specified aggregation                                             |\n",
    "| `cube()`    | Create a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them   |\n",
    "| `rollup()`  | Create a multi-dimensional rollup for the current DataFrame using the specified columns, so we can run aggregation on them |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Deduplicating Data\n",
    "\n",
    "A common ETL workload involves cleaning duplicated records that don't completely match up.  The source of the problem can be anything from user-generated content to schema evolution and data corruption.  Here, you match records and reduce duplicate records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import and Examine the Data\n",
    "\n",
    "The file is sitting in `s3://data.intellinum.co/bootcamp/common/dataframes/people-with-dups.txt`.\n",
    "\n",
    "**Hint:** You have to deal with the header and delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "cols = set(dupedDF.columns)\n",
    "\n",
    "dfTest(\"ET2-P-02-01-01\", 103000, dupedDF.count())\n",
    "dfTest(\"ET2-P-02-01-02\", True, \"salary\" in cols and \"lastName\" in cols)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add Columns to Filter Duplicates\n",
    "\n",
    "Add columns following to allow you to filter duplicate values.  Add the following:\n",
    "\n",
    "- `lcFirstName`: first name lower case\n",
    "- `lcLastName`: last name lower case\n",
    "- `lcMiddleName`: middle name lower case\n",
    "- `ssnNums`: social security number without hyphens between numbers\n",
    "\n",
    "Save the results to `dupedWithColsDF`.\n",
    "\n",
    "**Hint:** Use the Spark function `lower()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "cols = set(dupedWithColsDF.columns)\n",
    "\n",
    "dfTest(\"ET2-P-02-02-01\", 103000, dupedWithColsDF.count())\n",
    "dfTest(\"ET2-P-02-02-02\", True, \"lcFirstName\" in cols and \"lcLastName\" in cols)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deduplicate the Data\n",
    "\n",
    "Deduplicate the data by dropping duplicates of all records except for the original names (first, middle, and last) and the original `ssn`.  Save the result to `dedupedDF`.  Drop the columns you added in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "cols = set(dedupedDF.columns)\n",
    "\n",
    "dfTest(\"ET2-P-02-03-01\", 100000, dedupedDF.count())\n",
    "dfTest(\"ET2-P-02-03-02\", 7, len(cols))\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What built-in functions are available in Spark?  \n",
    "**Answer:** Built-in functions include SQL functions, common programming language primitives, and data warehousing specific functions.  See the Spark API Docs for more details. (<a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">Python</a> or <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\" target=\"_blank\">Scala</a>).\n",
    "\n",
    "**Question:** What's the best way to handle null values?  \n",
    "**Answer:** The answer depends largely on what you hope to do with your data moving forward. You can drop null values or impute them with a number of different techniques.  For instance, clustering your data to fill null values with the values of nearby neighbors often gives more insight to machine learning models than using a simple mean.\n",
    "\n",
    "**Question:** What are potential challenges of deduplicating data and imputing null values?  \n",
    "**Answer:** Challenges include knowing which is the correct record to keep and how to define logic that applies to the root cause of your situation. This decision making process depends largely on how removing or imputing data will affect downstream operations like database queries and machine learning workloads. Knowing the end application of the data helps determine the best strategy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
