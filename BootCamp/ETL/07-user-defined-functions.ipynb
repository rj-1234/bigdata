{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions\n",
    "\n",
    "Apache Spark&trade; allows you to create your own User Defined Functions (UDFs) specific to the needs of your data.\n",
    "\n",
    "## In this lesson you:\n",
    "* Write UDFs with a single DataFrame column inputs\n",
    "* Transform data using UDFs using both the DataFrame and SQL API\n",
    "* Analyze the performance trade-offs between built-in functions and UDFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformations with User Defined Functions\n",
    "\n",
    "Spark's built-in functions provide a wide array of functionality, covering the vast majority of data transformation use cases. Often what differentiates strong Spark programmers is their ability to utilize built-in functions since Spark offers many highly optimized options to manipulate data. This matters for two reasons:<br><br>\n",
    "\n",
    "- First, *built-in functions are finely tuned* so they run faster than less efficient code provided by the user.  \n",
    "- Secondly, Spark (or, more specifically, Spark's optimization engine, the Catalyst Optimizer) knows the objective of built-in functions so it can *optimize the execution of your code by changing the order of your tasks.* \n",
    "\n",
    "In brief, use built-in functions whenever possible.\n",
    "\n",
    "There are, however, many specific use cases not covered by built-in functions. **User Defined Functions (UDFs) are useful when you need to define logic specific to your use case and when you need to encapsulate that solution for reuse.** They should only be used when there is no clear way to accomplish a task using built-in functions.\n",
    "\n",
    "UDFs are generally more performant in Scala than Python since for Python, Spark has to spin up a Python interpreter on every executor to run the function. This causes a substantial performance bottleneck due to communication across the Py4J bridge (how the JVM inter-operates with Python) and the slower nature of Python execution.\n",
    "\n",
    "<div><img src=\"../../resources/built-in-vs-udfs.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_07-udf\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_07-udf\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Basic UDF\n",
    "\n",
    "UDFs take a function or lambda and make it available for Spark to use.  Start by writing code in your language of choice that will operate on a single row of a single column in your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a basic function that splits a string on an `e`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is my ', 'xampl', ' string']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_split(x):\n",
    "    return x.split(\"e\")\n",
    "\n",
    "manual_split(\"this is my example string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the function as a UDF by designating the following:\n",
    "\n",
    "* A name for access in Python (`manualSplitPythonUDF`)\n",
    "* A name for access in SQL (`manualSplitSQLUDF`)\n",
    "* The function itself (`manual_split`)\n",
    "* The return type for the function (`StringType`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "manualSplitPythonUDF = spark.udf.register(\"manualSplitSQLUDF\", manual_split, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame of 100k values with a string to index. Do this by using a hash function, in this case `SHA-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6f65a37773e1a173aefe023667fe23442efaf0e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3c5f154011aa08151d2cc5b458a48b33ab656c67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>28fc878d1f1942bd718cefa60424d9e18e8756db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9c54ebb1c11c92d4ce7c2d52a15d21c337e0ced5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>46500976da0a25029a93419acd36935d06c86e95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9d8e41374ef6637e274e4b5487a6428b72b5fb2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>29d9fa515d557568abc85d08d1217b6b21c4841d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>672da7735c57d6707a8307bd18b1fd4dc04821ee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>e6fb1c43359cff2d70014e776bd47565bdbf40a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8e3f2b2dfae9a25aa4a940067e34bf4cebde5926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      hash\n",
       "0   1  6f65a37773e1a173aefe023667fe23442efaf0e5\n",
       "1   2  3c5f154011aa08151d2cc5b458a48b33ab656c67\n",
       "2   3  28fc878d1f1942bd718cefa60424d9e18e8756db\n",
       "3   4  9c54ebb1c11c92d4ce7c2d52a15d21c337e0ced5\n",
       "4   5  46500976da0a25029a93419acd36935d06c86e95\n",
       "5   6  9d8e41374ef6637e274e4b5487a6428b72b5fb2a\n",
       "6   7  29d9fa515d557568abc85d08d1217b6b21c4841d\n",
       "7   8  672da7735c57d6707a8307bd18b1fd4dc04821ee\n",
       "8   9  e6fb1c43359cff2d70014e776bd47565bdbf40a2\n",
       "9  10  8e3f2b2dfae9a25aa4a940067e34bf4cebde5926"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sha1, rand\n",
    "randomDF = (spark.range(1, 10000 * 10 * 10 * 10)\n",
    "  .withColumn(\"random_value\", rand(seed=10).cast(\"string\"))\n",
    "  .withColumn(\"hash\", sha1(\"random_value\"))\n",
    "  .drop(\"random_value\")\n",
    ")\n",
    "\n",
    "display(randomDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the UDF by using it just like any other Spark function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash</th>\n",
       "      <th>augmented_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6f65a37773e1a173aefe023667fe23442efaf0e5</td>\n",
       "      <td>[6f65a37773, 1a173a, f, 023667f, 23442, faf0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3c5f154011aa08151d2cc5b458a48b33ab656c67</td>\n",
       "      <td>[3c5f154011aa08151d2cc5b458a48b33ab656c67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>28fc878d1f1942bd718cefa60424d9e18e8756db</td>\n",
       "      <td>[28fc878d1f1942bd718c, fa60424d9, 18, 8756db]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9c54ebb1c11c92d4ce7c2d52a15d21c337e0ced5</td>\n",
       "      <td>[9c54, bb1c11c92d4c, 7c2d52a15d21c337, 0c, d5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>46500976da0a25029a93419acd36935d06c86e95</td>\n",
       "      <td>[46500976da0a25029a93419acd36935d06c86, 95]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9d8e41374ef6637e274e4b5487a6428b72b5fb2a</td>\n",
       "      <td>[9d8, 41374, f6637, 274, 4b5487a6428b72b5fb2a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>29d9fa515d557568abc85d08d1217b6b21c4841d</td>\n",
       "      <td>[29d9fa515d557568abc85d08d1217b6b21c4841d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>672da7735c57d6707a8307bd18b1fd4dc04821ee</td>\n",
       "      <td>[672da7735c57d6707a8307bd18b1fd4dc04821, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>e6fb1c43359cff2d70014e776bd47565bdbf40a2</td>\n",
       "      <td>[, 6fb1c43359cff2d70014, 776bd47565bdbf40a2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8e3f2b2dfae9a25aa4a940067e34bf4cebde5926</td>\n",
       "      <td>[8, 3f2b2dfa, 9a25aa4a940067, 34bf4c, bd, 5926]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      hash  \\\n",
       "0   1  6f65a37773e1a173aefe023667fe23442efaf0e5   \n",
       "1   2  3c5f154011aa08151d2cc5b458a48b33ab656c67   \n",
       "2   3  28fc878d1f1942bd718cefa60424d9e18e8756db   \n",
       "3   4  9c54ebb1c11c92d4ce7c2d52a15d21c337e0ced5   \n",
       "4   5  46500976da0a25029a93419acd36935d06c86e95   \n",
       "5   6  9d8e41374ef6637e274e4b5487a6428b72b5fb2a   \n",
       "6   7  29d9fa515d557568abc85d08d1217b6b21c4841d   \n",
       "7   8  672da7735c57d6707a8307bd18b1fd4dc04821ee   \n",
       "8   9  e6fb1c43359cff2d70014e776bd47565bdbf40a2   \n",
       "9  10  8e3f2b2dfae9a25aa4a940067e34bf4cebde5926   \n",
       "\n",
       "                                      augmented_col  \n",
       "0  [6f65a37773, 1a173a, f, 023667f, 23442, faf0, 5]  \n",
       "1        [3c5f154011aa08151d2cc5b458a48b33ab656c67]  \n",
       "2     [28fc878d1f1942bd718c, fa60424d9, 18, 8756db]  \n",
       "3    [9c54, bb1c11c92d4c, 7c2d52a15d21c337, 0c, d5]  \n",
       "4       [46500976da0a25029a93419acd36935d06c86, 95]  \n",
       "5    [9d8, 41374, f6637, 274, 4b5487a6428b72b5fb2a]  \n",
       "6        [29d9fa515d557568abc85d08d1217b6b21c4841d]  \n",
       "7      [672da7735c57d6707a8307bd18b1fd4dc04821, , ]  \n",
       "8      [, 6fb1c43359cff2d70014, 776bd47565bdbf40a2]  \n",
       "9   [8, 3f2b2dfa, 9a25aa4a940067, 34bf4c, bd, 5926]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomAugmentedDF = randomDF.select(\"*\", manualSplitPythonUDF(\"hash\").alias(\"augmented_col\"))\n",
    "\n",
    "display(randomAugmentedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame and SQL APIs\n",
    "\n",
    "When you registered the UDF, it was named `manualSplitSQLUDF` for access in the SQL API. This gives us the same access to the UDF you had in the python DataFrames API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register `randomDF` to access it within SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomDF.createOrReplaceTempView(\"randomTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now switch to the SQL API and use the same UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash</th>\n",
       "      <th>augmented_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6f65a37773e1a173aefe023667fe23442efaf0e5</td>\n",
       "      <td>[6f65a37773, 1a173a, f, 023667f, 23442, faf0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3c5f154011aa08151d2cc5b458a48b33ab656c67</td>\n",
       "      <td>[3c5f154011aa08151d2cc5b458a48b33ab656c67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>28fc878d1f1942bd718cefa60424d9e18e8756db</td>\n",
       "      <td>[28fc878d1f1942bd718c, fa60424d9, 18, 8756db]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9c54ebb1c11c92d4ce7c2d52a15d21c337e0ced5</td>\n",
       "      <td>[9c54, bb1c11c92d4c, 7c2d52a15d21c337, 0c, d5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>46500976da0a25029a93419acd36935d06c86e95</td>\n",
       "      <td>[46500976da0a25029a93419acd36935d06c86, 95]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9d8e41374ef6637e274e4b5487a6428b72b5fb2a</td>\n",
       "      <td>[9d8, 41374, f6637, 274, 4b5487a6428b72b5fb2a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>29d9fa515d557568abc85d08d1217b6b21c4841d</td>\n",
       "      <td>[29d9fa515d557568abc85d08d1217b6b21c4841d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>672da7735c57d6707a8307bd18b1fd4dc04821ee</td>\n",
       "      <td>[672da7735c57d6707a8307bd18b1fd4dc04821, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>e6fb1c43359cff2d70014e776bd47565bdbf40a2</td>\n",
       "      <td>[, 6fb1c43359cff2d70014, 776bd47565bdbf40a2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8e3f2b2dfae9a25aa4a940067e34bf4cebde5926</td>\n",
       "      <td>[8, 3f2b2dfa, 9a25aa4a940067, 34bf4c, bd, 5926]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      hash  \\\n",
       "0   1  6f65a37773e1a173aefe023667fe23442efaf0e5   \n",
       "1   2  3c5f154011aa08151d2cc5b458a48b33ab656c67   \n",
       "2   3  28fc878d1f1942bd718cefa60424d9e18e8756db   \n",
       "3   4  9c54ebb1c11c92d4ce7c2d52a15d21c337e0ced5   \n",
       "4   5  46500976da0a25029a93419acd36935d06c86e95   \n",
       "5   6  9d8e41374ef6637e274e4b5487a6428b72b5fb2a   \n",
       "6   7  29d9fa515d557568abc85d08d1217b6b21c4841d   \n",
       "7   8  672da7735c57d6707a8307bd18b1fd4dc04821ee   \n",
       "8   9  e6fb1c43359cff2d70014e776bd47565bdbf40a2   \n",
       "9  10  8e3f2b2dfae9a25aa4a940067e34bf4cebde5926   \n",
       "\n",
       "                                      augmented_col  \n",
       "0  [6f65a37773, 1a173a, f, 023667f, 23442, faf0, 5]  \n",
       "1        [3c5f154011aa08151d2cc5b458a48b33ab656c67]  \n",
       "2     [28fc878d1f1942bd718c, fa60424d9, 18, 8756db]  \n",
       "3    [9c54, bb1c11c92d4c, 7c2d52a15d21c337, 0c, d5]  \n",
       "4       [46500976da0a25029a93419acd36935d06c86, 95]  \n",
       "5    [9d8, 41374, f6637, 274, 4b5487a6428b72b5fb2a]  \n",
       "6        [29d9fa515d557568abc85d08d1217b6b21c4841d]  \n",
       "7      [672da7735c57d6707a8307bd18b1fd4dc04821, , ]  \n",
       "8      [, 6fb1c43359cff2d70014, 776bd47565bdbf40a2]  \n",
       "9   [8, 3f2b2dfa, 9a25aa4a940067, 34bf4c, bd, 5926]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(\n",
    "    spark.sql(\"\"\"\n",
    "SELECT id,\n",
    "  hash,\n",
    "  manualSplitSQLUDF(hash) as augmented_col\n",
    "FROM\n",
    "  randomTable\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an easy way to generalize UDFs, allowing teams to share their code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Trade-offs\n",
    "\n",
    "The performance of custom UDFs normally trail far behind built-in functions.  Take a look at this other example to compare built-in functions to custom UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a large DataFrame of random values, cache the result in order to keep the DataFrame in memory, and perform a `.count()` to trigger the cache to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>random_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.192791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.875940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.151727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.135781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.635407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.311731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.301832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.743595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.099060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.258167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  random_float\n",
       "0   0      0.192791\n",
       "1   0      0.875940\n",
       "2   0      0.151727\n",
       "3   0      0.135781\n",
       "4   0      0.635407\n",
       "5   0      0.311731\n",
       "6   0      0.301832\n",
       "7   0      0.743595\n",
       "8   0      0.099060\n",
       "9   0      0.258167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "randomFloatsDF = (spark.range(0, 100 * 1000 * 1000)\n",
    "  .withColumn(\"id\", (col(\"id\") / 1000).cast(\"integer\"))\n",
    "  .withColumn(\"random_float\", rand())\n",
    ")\n",
    "\n",
    "randomFloatsDF.cache()\n",
    "randomFloatsDF.count()\n",
    "\n",
    "display(randomFloatsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register a new UDF that increments a column by 1.  Here, use a lambda instead of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "  \n",
    "plusOneUDF = spark.udf.register(\"plusOneUDF\", lambda x: x + 1, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results using the `%timeit` function.  Run it a few times and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9 s ± 519 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit randomFloatsDF.withColumn(\"incremented_float\", plusOneUDF(\"random_float\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33 s ± 318 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit randomFloatsDF.withColumn(\"incremented_float\", col(\"random_float\") + 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which was faster, the UDF or the built-in functionality?  By how much?  This can differ based upon whether you work through this course in Scala (which is much faster) or Python. Please go ahead and try this on Zeppelin using scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Converting IP Addresses to Decimals\n",
    "\n",
    "Write a UDF that translates an IPv4 address string (e.g. `123.123.123.123`) into a numeric value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Function\n",
    "\n",
    "IP addresses pose challenges for efficient database lookups.  One way of dealing with this is to store an IP address in numerical form.  Write the function `IPConvert` that satisfies the following:\n",
    "\n",
    "Input: IP address as a string (e.g. `123.123.123.123`)  \n",
    "Output: an integer representation of the IP address (e.g. `2071690107`)\n",
    "\n",
    "If the input string is `1.2.3.4`, think of it like `A.B.C.D` where A is 1, B is 2, etc. Solve this with the following steps:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; (A x 256^3) + (B x 256^2) + (C x 256) + D <br>\n",
    "&nbsp;&nbsp;&nbsp; (1 x 256^3) + (2 x 256^2) + (3 x 256) + 4 <br>\n",
    "&nbsp;&nbsp;&nbsp; 116777216 + 131072 + 768 + 4 <br>\n",
    "&nbsp;&nbsp;&nbsp; 16909060\n",
    "\n",
    "Make a function to implement this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "dfTest(\"ET2-P-03-01-01\", 16909060, IPConvert(\"1.2.3.4\"))\n",
    "dfTest(\"ET2-P-03-01-02\", 168430090, IPConvert(\"10.10.10.10\"))\n",
    "dfTest(\"ET2-P-03-01-03\", 386744599, IPConvert(\"23.13.65.23\"))\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Register a UDF\n",
    "\n",
    "Register your function as `IPConvertUDF`.  Be sure to use `LongType` as your output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "testDF = spark.createDataFrame((\n",
    "  (\"1.2.3.4\", ),\n",
    "  (\"10.10.10.10\", ),\n",
    "  (\"23.13.65.23\", )\n",
    "), (\"ip\",))\n",
    "result = [i[0] for i in testDF.select(IPConvertUDF(\"ip\")).collect()]\n",
    "\n",
    "dfTest(\"ET2-P-03-02-01\", 16909060, result[0])\n",
    "dfTest(\"ET2-P-03-02-02\", 168430090, result[1])\n",
    "dfTest(\"ET2-P-03-02-03\", 386744599, result[2])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply the UDF\n",
    "\n",
    "Apply the UDF on the `IP` column of the DataFrame created below, creating the new column `parsedIP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "result2 = [i[1] for i in IPDFWithParsedIP.collect()]\n",
    "\n",
    "dfTest(\"ET2-P-03-03-01\", 2071690107, result2[0])\n",
    "dfTest(\"ET2-P-03-03-02\", 16909060, result2[1])\n",
    "dfTest(\"ET2-P-03-03-03\", 2130706432, result2[2])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What are the performance trade-offs between UDFs and built-in functions?  When should I use each?  \n",
    "**Answer:** Built-in functions are normally faster than UDFs and should be used when possible.  UDFs should be used when specific use cases arise that aren't addressed by built-in functions.\n",
    "\n",
    "**Question:** How can I use UDFs?  \n",
    "**Answer:** UDFs can be used in any Spark API. They can be registered for use in SQL and can otherwise be used in Scala, Python, R, and Java.\n",
    "\n",
    "**Question:** Why are built-in functions faster?  \n",
    "**Answer:** Reasons include:\n",
    "* The catalyst optimizer knows how to optimize built-in functions\n",
    "* They are written in highly optimized Scala\n",
    "* There is no serialization cost at the time of running a built-in function\n",
    "\n",
    "**Question:** Can UDFs have multiple column inputs and outputs?  \n",
    "**Answer:** Yes, UDFs can have multiple column inputs and multiple complex outputs. This is covered in the following lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
