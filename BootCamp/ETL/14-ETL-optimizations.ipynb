{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Optimizations\n",
    "\n",
    "Apache Spark&trade; clusters can be optimized using compression and caching best practices along with autoscaling clusters.\n",
    "\n",
    "## In this lesson you:\n",
    "* Compare trade-offs in compression algorithms in terms of parallelization, data transfer, file types, and read vs write time\n",
    "* Cache data at the optimal point in a workload to limit data transfer across the network\n",
    "* Configure clusters based on the demands of your workload including size, location, and types of machines\n",
    "* Employ an autoscaling strategy to dynamically adapt to changes in data size\n",
    "* Monitor clusters using the Ganglia UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing ETL Workloads\n",
    "\n",
    "Optimizing Spark jobs boils down to a few common themes, many of which were addressed in other lessons in this course.  Additional optimizations include compression, caching, and hardware choices.\n",
    "\n",
    "Many aspects of computer science are about trade-offs rather than a single, optimal solution.  For instance, data compression is a trade-off between decompression speed, splitability, and compressed to uncompressed data size.  Hardware choice is a trade-off between CPU-bound and IO-bound workloads.  This lesson explores the trade-offs between these issues with rules of thumb to apply them to any Spark workload.\n",
    "\n",
    "While a number of compression algorithms exist, the basic intuition behind them is that they reduce space by reducing redundancies in the data.  Compression matters in big data environments because many jobs are IO bound (or the bottleneck is data transfer) rather than CPU bound.  Compressing data allows for the reduction of data transfer as well as reduction of storage space needed to store that data.\n",
    "\n",
    "<div><img src=\"../../resources/data-compression.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "import uuid\n",
    "import time\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_14-ETL-optimizations\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_14-ETL-optimizations\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','io.delta:delta-core_2.11:0.2.0,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.2,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.2,net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    try:\n",
    "        # For spark-core \n",
    "        result = df.limit(limit).toPandas()\n",
    "    except Exception as e:\n",
    "        # For structured-streaming\n",
    "        stream_name = str(uuid.uuid1()).replace(\"-\",\"\")\n",
    "        query = (\n",
    "          df\n",
    "            .writeStream\n",
    "            .format(\"memory\")        # memory = store in-memory table (for debugging only)\n",
    "            .queryName(stream_name) # show = name of the in-memory table\n",
    "            .trigger(processingTime='1 seconds') #Trigger = 1 second\n",
    "            .outputMode(\"append\")  # append\n",
    "            .start()\n",
    "        )\n",
    "        while query.isActive:\n",
    "            time.sleep(1)\n",
    "            result = spark.sql(f\"select * from {stream_name} limit {limit}\").toPandas()\n",
    "            print(\"Wait until the stream is ready...\")\n",
    "            if result.empty == False:\n",
    "                break\n",
    "        result = spark.sql(f\"select * from {stream_name} limit {limit}\").toPandas()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def untilStreamIsReady(name):\n",
    "    queries = list(filter(lambda query: query.name == name, spark.streams.active))\n",
    "\n",
    "    if len(queries) == 0:\n",
    "        print(\"The stream is not active.\")\n",
    "\n",
    "    else:\n",
    "        while (queries[0].isActive and len(queries[0].recentProgress) == 0):\n",
    "            pass # wait until there is any type of progress\n",
    "\n",
    "        if queries[0].isActive:\n",
    "            queries[0].awaitTermination(5)\n",
    "            print(\"The stream is active and ready.\")\n",
    "        else:\n",
    "            print(\"The stream is not active.\")\n",
    "            \n",
    "            \n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression Best Practices\n",
    "\n",
    "There are three compression algorithms commonly used in Spark environments: GZIP, Snappy, and bzip2.  Choosing between this option is a trade-off between the compression ratio, the CPU usage needed to compress and decompress the data, and whether the data it saves is splittable and therefore able to be read and written in parallel.\n",
    "\n",
    "|                   | GZIP   | Snappy | bzip2 |\n",
    "|:------------------|:-------|:-------|:------|\n",
    "| Compression ratio | high   | medium | high  |\n",
    "| CPU usage         | medium | low    | high  |\n",
    "| Splittable        | no     | yes    | no    |\n",
    "\n",
    "While GZIP offers the highest compression ratio, it is not splittable and takes longer to encode and decode.  GZIP files can only use a single core to process.  Snappy offers less compression but is splittable and quick to encode and decode.  bzip offers a high compression ratio but at high CPU costs, only making it the preferred choice if storage space and/or network transfer is extremely limited.\n",
    "\n",
    "Parquet already uses some compression though Snappy compression applied to Parquet can reduce a file to half its size while improving job performance.  Parquet, unlike other files formats, is splittable regardless of compression format due to the internal layout of the file.\n",
    "\n",
    "<a href=\"https://issues.apache.org/jira/browse/SPARK-14482\" target=\"_blank\">The default compression codec for Parquet is Snappy.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a file to test compression options on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecountsEnAllDF = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n",
    "\n",
    "display(pagecountsEnAllDF)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the file as parquet using no compression, snappy and GZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_FIRST_NAME = # FILL_IN\n",
    "userhome = f\"s3a://temp.intellinum.co/{YOUR_FIRST_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_FIRST_NAME = \"yuan\"\n",
    "userhome = f\"s3a://temp.intellinum.co/{YOUR_FIRST_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTemplate = userhome + \"/elt-optimizations/pageCounts{}.csv\"\n",
    "compressions = [\"Uncompressed\", \"Snappy\", \"GZIP\"]\n",
    "\n",
    "uncompressedPath, snappyPath, GZIPPath = [pathTemplate.format(i) for i in compressions]\n",
    "\n",
    "pagecountsEnAllDF.write.mode(\"OVERWRITE\").csv(uncompressedPath)\n",
    "pagecountsEnAllDF.write.mode(\"OVERWRITE\").csv(snappyPath, compression = \"snappy\")\n",
    "pagecountsEnAllDF.write.mode(\"OVERWRITE\").csv(GZIPPath, compression = \"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the size differences in bytes for the compression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compressionType in compressions:\n",
    "    metadata = !aws s3 ls --summarize --human-readable --recursive {pathTemplate.format(compressionType).replace('s3a','s3')}\n",
    "    size = metadata[-1]\n",
    "    print(\"{}:  \\t{}\".format(compressionType, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Caching data is one way to improve query performance.  Cached data is maintained on a cluster rather than forgotten at the end of a query.  Without caching, Spark reads the data from its source again after every action. \n",
    "\n",
    "There are a number of different storage levels with caching, which are variants on memory, disk, or a combination of the too.  By default, Spark's storage level is `MEMORY_AND_DISK`\n",
    "\n",
    "`cache()` is the most common way of caching data while `persist()` allows for setting the storage level.\n",
    "\n",
    "It's worth noting that caching should be done with care since data cached at the wrong time can lead to less performant clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a DataFrame to test caching performance trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecountsEnAllDF = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n",
    "\n",
    "display(pagecountsEnAllDF)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `%timeit` function to see the average time for counting an uncached DataFrame.  Recall that Spark will have to reread the data from its source each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pagecountsEnAllDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the DataFrame.  An action will materialize the cache (store it on the Spark cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pagecountsEnAllDF\n",
    "  .cache()         # Mark the DataFrame as cached\n",
    "  .count()         # Materialize the cache\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same operation on the cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pagecountsEnAllDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the change?  Now unpersist the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Configuration\n",
    "\n",
    "Choosing the optimal cluster for a given workload depends on a variety of factors.  Some general rules of thumb include:<br><br>\n",
    "\n",
    "* **Fewer, large instances** are better than more, smaller instances since it reduces network shuffle\n",
    "* With jobs that have varying data size, **autoscale the cluster** to elastically vary the size of the cluster\n",
    "* Price sensitive solutions can use **spot pricing resources** at first, falling back to on demand resources when spot prices are unavailable\n",
    "* Run a job with a small cluster to get an idea of the number of tasks, then choose a cluster whose **number of cores is a multiple of those tasks**\n",
    "* Production jobs should take place on **isolated, new clusters**\n",
    "* **Colocate** the cluster in the same region and availability zone as your data\n",
    "\n",
    "Available resources are generally compute, memory, or storage optimized.  Normally start with compute-optimized clusters and fall back to the others based on demands of the workload (e.g. storage optimization for analytics or memory optimized for iterative algorithms such as machine learning).\n",
    "\n",
    "When running a job, examine the CPU and network traffic using the Spark UI to confirm the demands of your job and adjust the cluster configuration accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Comparing Compression Algorithms\n",
    "\n",
    "Compare algorithms on compression ratio and time to compress for different file types and number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create and Cache a DataFrame for Analysis\n",
    "\n",
    "Create a DataFrame `pagecountsEnAllDF`, cache it, and perform a count to realize the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecountsEnAllDF = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/wikipedia/pagecounts/staging_parquet_en_only_clean/\")\n",
    "pagecountsEnAllDF.cache()\n",
    "pagecountsEnAllDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Examine the Functions for Comparing Writes and Reads\n",
    "\n",
    "Examine the following functions, defined for you, for comparing write and read times by file type, number of partitions, and compression type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def write_read_time(df, file_type, partitions=1, compression=None, outputPath=userhome + \"/bootcamp/comparisonTest\"):\n",
    "    '''\n",
    "    Prints write time and read time for a given DataFrame with given params\n",
    "    '''\n",
    "    start_time = time()\n",
    "    _df = df.repartition(partitions).write.mode(\"OVERWRITE\")\n",
    "    \n",
    "    if compression:\n",
    "        _df = _df.option(\"compression\", compression)\n",
    "    if file_type == \"csv\":\n",
    "        _df.csv(outputPath)\n",
    "    elif file_type == \"parquet\":\n",
    "        _df.parquet(outputPath)\n",
    "      \n",
    "    total_time = round(time() - start_time, 1)\n",
    "    print(\"Save time of {}s for\\tfile_type: {}\\tpartitions: {}\\tcompression: {}\".format(total_time, file_type, partitions, compression))\n",
    "    \n",
    "    start_time = time()\n",
    "    if file_type == \"csv\":\n",
    "        spark.read.csv(outputPath).count()\n",
    "    elif file_type == \"parquet\":\n",
    "        spark.read.parquet(outputPath).count()\n",
    "      \n",
    "    total_time = round(time() - start_time, 2)\n",
    "    print(\"\\tRead time of {}s\".format(total_time))\n",
    "  \n",
    "  \n",
    "def time_all(df, file_type_list=[\"csv\", \"parquet\"], partitions_list=[1, 16, 32, 64], compression_list=[None, \"gzip\", \"snappy\"]):\n",
    "    '''\n",
    "    Wrapper function for write_read_time() to gridsearch lists of file types, partitions, and compression types\n",
    "    '''\n",
    "    for file_type in file_type_list:\n",
    "        for partitions in partitions_list:\n",
    "            for compression in compression_list:\n",
    "                write_read_time(df, file_type, partitions, compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Examine the Output\n",
    "\n",
    "Apply `time_all()` to `pagecountsEnAllDF` and examine the results.  Why do you see these changes across different file types, partition numbers, and compression algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_all(pagecountsEnAllDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** Why does compression matter when storage space is so inexpensive?  \n",
    "**Answer:** Compression matters in big data environments largely due to the IO bound nature of workloads.  Compression allows for less data transfer across the network, speeding up tasks significantly.  It can also reduce storage costs substantially as data size grows.\n",
    "\n",
    "**Question:** What should or shouldn't be compressed?  \n",
    "**Answer:** One best practice would be to compress all files using snappy, which balances compute and compression ratio trade-offs nicely.  Compression depends largely on the type of data being compressed so a text file type like CSV will likely compress significantly more than a parquet file of integers, for example, since parquet will store them as binary by default.\n",
    "\n",
    "**Question:** When should I cache my data?  \n",
    "**Answer:** Caching should take place with any iterative workload where you read data multiple times.  Reexamining when you cache your data often leads to performance improvements since poor caching can have negative downstream effects.\n",
    "\n",
    "**Question:** What kind of cluster should I use?  \n",
    "**Answer:** Choosing a cluster depends largely on workload but a good rule of thumb is to use larger and fewer compute-optimized machines at first.  You can then tune the cluster size and type depending on the workload.  Autoscaling also allows for dynamic resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT Next Steps\n",
    "* Please complete the <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd5whqoFBjNEEMvgwW5KRr-PeMyv6Lsczxk1p0es9s3IigEYQ/viewform?vc=0&c=0&w=1\" target=\"_blank\">short feedback survey</a>.  Your input is extremely important and shapes future course development.\n",
    "* Congratulations, you have completed ETL Part 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
