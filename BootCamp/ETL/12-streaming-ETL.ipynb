{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming ETL\n",
    "\n",
    "Apache Spark&trade; makes it easy to build scalable and fault-tolerant streaming ETL applications.\n",
    "\n",
    "## In this lesson you:\n",
    "* Define logic to read from a stream of data\n",
    "* Perform a basic ETL job on a streaming data source\n",
    "* Join a stream to historical data\n",
    "* Write to an always up-to-date Delta table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40587)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40587)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:40587)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-95289cf195c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PYSPARK_PYTHON\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./MN/pyspark24_py36/bin/python\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyspark_etl_12-streaming-etl-rj\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yarn-client'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/pyspark/conf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:40587)"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "import uuid\n",
    "import time\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_12-streaming-etl\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_12-streaming-etl-rj\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','io.delta:delta-core_2.11:0.2.0,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.2,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.2,net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    try:\n",
    "        # For spark-core \n",
    "        result = df.limit(limit).toPandas()\n",
    "    except Exception as e:\n",
    "        # For structured-streaming\n",
    "        stream_name = str(uuid.uuid1()).replace(\"-\",\"\")\n",
    "        query = (\n",
    "          df\n",
    "            .writeStream\n",
    "            .format(\"memory\")        # memory = store in-memory table (for debugging only)\n",
    "            .queryName(stream_name) # show = name of the in-memory table\n",
    "            .trigger(processingTime='1 seconds') #Trigger = 1 second\n",
    "            .outputMode(\"append\")  # append\n",
    "            .start()\n",
    "        )\n",
    "        while query.isActive:\n",
    "            time.sleep(1)\n",
    "            result = spark.sql(f\"select * from {stream_name} limit {limit}\").toPandas()\n",
    "            print(\"Wait until the stream is ready...\")\n",
    "            if result.empty == False:\n",
    "                break\n",
    "        result = spark.sql(f\"select * from {stream_name} limit {limit}\").toPandas()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def untilStreamIsReady(name):\n",
    "    queries = list(filter(lambda query: query.name == name, spark.streams.active))\n",
    "\n",
    "    if len(queries) == 0:\n",
    "        print(\"The stream is not active.\")\n",
    "\n",
    "    else:\n",
    "        while (queries[0].isActive and len(queries[0].recentProgress) == 0):\n",
    "            pass # wait until there is any type of progress\n",
    "\n",
    "        if queries[0].isActive:\n",
    "            queries[0].awaitTermination(5)\n",
    "            print(\"The stream is active and ready.\")\n",
    "        else:\n",
    "            print(\"The stream is not active.\")\n",
    "            \n",
    "            \n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL on Streaming Data\n",
    "\n",
    "Spark Streaming enables scalable and fault-tolerant ETL operations that continuously clean and aggregate data before pushing it to data stores.  Streaming applications can also incorporate machine learning and other Spark features to trigger actions in real time, such as flagging potentially fraudulent user activity.  This lesson is meant as an introduction to streaming applications as they pertain to production ETL jobs.  \n",
    "\n",
    "Streaming poses a number of specific obstacles. These obstacles include:<br><br>\n",
    "\n",
    "* *End-to-end reliability and correctness:* Applications must be resilient to failures of any element of the pipeline caused by network issues, traffic spikes, and/or hardware malfunctions\n",
    "* *Handle complex transformations:* applications receive many data formats that often involve complex business logic\n",
    "* *Late and out-of-order data:* network issues can result in data that arrives late and out of its intended order\n",
    "* *Integrate with other systems:* Applications must integrate with the rest of a data infrastructure\n",
    "\n",
    "Streaming data sources in Spark offer the same DataFrames API for interacting with your data.  The crucial difference is that in structured streaming, the DataFrame is unbounded.  In other words, data arrives in an input stream and new records are appended to the input DataFrame.\n",
    "\n",
    "<div><img src=\"../../resources/structured-streamining-model.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Stream\n",
    "\n",
    "As data technology matures, the industry has been converging on a set of technologies.  Apache Kafka and the AWS managed alternative Kinesis has become the ingestion engine at the heart of many pipelines.  \n",
    "\n",
    "This technology brokers messages between producers, such as an IoT device writing data, and consumers, such as a Spark cluster reading data to perform real time analytics. There can be a many-to-many relationship between producers and consumers and the broker itself is scalable and fault tolerant.\n",
    "\n",
    "Connect to a Kafka topic that is streaming Wikipedia event data.\n",
    "\n",
    "There are a number of ways to stream data.  One other common design pattern is to stream from an S3 bucket where any new files that appear will be read by the stream.  In this example, we'll be streaming directly from Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by defining the schema of the data in the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, IntegerType, StructType, StringType, StructField, TimestampType\n",
    "\n",
    "schema = (StructType()\n",
    "  .add(\"timestamp\", TimestampType())\n",
    "  .add(\"url\", StringType())\n",
    "  .add(\"userURL\", StringType())\n",
    "  .add(\"pageURL\", StringType())\n",
    "  .add(\"isNewPage\", BooleanType())\n",
    "  .add(\"geocoding\", StructType()\n",
    "    .add(\"countryCode2\", StringType())\n",
    "    .add(\"city\", StringType())\n",
    "    .add(\"latitude\", StringType())\n",
    "    .add(\"country\", StringType())\n",
    "    .add(\"longitude\", StringType())\n",
    "    .add(\"stateProvince\", StringType())\n",
    "    .add(\"countryCode3\", StringType())\n",
    "    .add(\"user\", StringType())\n",
    "    .add(\"namespace\", StringType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read from a stream, use `spark.readStream()`, which returns a `DataStreamReader` class.  Then, configure the stream by adding the following options:<br><br>\n",
    "\n",
    "* The server endpoint: `54.213.33.240:9092`\n",
    "* The topic to subscribe to: `en`\n",
    "* A location to log checkpoint metadata (more on this <a href='#Batch-and-Streaming-Writes'>below</a>)\n",
    "* The format: `kafka` \n",
    "\n",
    "Finally, use the load method, which loads the data stream from the Kafka source and returns it as an unbounded `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaDF = (spark\n",
    "  .readStream\n",
    "  .option(\"kafka.bootstrap.servers\", \"54.213.33.240:9092\")\n",
    "  .option(\"subscribe\", \"en\")\n",
    "  .option(\"startingOffsets\", \"latest\")\n",
    "  .format(\"kafka\")\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka transmits information using a key, value, and metadata such as topic and partition.  The information we're interested in is the `value` column.  Since this is a binary value, we must first cast it to a `StringType`.  We must also provide it with a schema.  Finally, we can expand the full structure of the JSON.\n",
    "\n",
    "\n",
    "Wait until the stream finishes initializing.  \n",
    "\n",
    "Often streams are started with the `.start()` method.  In this example, `display()`, provided in Bootcamp environments, is running that command for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "kafkaCleanDF = (kafkaDF\n",
    "  .select(from_json(col(\"value\").cast(StringType()), schema).alias(\"message\"))\n",
    "  .select(\"message.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['49911c0a999d11e9b8910a370dd9e3a4', 'test_query']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>userURL</th>\n",
       "      <th>pageURL</th>\n",
       "      <th>isNewPage</th>\n",
       "      <th>geocoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-28 13:23:49.688</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:Whispyhistory</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mike_Pringle_(doc...</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-28 13:23:49.748</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:2001:8004:15...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/The_Entire_Histor...</td>\n",
       "      <td>False</td>\n",
       "      <td>(AU, None, -25.0, Australia, -25.0, None, AUS,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-28 13:23:49.807</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:Toad02</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Oceana_(non-profi...</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-28 13:23:49.866</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:Kvng</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Bull_Pen_Pep_Band</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-28 13:23:49.928</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:S.A. Julio</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2011���12_Gimn��s...</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-28 13:23:49.987</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:Twofingered ...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User_talk:Nehme1499</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-06-28 13:23:50.678</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:BrownHairedGirl</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Edmonton-Highlands</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-06-28 13:23:50.737</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:Slambo</td>\n",
       "      <td>http://en.wikipedia.org/wiki/South_Island_line</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None, None, None, None, None, None, Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-06-28 13:23:50.796</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:37.228.231.144</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Vehicle_registrat...</td>\n",
       "      <td>False</td>\n",
       "      <td>(IE, Dublin, 53.333099365234375, Ireland, 53.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-06-28 13:23:50.958</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?diff=9038...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/User:146.115.131.36</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nassau_Broadcasti...</td>\n",
       "      <td>False</td>\n",
       "      <td>(US, Newton, 42.35329818725586, United States,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp                                                url  \\\n",
       "0 2019-06-28 13:23:49.688  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "1 2019-06-28 13:23:49.748  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "2 2019-06-28 13:23:49.807  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "3 2019-06-28 13:23:49.866  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "4 2019-06-28 13:23:49.928  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "5 2019-06-28 13:23:49.987  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "6 2019-06-28 13:23:50.678  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "7 2019-06-28 13:23:50.737  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "8 2019-06-28 13:23:50.796  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "9 2019-06-28 13:23:50.958  https://en.wikipedia.org/w/index.php?diff=9038...   \n",
       "\n",
       "                                             userURL  \\\n",
       "0    http://en.wikipedia.org/wiki/User:Whispyhistory   \n",
       "1  http://en.wikipedia.org/wiki/User:2001:8004:15...   \n",
       "2           http://en.wikipedia.org/wiki/User:Toad02   \n",
       "3             http://en.wikipedia.org/wiki/User:Kvng   \n",
       "4       http://en.wikipedia.org/wiki/User:S.A. Julio   \n",
       "5  http://en.wikipedia.org/wiki/User:Twofingered ...   \n",
       "6  http://en.wikipedia.org/wiki/User:BrownHairedGirl   \n",
       "7           http://en.wikipedia.org/wiki/User:Slambo   \n",
       "8   http://en.wikipedia.org/wiki/User:37.228.231.144   \n",
       "9   http://en.wikipedia.org/wiki/User:146.115.131.36   \n",
       "\n",
       "                                             pageURL  isNewPage  \\\n",
       "0  http://en.wikipedia.org/wiki/Mike_Pringle_(doc...      False   \n",
       "1  http://en.wikipedia.org/wiki/The_Entire_Histor...      False   \n",
       "2  http://en.wikipedia.org/wiki/Oceana_(non-profi...      False   \n",
       "3     http://en.wikipedia.org/wiki/Bull_Pen_Pep_Band      False   \n",
       "4  http://en.wikipedia.org/wiki/2011���12_Gimn��s...      False   \n",
       "5   http://en.wikipedia.org/wiki/User_talk:Nehme1499      False   \n",
       "6    http://en.wikipedia.org/wiki/Edmonton-Highlands      False   \n",
       "7     http://en.wikipedia.org/wiki/South_Island_line      False   \n",
       "8  http://en.wikipedia.org/wiki/Vehicle_registrat...      False   \n",
       "9  http://en.wikipedia.org/wiki/Nassau_Broadcasti...      False   \n",
       "\n",
       "                                           geocoding  \n",
       "0  (None, None, None, None, None, None, None, Non...  \n",
       "1  (AU, None, -25.0, Australia, -25.0, None, AUS,...  \n",
       "2  (None, None, None, None, None, None, None, Non...  \n",
       "3  (None, None, None, None, None, None, None, Non...  \n",
       "4  (None, None, None, None, None, None, None, Non...  \n",
       "5  (None, None, None, None, None, None, None, Non...  \n",
       "6  (None, None, None, None, None, None, None, Non...  \n",
       "7  (None, None, None, None, None, None, None, Non...  \n",
       "8  (IE, Dublin, 53.333099365234375, Ireland, 53.3...  \n",
       "9  (US, Newton, 42.35329818725586, United States,...  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(kafkaCleanDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Stream\n",
    "\n",
    "We can now start to apply transformation logic to our data in real time.  Parse out only the non-null country data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pageURL</th>\n",
       "      <th>countryCode2</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-28 13:29:52.516</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Capital_punishmen...</td>\n",
       "      <td>SI</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp                                            pageURL  \\\n",
       "0 2019-06-28 13:29:52.516  http://en.wikipedia.org/wiki/Capital_punishmen...   \n",
       "\n",
       "  countryCode2  city  \n",
       "0           SI  None  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goeocodingDF = (kafkaCleanDF\n",
    "  .filter(col(\"geocoding.country\").isNotNull())\n",
    "  .select(\"timestamp\", \"pageURL\", \"geocoding.countryCode2\", \"geocoding.city\")\n",
    ")\n",
    "\n",
    "display(goeocodingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_jsq',\n",
       " 'awaitTermination',\n",
       " 'exception',\n",
       " 'explain',\n",
       " 'id',\n",
       " 'isActive',\n",
       " 'lastProgress',\n",
       " 'name',\n",
       " 'processAllAvailable',\n",
       " 'recentProgress',\n",
       " 'runId',\n",
       " 'status',\n",
       " 'stop']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark.streams.active[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff734795c88>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "?StreamingQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "StreamingQuery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with Historical Data\n",
    "\n",
    "Joins between historical data and streams operate in much the same way that other joins work.  Join the stream of Wikipedia data on country metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a lookup table of country data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EnglishShortName</th>\n",
       "      <th>alpha2Code</th>\n",
       "      <th>alpha3Code</th>\n",
       "      <th>numericCode</th>\n",
       "      <th>ISO31662SubdivisionCode</th>\n",
       "      <th>independentTerritory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>004</td>\n",
       "      <td>ISO 3166-2:AF</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>AX</td>\n",
       "      <td>ALA</td>\n",
       "      <td>248</td>\n",
       "      <td>ISO 3166-2:AX</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>008</td>\n",
       "      <td>ISO 3166-2:AL</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>012</td>\n",
       "      <td>ISO 3166-2:DZ</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "      <td>ASM</td>\n",
       "      <td>016</td>\n",
       "      <td>ISO 3166-2:AS</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>AND</td>\n",
       "      <td>020</td>\n",
       "      <td>ISO 3166-2:AD</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AO</td>\n",
       "      <td>AGO</td>\n",
       "      <td>024</td>\n",
       "      <td>ISO 3166-2:AO</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>AI</td>\n",
       "      <td>AIA</td>\n",
       "      <td>660</td>\n",
       "      <td>ISO 3166-2:AI</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Antarctica</td>\n",
       "      <td>AQ</td>\n",
       "      <td>ATA</td>\n",
       "      <td>010</td>\n",
       "      <td>ISO 3166-2:AQ</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>AG</td>\n",
       "      <td>ATG</td>\n",
       "      <td>028</td>\n",
       "      <td>ISO 3166-2:AG</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      EnglishShortName alpha2Code alpha3Code numericCode  \\\n",
       "0          Afghanistan         AF        AFG         004   \n",
       "1        Åland Islands         AX        ALA         248   \n",
       "2              Albania         AL        ALB         008   \n",
       "3              Algeria         DZ        DZA         012   \n",
       "4       American Samoa         AS        ASM         016   \n",
       "5              Andorra         AD        AND         020   \n",
       "6               Angola         AO        AGO         024   \n",
       "7             Anguilla         AI        AIA         660   \n",
       "8           Antarctica         AQ        ATA         010   \n",
       "9  Antigua and Barbuda         AG        ATG         028   \n",
       "\n",
       "  ISO31662SubdivisionCode independentTerritory  \n",
       "0           ISO 3166-2:AF                  Yes  \n",
       "1           ISO 3166-2:AX                   No  \n",
       "2           ISO 3166-2:AL                  Yes  \n",
       "3           ISO 3166-2:DZ                  Yes  \n",
       "4           ISO 3166-2:AS                   No  \n",
       "5           ISO 3166-2:AD                  Yes  \n",
       "6           ISO 3166-2:AO                  Yes  \n",
       "7           ISO 3166-2:AI                   No  \n",
       "8           ISO 3166-2:AQ                   No  \n",
       "9           ISO 3166-2:AG                  Yes  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/countries/ISOCountryCodes/ISOCountryLookup.parquet\")\n",
    "\n",
    "display(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two `DataFrame`s on their two-letter country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pageURL</th>\n",
       "      <th>countryCode2</th>\n",
       "      <th>city</th>\n",
       "      <th>EnglishShortName</th>\n",
       "      <th>alpha2Code</th>\n",
       "      <th>alpha3Code</th>\n",
       "      <th>numericCode</th>\n",
       "      <th>ISO31662SubdivisionCode</th>\n",
       "      <th>independentTerritory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-28 05:10:18.187</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Special:Log/abuse...</td>\n",
       "      <td>BO</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>Bolivia (Plurinational State of)</td>\n",
       "      <td>BO</td>\n",
       "      <td>BOL</td>\n",
       "      <td>068</td>\n",
       "      <td>ISO 3166-2:BO</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-28 05:10:18.246</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos��_Mar��a_Carr...</td>\n",
       "      <td>BO</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>Bolivia (Plurinational State of)</td>\n",
       "      <td>BO</td>\n",
       "      <td>BOL</td>\n",
       "      <td>068</td>\n",
       "      <td>ISO 3166-2:BO</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-28 05:10:18.480</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_States_men...</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>840</td>\n",
       "      <td>ISO 3166-2:US</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp                                            pageURL  \\\n",
       "0 2019-06-28 05:10:18.187  http://en.wikipedia.org/wiki/Special:Log/abuse...   \n",
       "1 2019-06-28 05:10:18.246  http://en.wikipedia.org/wiki/Jos��_Mar��a_Carr...   \n",
       "2 2019-06-28 05:10:18.480  http://en.wikipedia.org/wiki/United_States_men...   \n",
       "\n",
       "  countryCode2        city                  EnglishShortName alpha2Code  \\\n",
       "0           BO  Santa Cruz  Bolivia (Plurinational State of)         BO   \n",
       "1           BO  Santa Cruz  Bolivia (Plurinational State of)         BO   \n",
       "2           US        None          United States of America         US   \n",
       "\n",
       "  alpha3Code numericCode ISO31662SubdivisionCode independentTerritory  \n",
       "0        BOL         068           ISO 3166-2:BO                  Yes  \n",
       "1        BOL         068           ISO 3166-2:BO                  Yes  \n",
       "2        USA         840           ISO 3166-2:US                  Yes  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedDF = goeocodingDF.join(countries, goeocodingDF.countryCode2 == countries.alpha2Code)\n",
    "\n",
    "display(joinedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch and Streaming Writes\n",
    "\n",
    "The logic that applies to batch processing can be ported over to streaming often with only minor modifications.  One best practice is to run batch operations as streaming jobs using a single trigger.  By using a checkpoint location, the metadata on which data has already been processed will be maintained so the cluster can be shut down without a loss of information.  This works best on streaming from a directory, where new files that appear are added to the stream.\n",
    "\n",
    "Writes can be done against always up-to-date parquet files or a Delta table, which offers ACID compliant transactions on top of parquet.\n",
    "\n",
    "For more information on Delta Lake, see the <a href=\"https://delta.io/\" target=\"_blank\">Delta Lake Official website</a><br>\n",
    "Read more about triggers in the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\" target=\"_blank\">Structured Streaming Programming Guide</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to a Delta table and partition by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_FIRST_NAME = 'rajeev'\n",
    "userhome = f\"s3a://temp.intellinum.co/{YOUR_FIRST_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls temp.intellinum.co/rajeev/etl1p/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = f\"{userhome}/etl1p\"\n",
    "queryName = f\"query_delta_lake_{str(uuid.uuid1()).replace('-','')}\"\n",
    "checkpointPath = f\"{basePath}/joined.checkpoint.{queryName}\"\n",
    "joinedPath = \"{}/joined\".format(basePath)\n",
    "\n",
    "delta_output_stream = (joinedDF\n",
    "  .writeStream                                   # Write the stream\n",
    "  .format(\"delta\")                               # Use the delta format\n",
    "  .partitionBy(\"countryCode2\")                   # Specify a feature to partition on\n",
    "  .option(\"checkpointLocation\", checkpointPath)  # Specify where to log metadata\n",
    "  .option(\"path\", joinedPath)                    # Specify the output path\n",
    "  .outputMode(\"append\")                          # Append new records to the output path\n",
    "  .queryName(queryName)                         # The name of the stream\n",
    "  .start()                                       # Start the operation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE joined.checkpoint.query_delta_lake_087336de996311e9b8910a370dd9e3a4/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls temp.intellinum.co/rajeev/etl1p/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream is active and ready.\n"
     ]
    }
   ],
   "source": [
    "untilStreamIsReady(queryName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see that the data is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      8"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "countsDF = (spark.read.format(\"delta\").load(joinedPath)\n",
    "  .select(count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "display(countsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE _delta_log/\r\n",
      "                           PRE countryCode2=AU/\r\n",
      "                           PRE countryCode2=IN/\r\n",
      "                           PRE countryCode2=MY/\r\n",
      "                           PRE countryCode2=PH/\r\n",
      "                           PRE countryCode2=US/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {joinedPath.replace(\"s3a\",\"s3\")}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stop all streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Recovery\n",
    "\n",
    "To recover from cluster failure, Spark uses checkpointing for maintaining state.  In our initial command, we accomplished this using `.option(\"checkpointLocation\", checkpointPath)`.  The checkpoint directory is per query and while that query is active, Spark continuously writes metadata of the processed data to this checkpoint directory.  Even if the entire cluster fails, the query can be restarted on a new cluster using the same directory and Spark can use this to start a new query where the failed one left off.  \n",
    "\n",
    "**This is how Spark ensures end-to-end, exactly-once guarantees and how Spark maintains metadata on what data it has already seen between streaming jobs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE commits/\r\n",
      "                           PRE offsets/\r\n",
      "                           PRE sources/\r\n",
      "2019-06-28 05:10:33         45 metadata\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {checkpointPath.replace(\"s3a\",\"s3\")}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Streaming ETL\n",
    "\n",
    "This exercise entails reading from a Kafka stream of product orders and writing the results to a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Streaming DataFrame\n",
    "\n",
    "Create the streaming DataFrame `productsDF` using the following specifics:<br><br>\n",
    "\n",
    "* Server endpoint: `54.213.33.240:9092`\n",
    "* Topic name: `product-orders`\n",
    "* Format: `kafka`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "productsDF = (spark\n",
    "  .readStream\n",
    "  .option(\"kafka.bootstrap.servers\", \"54.213.33.240:9092\")\n",
    "  .option(\"subscribe\", \"product-orders\")\n",
    "  .option(\"startingOffsets\", \"latest\")\n",
    "  .format(\"kafka\")\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544529</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544530</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544531</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544532</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544533</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544534</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544535</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544536</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544537</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>[123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...</td>\n",
       "      <td>product-orders</td>\n",
       "      <td>0</td>\n",
       "      <td>18544538</td>\n",
       "      <td>1969-12-31 23:59:59.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value           topic  \\\n",
       "0  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "1  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "2  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "3  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "4  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "5  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "6  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "7  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "8  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "9  None  [123, 34, 111, 114, 100, 101, 114, 73, 68, 34,...  product-orders   \n",
       "\n",
       "   partition    offset               timestamp  timestampType  \n",
       "0          0  18544529 1969-12-31 23:59:59.999              0  \n",
       "1          0  18544530 1969-12-31 23:59:59.999              0  \n",
       "2          0  18544531 1969-12-31 23:59:59.999              0  \n",
       "3          0  18544532 1969-12-31 23:59:59.999              0  \n",
       "4          0  18544533 1969-12-31 23:59:59.999              0  \n",
       "5          0  18544534 1969-12-31 23:59:59.999              0  \n",
       "6          0  18544535 1969-12-31 23:59:59.999              0  \n",
       "7          0  18544536 1969-12-31 23:59:59.999              0  \n",
       "8          0  18544537 1969-12-31 23:59:59.999              0  \n",
       "9          0  18544538 1969-12-31 23:59:59.999              0  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(productsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dfTest(\"ET3-P-02-01-01\", True, productsDF.schema['key'].dataType in [BinaryType(), StringType()])\n",
    "dfTest(\"ET3-P-02-01-02\", True, productsDF.schema['value'].dataType in [BinaryType(), StringType()])\n",
    "dfTest(\"ET3-P-02-01-03\", True, productsDF.schema['topic'].dataType == StringType())\n",
    "dfTest(\"ET3-P-02-01-04\", True, productsDF.schema['partition'].dataType == IntegerType())\n",
    "dfTest(\"ET3-P-02-01-05\", True, productsDF.schema['offset'].dataType == LongType())\n",
    "dfTest(\"ET3-P-02-01-06\", True, productsDF.schema['timestamp'].dataType in [TimestampType(), StringType()])\n",
    "dfTest(\"ET3-P-02-01-07\", True, productsDF.schema['timestampType'].dataType == IntegerType())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Defining a Schema\n",
    "\n",
    "Define a schema that consists of the following values.  Save it as `productSchema`<br>\n",
    "\n",
    "| Field            | Type             |\n",
    "|:-----------------|:-----------------|\n",
    "| `orderID`        | `IntegerType()`  |\n",
    "| `productID`      | `IntegerType()`  |\n",
    "| `orderTimestamp` | `TimestampType()`|\n",
    "| `orderQty`       | `IntegerType()`  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "productSchema = (StructType()\n",
    "  .add(\"orderID\", IntegerType())\n",
    "  .add(\"productID\", IntegerType())\n",
    "  .add(\"orderTimestamp\", TimestampType())\n",
    "  .add(\"orderQty\", IntegerType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType\n",
    "\n",
    "dfTest(\"ET3-P-02-02-01\", True, productSchema['orderID'].dataType == IntegerType())\n",
    "dfTest(\"ET3-P-02-02-02\", True, productSchema['productID'].dataType == IntegerType())\n",
    "dfTest(\"ET3-P-02-02-03\", True, productSchema['orderTimestamp'].dataType in [StringType(), TimestampType()])\n",
    "dfTest(\"ET3-P-02-02-04\", True, productSchema['orderQty'].dataType == IntegerType())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Parsing the Kafka Data\n",
    "\n",
    "Parse the `value` column of the Kafka data.  Remember to use the `from_json()` function and apply your schema.  Save the results to `ordersDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "ordersDF = (productsDF\n",
    "            .select(F.from_json(F.col(\"value\").cast(StringType()), productSchema).alias(\"message\"))\n",
    "            .select(\"message.*\")\n",
    "#             .withColumnRenamed('orderID', 'order.orderID')\n",
    "#             .withColumnRenamed('productID', 'order.productID')\n",
    "#             .withColumnRenamed('orderTimestamp', 'order.orderTimestamp')\n",
    "#             .withColumnRenamed('orderQty', 'order.orderQty')\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: integer (nullable = true)\n",
      " |-- productID: integer (nullable = true)\n",
      " |-- orderTimestamp: timestamp (nullable = true)\n",
      " |-- orderQty: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n",
      "Wait until the stream is ready...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderID</th>\n",
       "      <th>productID</th>\n",
       "      <th>orderTimestamp</th>\n",
       "      <th>orderQty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1190807</td>\n",
       "      <td>872</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1190808</td>\n",
       "      <td>829</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1190809</td>\n",
       "      <td>911</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1190810</td>\n",
       "      <td>890</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1190811</td>\n",
       "      <td>837</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1190812</td>\n",
       "      <td>870</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1190813</td>\n",
       "      <td>854</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1190814</td>\n",
       "      <td>804</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1190815</td>\n",
       "      <td>875</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1190816</td>\n",
       "      <td>923</td>\n",
       "      <td>2019-06-28 05:19:13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orderID  productID      orderTimestamp  orderQty\n",
       "0  1190807        872 2019-06-28 05:19:13         7\n",
       "1  1190808        829 2019-06-28 05:19:13         6\n",
       "2  1190809        911 2019-06-28 05:19:13         6\n",
       "3  1190810        890 2019-06-28 05:19:13         2\n",
       "4  1190811        837 2019-06-28 05:19:13         6\n",
       "5  1190812        870 2019-06-28 05:19:13         3\n",
       "6  1190813        854 2019-06-28 05:19:13         5\n",
       "7  1190814        804 2019-06-28 05:19:13         8\n",
       "8  1190815        875 2019-06-28 05:19:13        10\n",
       "9  1190816        923 2019-06-28 05:19:13        10"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(ordersDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`order.orderID`' given input columns: [orderID, productID, orderTimestamp, orderQty];;\\n'Project ['order.orderID AS orderID#279368, 'order.productID AS productID#279369, 'order.orderTimestamp AS orderTimestamp#279370, 'order.orderQty AS orderQty#279371]\\n+- Project [message#279196.orderID AS orderID#279198, message#279196.productID AS productID#279199, message#279196.orderTimestamp AS orderTimestamp#279200, message#279196.orderQty AS orderQty#279201]\\n   +- Project [jsontostructs(StructField(orderID,IntegerType,true), StructField(productID,IntegerType,true), StructField(orderTimestamp,TimestampType,true), StructField(orderQty,IntegerType,true), cast(value#279055 as string), Some(Etc/Universal)) AS message#279196]\\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1d232152, kafka, Map(startingOffsets -> latest, subscribe -> product-orders, kafka.bootstrap.servers -> 54.213.33.240:9092), [key#279054, value#279055, topic#279056, partition#279057, offset#279058L, timestamp#279059, timestampType#279060], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4ccf0950,kafka,List(),None,List(),None,Map(startingOffsets -> latest, subscribe -> product-orders, kafka.bootstrap.servers -> 54.213.33.240:9092),None), kafka, [key#279047, value#279048, topic#279049, partition#279050, offset#279051L, timestamp#279052, timestampType#279053]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114779.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`order.orderID`' given input columns: [orderID, productID, orderTimestamp, orderQty];;\n'Project ['order.orderID AS orderID#279368, 'order.productID AS productID#279369, 'order.orderTimestamp AS orderTimestamp#279370, 'order.orderQty AS orderQty#279371]\n+- Project [message#279196.orderID AS orderID#279198, message#279196.productID AS productID#279199, message#279196.orderTimestamp AS orderTimestamp#279200, message#279196.orderQty AS orderQty#279201]\n   +- Project [jsontostructs(StructField(orderID,IntegerType,true), StructField(productID,IntegerType,true), StructField(orderTimestamp,TimestampType,true), StructField(orderQty,IntegerType,true), cast(value#279055 as string), Some(Etc/Universal)) AS message#279196]\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1d232152, kafka, Map(startingOffsets -> latest, subscribe -> product-orders, kafka.bootstrap.servers -> 54.213.33.240:9092), [key#279054, value#279055, topic#279056, partition#279057, offset#279058L, timestamp#279059, timestampType#279060], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4ccf0950,kafka,List(),None,List(),None,Map(startingOffsets -> latest, subscribe -> product-orders, kafka.bootstrap.servers -> 54.213.33.240:9092),None), kafka, [key#279047, value#279048, topic#279049, partition#279050, offset#279051L, timestamp#279052, timestampType#279053]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat sun.reflect.GeneratedMethodAccessor589.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-b8a0dac0097a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order.productID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"productID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order.orderTimestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"orderTimestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        col(\"order.orderQty\").alias(\"orderQty\"))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"orderID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"orderQty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"orderTimestamp\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"productID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pyspark24_py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`order.orderID`' given input columns: [orderID, productID, orderTimestamp, orderQty];;\\n'Project ['order.orderID AS orderID#279368, 'order.productID AS productID#279369, 'order.orderTimestamp AS orderTimestamp#279370, 'order.orderQty AS orderQty#279371]\\n+- Project [message#279196.orderID AS orderID#279198, message#279196.productID AS productID#279199, message#279196.orderTimestamp AS orderTimestamp#279200, message#279196.orderQty AS orderQty#279201]\\n   +- Project [jsontostructs(StructField(orderID,IntegerType,true), StructField(productID,IntegerType,true), StructField(orderTimestamp,TimestampType,true), StructField(orderQty,IntegerType,true), cast(value#279055 as string), Some(Etc/Universal)) AS message#279196]\\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1d232152, kafka, Map(startingOffsets -> latest, subscribe -> product-orders, kafka.bootstrap.servers -> 54.213.33.240:9092), [key#279054, value#279055, topic#279056, partition#279057, offset#279058L, timestamp#279059, timestampType#279060], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4ccf0950,kafka,List(),None,List(),None,Map(startingOffsets -> latest, subscribe -> product-orders, kafka.bootstrap.servers -> 54.213.33.240:9092),None), kafka, [key#279047, value#279048, topic#279049, partition#279050, offset#279051L, timestamp#279052, timestampType#279053]\\n\""
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "\n",
    "_df = ordersDF.select(col(\"order.orderID\").alias(\"orderID\"),\n",
    "                       col(\"order.productID\").alias(\"productID\"),\n",
    "                       col(\"order.orderTimestamp\").alias(\"orderTimestamp\"),\n",
    "                       col(\"order.orderQty\").alias(\"orderQty\"))\n",
    "\n",
    "expected = {\"orderID\": IntegerType(), \"orderQty\": IntegerType(), \"orderTimestamp\": TimestampType(), \"productID\": IntegerType()}\n",
    "sch = {c.name: c.dataType for c in _df.schema}\n",
    "\n",
    "dfTest(\"ET3-P-02-03-01\", True, sch == expected)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write to a Delta Table\n",
    "\n",
    "Write the results to an always up-to-date Delta table to the provided path.  The table should have the following columns:<br><br>\n",
    "\n",
    "1. `orderID`\n",
    "1. `productID`\n",
    "1. `orderTimestamp`\n",
    "1. `orderQty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls temp.intellinum.co/rajeev/etl1p/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "basePath = f\"{userhome}/etl1p\"\n",
    "queryName = f\"query_delta_lake_{str(uuid.uuid1()).replace('-','')}\"\n",
    "checkpointPath = f\"{basePath}/orders.checkpoint.{queryName}\"\n",
    "ordersPath = \"{}/orders\".format(basePath)\n",
    "\n",
    "# FILL_IN\n",
    "delta_output_stream = (ordersDF\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", checkpointPath)\n",
    "  .option(\"path\", ordersPath)\n",
    "  .outputMode(\"append\")\n",
    "  .queryName(queryName)\n",
    "  .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls temp.intellinum.co/rajeev/etl1p/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream is active and ready.\n"
     ]
    }
   ],
   "source": [
    "untilStreamIsReady(queryName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE _delta_log/\r\n",
      "2019-06-28 05:19:43       1427 part-00000-16ec59b8-f78c-44a1-9934-8dc1b7b24b33-c000.snappy.parquet\r\n",
      "2019-06-28 05:19:52       4173 part-00000-224620c0-3b01-4b25-b1b4-260ddb2b692a-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {ordersPath.replace(\"s3a\",\"s3\")}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spark.read.format(\"delta\").load(ordersPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: integer (nullable = true)\n",
      " |-- productID: integer (nullable = true)\n",
      " |-- orderTimestamp: timestamp (nullable = true)\n",
      " |-- orderQty: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderID</th>\n",
       "      <th>productID</th>\n",
       "      <th>orderTimestamp</th>\n",
       "      <th>orderQty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1183763</td>\n",
       "      <td>838</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1183764</td>\n",
       "      <td>904</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1183765</td>\n",
       "      <td>765</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1183766</td>\n",
       "      <td>882</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1183767</td>\n",
       "      <td>810</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1183768</td>\n",
       "      <td>850</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1183769</td>\n",
       "      <td>854</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1183770</td>\n",
       "      <td>901</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1183771</td>\n",
       "      <td>844</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1183772</td>\n",
       "      <td>952</td>\n",
       "      <td>2019-06-28 05:19:40</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orderID  productID      orderTimestamp  orderQty\n",
       "0  1183763        838 2019-06-28 05:19:40         9\n",
       "1  1183764        904 2019-06-28 05:19:40         7\n",
       "2  1183765        765 2019-06-28 05:19:40        10\n",
       "3  1183766        882 2019-06-28 05:19:40         1\n",
       "4  1183767        810 2019-06-28 05:19:40         2\n",
       "5  1183768        850 2019-06-28 05:19:40         5\n",
       "6  1183769        854 2019-06-28 05:19:40         6\n",
       "7  1183770        901 2019-06-28 05:19:40         1\n",
       "8  1183771        844 2019-06-28 05:19:40         3\n",
       "9  1183772        952 2019-06-28 05:19:40         8"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "ordersPath = \"{}/orders\".format(basePath)\n",
    "_df = spark.read.format(\"delta\").load(ordersPath)\n",
    "\n",
    "expected = {\"orderID\": IntegerType(), \"orderQty\": IntegerType(), \"orderTimestamp\": TimestampType(), \"productID\": IntegerType()}\n",
    "sch = {c.name: c.dataType for c in _df.schema}\n",
    "\n",
    "dfTest(\"ET3-P-02-04-01\", True, sch == expected)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Now Stop the Stream and Delete Files\n",
    "\n",
    "Stop the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursively delete the files you created (this is a permanent operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://temp.intellinum.co/rajeev/etl1p'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/commits/0\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/metadata\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders/_delta_log/00000000000000000001.json\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/offsets/3\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders/_delta_log/00000000000000000002.json\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/commits/1\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/offsets/1\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders/part-00000-16ec59b8-f78c-44a1-9934-8dc1b7b24b33-c000.snappy.parquet\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/sources/0/0\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/commits/2\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/offsets/0\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders/_delta_log/00000000000000000000.json\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders/part-00000-7172a9b4-8b28-4d19-bfea-e4d64e64ee5d-c000.snappy.parquet\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders/part-00000-224620c0-3b01-4b25-b1b4-260ddb2b692a-c000.snappy.parquet\n",
      "delete: s3://temp.intellinum.co/rajeev/etl1p/orders.checkpoint.query_delta_lake_4d74a410996411e9b8910a370dd9e3a4/offsets/2\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive {basePath.replace('s3a','s3')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What is the best practice for different versions of ETL jobs?  \n",
    "**Answer:** Generally speaking, an ETL solution should have three versions:\n",
    "0. *Batch* or the ability to run a periodic workload\n",
    "0. *Streaming* or the ability to process incoming data in real time\n",
    "0. *Incremental* or the ability to process a specific set of data, especially in the case of job failure.<br>\n",
    "In practice, batch and streaming jobs are oftentimes combined where a batch job is a streaming workload using a single trigger.\n",
    "\n",
    "**Question:** What are commonly approached as data streams?  \n",
    "**Answer:** Apache Kafka and the AWS managed alternative Kinesis are common data streams.  Additionally, it's common to monitor a directory for incoming files.  When a new file appears, it is brought into the stream for processing.\n",
    "\n",
    "**Question:** How does Spark ensure exactly-once data delivery and maintain metadata on a stream?  \n",
    "**Answer:** Checkpoints give Spark this fault tolerance through the ability to maintain state off of the cluster.\n",
    "\n",
    "**Question:** How does the Spark approach to streaming integrate with other Spark features?  \n",
    "**Answer:** Spark Streaming uses the same DataFrame API, allowing easy integration with other Spark functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
