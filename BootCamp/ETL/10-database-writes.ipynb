{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Writes\n",
    "\n",
    "Apache Spark&trade; allows you to write to a number of target databases in parallel, storing the transformed data from from your ETL job.\n",
    "\n",
    "## In this lesson you:\n",
    "* Write to a target database in serial and in parallel\n",
    "* Repartition DataFrames to optimize table inserts\n",
    "* Coalesce DataFrames to minimize data shuffling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Writes in Spark\n",
    "\n",
    "Writing to a database in Spark differs from other tools largely due to its distributed nature. There are a number of variables that can be tweaked to optimize performance, largely relating to how data is organized on the cluster. Partitions are the first step in understanding performant database connections.\n",
    "\n",
    "**A partition is a portion of your total data set,** which is divided into many of these portions so Spark can distribute your work across a cluster. \n",
    "\n",
    "The other concept needed to understand Spark's computation is a slot (also known as a core). **A slot/core is a resource available for the execution of computation in parallel.** In brief, a partition refers to the distribution of data while a slot refers to the distribution of computation.\n",
    "\n",
    "<div><img src=\"../../resources/partitions-and-cores.png\" style=\"height: 400px; margin: 20px\"/></div>\n",
    "\n",
    "As a general rule of thumb, the number of partitions should be a multiple of the number of cores. For instance, with 5 partitions and 8 slots, 3 of the slots will be underutilized. With 9 partitions and 8 slots, a job will take twice as long as it waits for the extra partition to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Partitions\n",
    "\n",
    "In the context of JDBC database writes, **the number of partitions determine the number of connections used to push data through the JDBC API.** There are two ways to control this parallelism:  \n",
    "\n",
    "| Function | Transformation Type | Use | Evenly distributes data across partitions? |\n",
    "| :----------------|:----------------|:----------------|:----------------| \n",
    "| `.coalesce(n)`   | narrow (does not shuffle data) | reduce the number of partitions | no |\n",
    "| `.repartition(n)`| wide (includes a shuffle operation) | increase the number of partitions | yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_10-database-writes\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_10-database-writes\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing a DataFrame of Wikipedia pageviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "      <th>requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-22T14:13:34</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-22T14:23:18</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-22T14:36:47</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-22T14:38:39</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-22T14:57:11</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-03-22T15:03:18</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-03-22T15:16:47</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-03-22T15:45:03</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-03-22T15:58:32</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-03-22T16:06:11</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     site  requests\n",
       "0  2015-03-22T14:13:34   mobile      1425\n",
       "1  2015-03-22T14:23:18  desktop      2534\n",
       "2  2015-03-22T14:36:47  desktop      2444\n",
       "3  2015-03-22T14:38:39   mobile      1488\n",
       "4  2015-03-22T14:57:11   mobile      1519\n",
       "5  2015-03-22T15:03:18   mobile      1559\n",
       "6  2015-03-22T15:16:47   mobile      1510\n",
       "7  2015-03-22T15:45:03  desktop      2673\n",
       "8  2015-03-22T15:58:32  desktop      2463\n",
       "9  2015-03-22T16:06:11  desktop      2525"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiDF = (spark.read\n",
    "  .parquet(\"s3a://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second.parquet\")\n",
    ")\n",
    "display(wikiDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the number of partitions by changing the DataFrame into an RDD and using the `.getNumPartitions()` method.  \n",
    "Since the Parquet file was saved with 5 partitions, those partitions are retained when you import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 5\n"
     ]
    }
   ],
   "source": [
    "partitions = wikiDF.rdd.getNumPartitions()\n",
    "print(\"Partitions: {0:,}\".format( partitions ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the number of partitions to 16, use `.repartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 16\n"
     ]
    }
   ],
   "source": [
    "repartitionedWikiDF = wikiDF.repartition(16)\n",
    "print(\"Partitions: {0:,}\".format( repartitionedWikiDF.rdd.getNumPartitions() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the number of partitions, use `.coalesce()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "coalescedWikiDF = repartitionedWikiDF.coalesce(2)\n",
    "print(\"Partitions: {0:,}\".format( coalescedWikiDF.rdd.getNumPartitions() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Default Partitions\n",
    "\n",
    "Spark uses a default value of 200 partitions, which comes from real-world experience by Spark engineers. This is an adjustable configuration setting. Run the following cell to see this value.\n",
    "\n",
    "Get and set any number of different configuration settings in this manner. <a href=\"https://spark.apache.org/docs/latest/configuration.html\" target=\"_blank\">See the Spark documents</a> for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the number of partitions with the following cell.  **This changes the number of partitions after a shuffle operation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check to see how this changes an operation involving a data shuffle, such as an `.orderBy()`.  Recall that coalesced `coalescedWikiDF` to 2 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 8\n"
     ]
    }
   ],
   "source": [
    "orderByPartitions = coalescedWikiDF.orderBy(\"requests\").rdd.getNumPartitions()\n",
    "print(\"Partitions: {0:,}\".format( orderByPartitions ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.orderBy()` triggered the repartition of the DataFrame into 8 partitions.  Now reset the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Database Writes\n",
    "\n",
    "In that lesson you defined the number of partitions in the call to the database.  \n",
    "\n",
    "**By contrast and when writing to a database, the number of active connections to the database is determined by the number of partitions of the DataFrame.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this by writing `wikiDF` to the `/tmp` directory.  Recall that `wikiDF` has 5 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_FIRST_NAME = \"rajeev\" # CHANGE ME!\n",
    "userhome = f\"s3a://temp.intellinum.co/{YOUR_FIRST_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiDF.write.mode(\"OVERWRITE\").parquet(userhome+\"/wiki.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the number of partitions in `wiki.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-24 18:46:33          0 _SUCCESS\r\n",
      "2019-06-24 18:46:32   25172128 part-00000-1ebb3dfb-9593-4dff-ae43-d00839f879d7-c000.snappy.parquet\r\n",
      "2019-06-24 18:46:30   24794655 part-00001-1ebb3dfb-9593-4dff-ae43-d00839f879d7-c000.snappy.parquet\r\n",
      "2019-06-24 18:46:29   11632341 part-00002-1ebb3dfb-9593-4dff-ae43-d00839f879d7-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {userhome.replace(\"s3a\",\"s3\")+\"/wiki.parquet/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has 5 parts, meaning Spark wrote the data through 5 different connections to this directory in the file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining in the Spark UI\n",
    "\n",
    "Click the hyperlink to `Spark UI` under the following code cell in order to see a breakdown of the job you triggered. The link might not work as is, you need to replace the `ip-xx-xx-xx-xx.ec2.internal` part with the DNS url of the jupyter. For example:\n",
    "\n",
    "---\n",
    "Change\n",
    "```\n",
    "http://ip-172-31-20-26.ec2.internal:4043/\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "http://ec2-54-172-218-250.compute-1.amazonaws.com:4043/\n",
    "```\n",
    "---\n",
    "\n",
    "You will be redirected to port `20888`. Again , the link doesn't work, you need to fix the `ip-xx-xx-xx-xx.ec2.internal` part. For example:\n",
    "\n",
    "Change\n",
    "```\n",
    "http://ip-172-31-20-26.ec2.internal:20888/proxy/application_1561313424801_0011/\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "\n",
    "http://ec2-54-172-218-250.compute-1.amazonaws.com:20888/proxy/application_1561313424801_0011/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-20-26.ec2.internal:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn-client</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_etl_10-database-writes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f394a97cf98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the jobs tab on Spark UI while you run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiDF.repartition(12).write.mode(\"OVERWRITE\").parquet(userhome+\"/wiki.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 stages were initially triggered, one for each partition of our data.  When you repartitioned the DataFrame to 12 partitions, 12 stages were needed, one to write each partition of the data.  Run the following and observe how the repartitioning changes the number of stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiDF.repartition(10).write.mode(\"OVERWRITE\").parquet(userhome+\"/wiki.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Upserts\n",
    "\n",
    "Upserts insert a record into a database if it doesn't already exist, and updates the existing record if it does.  **Upserts are not supported in core Spark** due to the transactional nature of upserting and the immutable nature of Spark. You can only append or overwrite.  Databricks offers a data management system called Databricks Delta that does allow for upserts and other transactional functionality. [See the Databricks Delta docs for more information.](https://docs.databricks.com/delta/delta-batch.html#upserts-merge-into)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Changing Partitions\n",
    "\n",
    "Change the number of partitions to prepare the optimal database write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Helper Functions and Data\n",
    "\n",
    "A function is defined for you to print out the number of records in each DataFrame.  Run the following cell to define that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRecordsPerPartition(df):\n",
    "    '''\n",
    "    Utility method to count & print the number of records in each partition\n",
    "    '''\n",
    "    print(\"Per-Partition Counts:\")\n",
    "    \n",
    "    def countInPartition(iterator): \n",
    "        yield __builtin__.sum(1 for _ in iterator)\n",
    "      \n",
    "    results = (df.rdd                   # Convert to an RDD\n",
    "      .mapPartitions(countInPartition)  # For each partition, count\n",
    "      .collect()                        # Return the counts to the driver\n",
    "    )\n",
    "\n",
    "    for result in results: \n",
    "        print(\"* \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data to sitting in\n",
    "`s3a://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second.parquet` to `wikiDF`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "      <th>requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-22T14:13:34</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-22T14:23:18</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-22T14:36:47</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-22T14:38:39</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-22T14:57:11</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-03-22T15:03:18</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-03-22T15:16:47</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-03-22T15:45:03</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-03-22T15:58:32</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-03-22T16:06:11</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     site  requests\n",
       "0  2015-03-22T14:13:34   mobile      1425\n",
       "1  2015-03-22T14:23:18  desktop      2534\n",
       "2  2015-03-22T14:36:47  desktop      2444\n",
       "3  2015-03-22T14:38:39   mobile      1488\n",
       "4  2015-03-22T14:57:11   mobile      1519\n",
       "5  2015-03-22T15:03:18   mobile      1559\n",
       "6  2015-03-22T15:16:47   mobile      1510\n",
       "7  2015-03-22T15:45:03  desktop      2673\n",
       "8  2015-03-22T15:58:32  desktop      2463\n",
       "9  2015-03-22T16:06:11  desktop      2525"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "path = 's3a://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second.parquet'\n",
    "wikiDF = spark.read.parquet(path)\n",
    "\n",
    "display(wikiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "dfTest(\"ET2-P-06-01-01\", 7200000, wikiDF.count())\n",
    "dfTest(\"ET2-P-06-01-02\", ['timestamp', 'site', 'requests'], wikiDF.columns)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the count of records by partition using `printRecordsPerPartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Partition Counts:\n",
      "* 2926025\n",
      "* 2926072\n",
      "* 1347903\n"
     ]
    }
   ],
   "source": [
    "printRecordsPerPartition(wikiDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Repartition the Data\n",
    "\n",
    "Define three new DataFrames:\n",
    "\n",
    "* `wikiDF1Partition`: `wikiDF` with 1 partition\n",
    "* `wikiDF16Partition`: `wikiDF` with 16 partitions\n",
    "* `wikiDF128Partition`: `wikiDF` with 128 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "16\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "wikiDF1Partition = wikiDF.repartition(1)\n",
    "wikiDF16Partition = wikiDF.repartition(16)\n",
    "wikiDF128Partition = wikiDF.repartition(128)\n",
    "\n",
    "print(wikiDF1Partition.rdd.getNumPartitions())\n",
    "print(wikiDF16Partition.rdd.getNumPartitions())\n",
    "print(wikiDF128Partition.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "\n",
    "dfTest(\"ET2-P-06-02-01\", 1, wikiDF1Partition.rdd.getNumPartitions())\n",
    "dfTest(\"ET2-P-06-02-02\", 16, wikiDF16Partition.rdd.getNumPartitions())\n",
    "dfTest(\"ET2-P-06-02-03\", 128, wikiDF128Partition.rdd.getNumPartitions())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Examine the Distribution of Records\n",
    "\n",
    "Use `printRecordsPerPartition()` to examine the distribution of records across the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Partition Counts:\n",
      "* 7200000\n",
      "Per-Partition Counts:\n",
      "* 450001\n",
      "* 450001\n",
      "* 450001\n",
      "* 450001\n",
      "* 450000\n",
      "* 449999\n",
      "* 449999\n",
      "* 449999\n",
      "* 449999\n",
      "* 449999\n",
      "* 449999\n",
      "* 449998\n",
      "* 450001\n",
      "* 450001\n",
      "* 450001\n",
      "* 450001\n",
      "Per-Partition Counts:\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56250\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56249\n",
      "* 56248\n",
      "* 56248\n",
      "* 56248\n",
      "* 56248\n",
      "* 56248\n",
      "* 56248\n",
      "* 56248\n",
      "* 56248\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n",
      "* 56251\n"
     ]
    }
   ],
   "source": [
    "printRecordsPerPartition(wikiDF1Partition)\n",
    "printRecordsPerPartition(wikiDF16Partition)\n",
    "printRecordsPerPartition(wikiDF128Partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Coalesce `wikiDF16Partition` and Examine the Results\n",
    "\n",
    "Coalesce `wikiDF16Partition` to `10` partitions, saving the result to `wikiDF16PartitionCoalesced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "wikiDF16PartitionCoalesced = wikiDF16Partition.coalesce(10)\n",
    "\n",
    "print(wikiDF16PartitionCoalesced.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "\n",
    "dfTest(\"ET2-P-06-03-01\", 10, wikiDF16PartitionCoalesced.rdd.getNumPartitions())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the new distribution of data using `printRecordsPerPartition`.  Is the distribution uniform?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** How do you determine the number of connections to the database you write to?  \n",
    "**Answer:** Spark makes one connection for each partition in your data. Increasing the number of partitions increases the database connections.\n",
    "\n",
    "**Question:** How do you increase and decrease the number of partitions in your data?  \n",
    "**Answer:** `.repartitions(n)` increases the number of partitions in your data. It can also decrease the number of partitions, but since this is a wide operation it should be used sparingly.  `.coalesce(n)` decreases the number of partitions in your data. If you use `.coalesce(n)` with a number greater than the current partitions, this DataFrame method will have no effect.\n",
    "\n",
    "**Question:** How can you change the default number of partitions?  \n",
    "**Answer:** Changing the configuration parameter `spark.sql.shuffle.partitions` will alter the default number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
