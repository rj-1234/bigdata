{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Failure\n",
    "\n",
    "Apache Spark&trade; allows for the creation of robust job failure strategies\n",
    "\n",
    "## In this lesson you:\n",
    "* Use antijoins to ensure that duplicate records are not loaded into your target database\n",
    "* Design a job failure monitoring strategy using job ID's\n",
    "* Evaluate job failure recovery strategies for idempotence \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idempotent Failure Recovery\n",
    "\n",
    "Jobs can fail for any number of reasons.  The majority of job failures are caused by input/output (I/O) problems but other issues include schema evolution, data corruption, and hardware failures.  Recovery from job failure should be guided by the principle of *idempotence, or the property of operations whereby the operation can be applied multiple times without changing the results beyond the first application.*\n",
    "\n",
    "More technically, the definition of idempotence is as follows where a function `f` applied to `x` is equal to that function applied to `x` two or more times:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`f(x) = f(f(x)) = f(f(f(x))) = ...`\n",
    "\n",
    "In ETL job recovery, we need to be able to run a job multiple times and get our data into our target database without duplicates.  This can be accomplished in a few ways:<br><br>\n",
    "\n",
    "* A **left antijoin** of new data on data already in a database will give you only the data that was not inserted\n",
    "* Overwriting all data is a resource-intensive way to ensure that all data was written\n",
    "* The transactionality of databases enable all-or-nothing database writes where failure of any part of the job will not result in any committed data\n",
    "* Leveraging primary keys in a database will only write data where the primary key is not already present or upsert the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "import uuid\n",
    "import time\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_13-failure-recovery\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_13-failure-recovery\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','io.delta:delta-core_2.11:0.2.0,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.2,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.2,net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    try:\n",
    "        # For spark-core \n",
    "        result = df.limit(limit).toPandas()\n",
    "    except Exception as e:\n",
    "        # For structured-streaming\n",
    "        stream_name = str(uuid.uuid1()).replace(\"-\",\"\")\n",
    "        query = (\n",
    "          df\n",
    "            .writeStream\n",
    "            .format(\"memory\")        # memory = store in-memory table (for debugging only)\n",
    "            .queryName(stream_name) # show = name of the in-memory table\n",
    "            .trigger(processingTime='1 seconds') #Trigger = 1 second\n",
    "            .outputMode(\"append\")  # append\n",
    "            .start()\n",
    "        )\n",
    "        while query.isActive:\n",
    "            time.sleep(1)\n",
    "            result = spark.sql(f\"select * from {stream_name} limit {limit}\").toPandas()\n",
    "            print(\"Wait until the stream is ready...\")\n",
    "            if result.empty == False:\n",
    "                break\n",
    "        result = spark.sql(f\"select * from {stream_name} limit {limit}\").toPandas()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def untilStreamIsReady(name):\n",
    "    queries = list(filter(lambda query: query.name == name, spark.streams.active))\n",
    "\n",
    "    if len(queries) == 0:\n",
    "        print(\"The stream is not active.\")\n",
    "\n",
    "    else:\n",
    "        while (queries[0].isActive and len(queries[0].recentProgress) == 0):\n",
    "            pass # wait until there is any type of progress\n",
    "\n",
    "        if queries[0].isActive:\n",
    "            queries[0].awaitTermination(5)\n",
    "            print(\"The stream is active and ready.\")\n",
    "        else:\n",
    "            print(\"The stream is not active.\")\n",
    "            \n",
    "            \n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Idempotent Strategy: Left Antijoin\n",
    "\n",
    "In traditional ETL, a job recovery strategy where only partial data was written to database would look something as follow:\n",
    "\n",
    "```\n",
    "begin transaction;\n",
    "  delete from production_table where batch_period = failed_batch_period;\n",
    "  insert into production_table select * from staging_table;\n",
    "  drop table staging_table;  \n",
    "end transaction;\n",
    "```\n",
    "\n",
    "This won't work in a Spark environment because data structures are immutable.  One alternative strategy among the several listed in the cell above relies on a left antijoin, which returns all data in the left table that doesn't exist in the right table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follow cell to create a mock production and staging table. Create a staging table from parquet that contains log records and then create a production table that only has 20 percent of the records from staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "staging_table = (spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/EDGAR-Log-20170329/enhanced/EDGAR-Log-20170329-sample.parquet/\")\n",
    "  .dropDuplicates(['ip', 'date', 'time']))\n",
    "\n",
    "production_table = staging_table.sample(.2, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see that the `poduction_table` only has 20% of the data from `staging_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2002676166946979"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_table.count() / staging_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two tables using a left antijoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "failedDF = staging_table.join(production_table, on=[\"ip\", \"date\", \"time\"], how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union `production_table` with the results from the left antijoin.\n",
    "\n",
    "Append operations are generally not idempotent as they can result in duplicate records.  Streaming operations that maintain state and append to an always up-to-date parquet or Delta table are idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF = production_table.union(failedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two tables are now equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_table.count() == fullDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What is idempotence?  \n",
    "**Answer:** For ETL jobs, idempotence is the ability to run the same job multiple times without getting duplicate data.  This is the primary axiom for ensuring that ETL workloads do not have any unexpected behavior.\n",
    "\n",
    "**Question:** How can I accomplish idempotence in Spark jobs?  \n",
    "**Answer:** There are a number of strategies for accomplishing this.  Doing an antijoin of your full data on already loaded data is one method.  This can be in the form of an incremental update script that would run on the case of job failure.  By counting the records at the beginning and end of a job, you can detect whether any unexpected behavior would demand the use of this incremental script.\n",
    "\n",
    "**Question:** How can I detect job failure?  \n",
    "**Answer:** This depends largely on the pipeline you're creating.  One common best practice is to have a monitoring job that periodically checks jobs for failure.  This can be tied to email or other alerting mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
