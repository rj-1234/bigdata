{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Part 1: Data Extraction\n",
    "\n",
    "In this course data engineers access data where it lives and then apply data extraction best practices, including schemas, corrupt record handling, and parallelized code. By the end of this course, you will extract data from multiple sources, use schema inference and apply user-defined schemas, and navigate Spark documents to source solutions.\n",
    "\n",
    "** The course is composed of the following lessons:**  \n",
    "1. Course Overview and Setup\n",
    "2. ETL Process Overview\n",
    "3. Connecting to S3\n",
    "4. Connecting to JDBC\n",
    "5. Applying Schemas to JSON Data\n",
    "6. Corrupt Record Handling\n",
    "7. Loading Data and Productionalizing\n",
    "8. Milestone Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://yuan.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_01_extraction\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_01_extraction\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://yuan.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL with Spark\n",
    "\n",
    "The **extract, transform, load (ETL)** process takes data from one or more sources, transforms it, normally by adding structure, and then loads it into a target database. \n",
    "\n",
    "A common ETL job takes log files from a web server, parses out pertinent fields so it can be readily queried, and then loads it into a database.\n",
    "\n",
    "ETL may seem simple: applying structure to data so itâ€™s in a desired form. However, the complexity of ETL is in the details. Data Engineers building ETL pipelines must understand and apply the following concepts:<br><br>\n",
    "\n",
    "* Optimizing data formats and connections\n",
    "* Determining the ideal schema\n",
    "* Handling corrupt records\n",
    "* Automating workloads\n",
    "\n",
    "This course addresses these concepts.\n",
    "\n",
    "<img src=\"../../resources/ETL-overview.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n",
    "\n",
    "Stay tuned for upcoming courses which will cover:<br><br>\n",
    "\n",
    "* Complex and performant data transformations\n",
    "* Schema changes over time  \n",
    "* Recovery from job failures\n",
    "* Avoiding duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Process Overview\n",
    "\n",
    "Apache Spark&trade; allow you to create an end-to-end _extract, transform, load (ETL)_ pipeline.\n",
    "## In this lesson you:\n",
    "* Create a basic end-to-end ETL pipeline\n",
    "* Demonstrate the Spark approach to ETL pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Approach\n",
    "\n",
    "Spark offers a compute engine and connectors to virtually any data source. By leveraging easily scaled infrastructure and accessing data where it lives, Spark addresses the core needs of a big data application.\n",
    "\n",
    "These principles comprise the Spark approach to ETL, providing a unified and scalable approach to big data pipelines: <br><br>\n",
    "\n",
    "1. Spark offer a **unified platform** \n",
    " - Spark combines ETL, stream processing, machine learning, and collaborative notebooks.\n",
    " - Data scientists, analysts, and engineers can write Spark code in Python, Scala, SQL, and R.\n",
    "2. Spark's unified platform is **scalable to petabytes of data and clusters of thousands of nodes**.  \n",
    " - The same code written on smaller data sets scales to large workloads, often with only small changes.\n",
    "2. Spark decouples data storage from the compute and query engine.  \n",
    " - Spark's query engine **connects to any number of data sources** such as S3, Azure Blob Storage, Redshift, and Kafka.  \n",
    " - This **minimizes costs**; a dedicated cluster does not need to be maintained and the compute cluster is **easily updated to the latest version** of Spark.\n",
    " \n",
    "<img src=\"../../resources/Workload_Tools_2-01.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Basic ETL Job\n",
    "\n",
    "In this lesson you use web log files from the <a href=\"https://www.sec.gov/dera/data/edgar-log-file-data-set.html\" target=\"_blank\">US Securities and Exchange Commission website</a> to do a basic ETL for a day of server activity. You will extract the fields of interest and load them into persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>zone</th>\n",
       "      <th>cik</th>\n",
       "      <th>accession</th>\n",
       "      <th>extention</th>\n",
       "      <th>code</th>\n",
       "      <th>size</th>\n",
       "      <th>idx</th>\n",
       "      <th>norefer</th>\n",
       "      <th>noagent</th>\n",
       "      <th>find</th>\n",
       "      <th>crawler</th>\n",
       "      <th>browser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.71.41.ihh</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1437491.0</td>\n",
       "      <td>0001245105-17-000052</td>\n",
       "      <td>xslF345X03/primary_doc.xml</td>\n",
       "      <td>301.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.196.240.dda</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1270985.0</td>\n",
       "      <td>0001188112-04-001037</td>\n",
       "      <td>.txt</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107.23.85.jfd</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1059376.0</td>\n",
       "      <td>0000905148-07-006108</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2727.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107.23.85.jfd</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1059376.0</td>\n",
       "      <td>0000905148-08-001993</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107.23.85.jfd</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1059376.0</td>\n",
       "      <td>0001104659-09-046963</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2715.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>107.23.85.jfd</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1364986.0</td>\n",
       "      <td>0000914121-06-002243</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2786.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107.23.85.jfd</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1364986.0</td>\n",
       "      <td>0000914121-06-002251</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108.240.248.gha</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1540159.0</td>\n",
       "      <td>0001217160-12-000029</td>\n",
       "      <td>f332scottlease.htm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>49578.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>108.59.8.fef</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>732834.0</td>\n",
       "      <td>0001209191-15-017349</td>\n",
       "      <td>xslF345X03/doc4.xml</td>\n",
       "      <td>301.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>108.91.91.hbc</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1629769.0</td>\n",
       "      <td>0001209191-17-023204</td>\n",
       "      <td>.txt</td>\n",
       "      <td>301.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ip        date      time zone        cik  \\\n",
       "0    101.71.41.ihh  2017-03-29  00:00:00  0.0  1437491.0   \n",
       "1  104.196.240.dda  2017-03-29  00:00:00  0.0  1270985.0   \n",
       "2    107.23.85.jfd  2017-03-29  00:00:00  0.0  1059376.0   \n",
       "3    107.23.85.jfd  2017-03-29  00:00:00  0.0  1059376.0   \n",
       "4    107.23.85.jfd  2017-03-29  00:00:00  0.0  1059376.0   \n",
       "5    107.23.85.jfd  2017-03-29  00:00:00  0.0  1364986.0   \n",
       "6    107.23.85.jfd  2017-03-29  00:00:00  0.0  1364986.0   \n",
       "7  108.240.248.gha  2017-03-29  00:00:00  0.0  1540159.0   \n",
       "8     108.59.8.fef  2017-03-29  00:00:00  0.0   732834.0   \n",
       "9    108.91.91.hbc  2017-03-29  00:00:00  0.0  1629769.0   \n",
       "\n",
       "              accession                   extention   code     size  idx  \\\n",
       "0  0001245105-17-000052  xslF345X03/primary_doc.xml  301.0    687.0  0.0   \n",
       "1  0001188112-04-001037                        .txt  200.0   7619.0  0.0   \n",
       "2  0000905148-07-006108                  -index.htm  200.0   2727.0  1.0   \n",
       "3  0000905148-08-001993                  -index.htm  200.0   2710.0  1.0   \n",
       "4  0001104659-09-046963                  -index.htm  200.0   2715.0  1.0   \n",
       "5  0000914121-06-002243                  -index.htm  200.0   2786.0  1.0   \n",
       "6  0000914121-06-002251                  -index.htm  200.0   2784.0  1.0   \n",
       "7  0001217160-12-000029          f332scottlease.htm  200.0  49578.0  0.0   \n",
       "8  0001209191-15-017349         xslF345X03/doc4.xml  301.0    673.0  0.0   \n",
       "9  0001209191-17-023204                        .txt  301.0    675.0  0.0   \n",
       "\n",
       "  norefer noagent  find crawler browser  \n",
       "0     0.0     0.0  10.0     0.0    None  \n",
       "1     0.0     0.0  10.0     0.0    None  \n",
       "2     0.0     0.0  10.0     0.0    None  \n",
       "3     0.0     0.0  10.0     0.0    None  \n",
       "4     0.0     0.0  10.0     0.0    None  \n",
       "5     0.0     0.0  10.0     0.0    None  \n",
       "6     0.0     0.0  10.0     0.0    None  \n",
       "7     0.0     0.0  10.0     0.0    None  \n",
       "8     0.0     0.0  10.0     0.0    None  \n",
       "9     0.0     0.0  10.0     0.0    None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"s3a://data.intellinum.co/bootcamp/common/EDGAR-Log-20170329/EDGAR-Log-20170329.csv\"\n",
    "\n",
    "logDF = (spark\n",
    "  .read\n",
    "  .option(\"header\", True)\n",
    "  .csv(path)\n",
    "  .sample(withReplacement=False, fraction=0.3, seed=3) # using a sample to reduce data size\n",
    ")\n",
    "\n",
    "display(logDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, review the server-side errors, which have error codes in the 500s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>extention</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:12</td>\n",
       "      <td>.txt</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:16</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:24</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:00:44</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:01:01</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:01:01</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:01:02</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:01:03</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:01:03</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>00:01:04</td>\n",
       "      <td>-index.htm</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time   extention   code\n",
       "0  2017-03-29  00:00:12        .txt  503.0\n",
       "1  2017-03-29  00:00:16  -index.htm  503.0\n",
       "2  2017-03-29  00:00:24  -index.htm  503.0\n",
       "3  2017-03-29  00:00:44  -index.htm  503.0\n",
       "4  2017-03-29  00:01:01  -index.htm  503.0\n",
       "5  2017-03-29  00:01:01  -index.htm  503.0\n",
       "6  2017-03-29  00:01:02  -index.htm  503.0\n",
       "7  2017-03-29  00:01:03  -index.htm  503.0\n",
       "8  2017-03-29  00:01:03  -index.htm  503.0\n",
       "9  2017-03-29  00:01:04  -index.htm  503.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "serverErrorDF = (logDF\n",
    "  .filter((col(\"code\") >= 500) & (col(\"code\") < 600))\n",
    "  .select(\"date\", \"time\", \"extention\", \"code\")\n",
    ")\n",
    "\n",
    "display(serverErrorDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "One aspect of ETL jobs is to validate that the data is what you expect.  This includes:<br><br>\n",
    "* Approximately the expected number of records\n",
    "* The expected fields are present\n",
    "* No unexpected missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the server-side errors by hour to confirm the data meets your expectations. Visualize it by using matplotlib or plotly. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour  count\n",
       "0      0   2094\n",
       "1      1   1598\n",
       "2      2   1136\n",
       "3      3   1027\n",
       "4      4   1135\n",
       "5      5   1182\n",
       "6      6   1109\n",
       "7      7   1004\n",
       "8      8   1127\n",
       "9      9   1053\n",
       "10    10   1063\n",
       "11    11   1099\n",
       "12    12   1008\n",
       "13    13   1099\n",
       "14    14   1141\n",
       "15    15   1248\n",
       "16    16   1275\n",
       "17    17   1384\n",
       "18    18   1172\n",
       "19    19   1244"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, hour, col\n",
    "\n",
    "countsDF = (serverErrorDF\n",
    "  .select(hour(from_utc_timestamp(col(\"time\"), \"GMT\")).alias(\"hour\"))\n",
    "  .groupBy(\"hour\")\n",
    "  .count()\n",
    "  .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "display(countsDF, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "name": "Error code count by the hour",
         "type": "bar",
         "uid": "41c07c93-73b4-4622-b466-69f8a0f5317e",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          2094,
          1598,
          1136,
          1027,
          1135,
          1182,
          1109,
          1004,
          1127,
          1053,
          1063,
          1099,
          1008,
          1099,
          1141,
          1248,
          1275,
          1384,
          1172,
          1244,
          1279,
          1130,
          1077,
          1090
         ]
        }
       ],
       "layout": {
        "bargap": 0.15,
        "bargroupgap": 0.1,
        "barmode": "group",
        "legend": {
         "x": 1,
         "y": 1
        },
        "title": {
         "text": "Error code count by the hour"
        },
        "xaxis": {
         "tickfont": {
          "size": 14
         },
         "title": {
          "text": "Hour"
         }
        },
        "yaxis": {
         "tickfont": {
          "size": 10
         },
         "title": {
          "text": "Count"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"e783aa06-471f-4ac0-983b-63caf2ecaf6b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"e783aa06-471f-4ac0-983b-63caf2ecaf6b\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'e783aa06-471f-4ac0-983b-63caf2ecaf6b',\n",
       "                        [{\"name\": \"Error code count by the hour\", \"type\": \"bar\", \"uid\": \"6e13c8d6-2f1e-4040-b55f-a39115e6982d\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], \"y\": [2094, 1598, 1136, 1027, 1135, 1182, 1109, 1004, 1127, 1053, 1063, 1099, 1008, 1099, 1141, 1248, 1275, 1384, 1172, 1244, 1279, 1130, 1077, 1090]}],\n",
       "                        {\"bargap\": 0.15, \"bargroupgap\": 0.1, \"barmode\": \"group\", \"legend\": {\"x\": 1, \"y\": 1}, \"title\": {\"text\": \"Error code count by the hour\"}, \"xaxis\": {\"tickfont\": {\"size\": 14}, \"title\": {\"text\": \"Hour\"}}, \"yaxis\": {\"tickfont\": {\"size\": 10}, \"title\": {\"text\": \"Count\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e783aa06-471f-4ac0-983b-63caf2ecaf6b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "errorByHourDF = countsDF.toPandas()\n",
    "\n",
    "plotData = go.Bar(\n",
    "                x = errorByHourDF[\"hour\"],\n",
    "                y = errorByHourDF[\"count\"],\n",
    "                name = \"Error code count by the hour\",\n",
    "            )\n",
    "\n",
    "layout = go.Layout(\n",
    "                title=\"Error code count by the hour\",\n",
    "                xaxis={\n",
    "                    \"title\" : \"Hour\",\n",
    "                    \"tickfont\" : {\n",
    "                        \"size\" : 14,\n",
    "                    }\n",
    "                },\n",
    "                yaxis={\n",
    "                    \"title\" : \"Count\",\n",
    "                    \"tickfont\" : {\n",
    "                        \"size\" : 10,\n",
    "                    }\n",
    "                },\n",
    "                legend = {\n",
    "                    \"x\" : 1,\n",
    "                    \"y\" : 1\n",
    "                },\n",
    "                barmode=\"group\",\n",
    "                bargap=0.15,\n",
    "                bargroupgap=0.1\n",
    "            )\n",
    "\n",
    "fig = go.Figure(data=[plotData], layout=layout)\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of errors by hour meets the expectations.  There is an uptick in errors around midnight, possibly due to server maintenance at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Back to S3\n",
    "\n",
    "A common and highly effective design pattern in the Spark ecosystem involves loading structured data back to S3 as a parquet file. Learn more about [the scalable and optimized data storage format parquet here](http://parquet.apache.org/).\n",
    "\n",
    "Save the parsed DataFrame back to S3 as parquet using the `.write` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "username = \"rajeev\"\n",
    "(serverErrorDF\n",
    "  .write\n",
    "  .mode(\"overwrite\") # overwrites a file if it already exists\n",
    "  .parquet(\"s3a://temp.intellinum.co/\" + username + \"/serverErrorDF.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our ETL Pipeline\n",
    "\n",
    "Here's what the ETL pipeline you just built looks like.  In the rest of this course you will work with more complex versions of this general pattern.\n",
    "\n",
    "| Code | Stage |\n",
    "|:------|:-------|\n",
    "| `logDF = (spark`                                                                          | Extract |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;`.read`                                                           | Extract |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;`.option(\"header\", True)`                                         | Extract |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;`.csv(<source>)`                                                  | Extract |\n",
    "| `)`                                                                                       | Extract |\n",
    "| `serverErrorDF = (logDF`                                                                  | Transform |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;`.filter((col(\"code\") >= 500) & (col(\"code\") < 600))`             | Transform |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;`.select(\"date\", \"time\", \"extention\", \"code\")`                    | Transform |\n",
    "| `)`                                                                                       | Transform |\n",
    "| `(serverErrorDF.write`                                                                 | Load |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;`.parquet(<destination>))`                                      | Load |\n",
    "\n",
    "This is a distributed job, so it can easily scale to fit the demands of your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Perform an ETL Job\n",
    "\n",
    "Write a basic ETL script that captures the 20 most active website users and load the results to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "from pyspark.sql.functions import desc, count\n",
    "\n",
    "ipCountDF = (logDF.groupBy(\"ip\").agg(count(\"ip\").alias(\"count\")).orderBy(desc(\"count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213.152.28.bhe</td>\n",
       "      <td>520522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158.132.91.haf</td>\n",
       "      <td>497817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.91.6.caf</td>\n",
       "      <td>239925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132.195.122.djf</td>\n",
       "      <td>197673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.91.2.aha</td>\n",
       "      <td>152204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>173.52.208.ehd</td>\n",
       "      <td>146972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>108.91.91.hbc</td>\n",
       "      <td>142731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>117.91.7.hgh</td>\n",
       "      <td>133579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97.100.78.cjb</td>\n",
       "      <td>129753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>217.174.255.dgd</td>\n",
       "      <td>123177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ip   count\n",
       "0   213.152.28.bhe  520522\n",
       "1   158.132.91.haf  497817\n",
       "2     117.91.6.caf  239925\n",
       "3  132.195.122.djf  197673\n",
       "4     117.91.2.aha  152204\n",
       "5   173.52.208.ehd  146972\n",
       "6    108.91.91.hbc  142731\n",
       "7     117.91.7.hgh  133579\n",
       "8    97.100.78.cjb  129753\n",
       "9  217.174.255.dgd  123177"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(ipCountDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "name": "Most Active Users",
         "type": "bar",
         "uid": "45eaa1ba-55bc-4510-aa50-a6f1a551082e",
         "x": [
          "213.152.28.bhe",
          "158.132.91.haf",
          "117.91.6.caf",
          "132.195.122.djf",
          "117.91.2.aha",
          "173.52.208.ehd",
          "108.91.91.hbc",
          "117.91.7.hgh",
          "97.100.78.cjb",
          "217.174.255.dgd",
          "64.140.243.cgg",
          "117.91.6.jfd",
          "117.91.7.bhf",
          "54.69.84.iji",
          "54.212.94.jcd",
          "101.231.120.dfg",
          "54.92.220.eij",
          "132.200.132.fdh",
          "117.91.6.dgd",
          "116.231.50.gae"
         ],
         "y": [
          520522,
          497817,
          239925,
          197673,
          152204,
          146972,
          142731,
          133579,
          129753,
          123177,
          121052,
          119873,
          111371,
          110040,
          104801,
          89679,
          84153,
          82639,
          72290,
          67585
         ]
        }
       ],
       "layout": {
        "bargap": 0.15,
        "bargroupgap": 0.1,
        "barmode": "group",
        "legend": {
         "x": 1,
         "y": 1
        },
        "title": {
         "text": "Most Active Users"
        },
        "xaxis": {
         "tickfont": {
          "size": 14
         },
         "title": {
          "text": "Hour"
         }
        },
        "yaxis": {
         "tickfont": {
          "size": 10
         },
         "title": {
          "text": "Count"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"860a8ace-1daf-4ec1-8323-d66eba95b117\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"860a8ace-1daf-4ec1-8323-d66eba95b117\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '860a8ace-1daf-4ec1-8323-d66eba95b117',\n",
       "                        [{\"name\": \"Most Active Users\", \"type\": \"bar\", \"uid\": \"5cf7937d-8079-421f-925a-51db9bbd35a0\", \"x\": [\"213.152.28.bhe\", \"158.132.91.haf\", \"117.91.6.caf\", \"132.195.122.djf\", \"117.91.2.aha\", \"173.52.208.ehd\", \"108.91.91.hbc\", \"117.91.7.hgh\", \"97.100.78.cjb\", \"217.174.255.dgd\", \"64.140.243.cgg\", \"117.91.6.jfd\", \"117.91.7.bhf\", \"54.69.84.iji\", \"54.212.94.jcd\", \"101.231.120.dfg\", \"54.92.220.eij\", \"132.200.132.fdh\", \"117.91.6.dgd\", \"116.231.50.gae\"], \"y\": [520522, 497817, 239925, 197673, 152204, 146972, 142731, 133579, 129753, 123177, 121052, 119873, 111371, 110040, 104801, 89679, 84153, 82639, 72290, 67585]}],\n",
       "                        {\"bargap\": 0.15, \"bargroupgap\": 0.1, \"barmode\": \"group\", \"legend\": {\"x\": 1, \"y\": 1}, \"title\": {\"text\": \"Most Active Users\"}, \"xaxis\": {\"tickfont\": {\"size\": 14}, \"title\": {\"text\": \"Hour\"}}, \"yaxis\": {\"tickfont\": {\"size\": 10}, \"title\": {\"text\": \"Count\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('860a8ace-1daf-4ec1-8323-d66eba95b117');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top20IpDF = ipCountDF.limit(20).toPandas()\n",
    "\n",
    "plotData = go.Bar(\n",
    "                x = top20IpDF[\"ip\"],\n",
    "                y = top20IpDF[\"count\"],\n",
    "                name = \"Most Active Users\",\n",
    "            )\n",
    "\n",
    "layout = go.Layout(\n",
    "                title=\"Most Active Users\",\n",
    "                xaxis={\n",
    "                    \"title\" : \"Hour\",\n",
    "                    \"tickfont\" : {\n",
    "                        \"size\" : 14,\n",
    "                    }\n",
    "                },\n",
    "                yaxis={\n",
    "                    \"title\" : \"Count\",\n",
    "                    \"tickfont\" : {\n",
    "                        \"size\" : 10,\n",
    "                    }\n",
    "                },\n",
    "                legend = {\n",
    "                    \"x\" : 1,\n",
    "                    \"y\" : 1\n",
    "                },\n",
    "                barmode=\"group\",\n",
    "                bargap=0.15,\n",
    "                bargroupgap=0.1\n",
    "            )\n",
    "\n",
    "fig = go.Figure(data=[plotData], layout=layout)\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "ip1, count1 = ipCountDF.first()\n",
    "cols = set(ipCountDF.columns)\n",
    "\n",
    "dfTest(\"ET1-P-02-01-01\", \"213.152.28.bhe\", ip1)\n",
    "dfTest(\"ET1-P-02-01-02\", True, count1 > 500000 and count1 < 550000)\n",
    "dfTest(\"ET1-P-02-01-03\", True, 'count' in cols)\n",
    "dfTest(\"ET1-P-02-01-03\", True, 'ip' in cols)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Save the Results\n",
    "\n",
    "Use your temporary folder to save the results back to S3 as `\"s3a://temp.intellinum.co/\" + username + \"/ipCount.parquet\"`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "username = \"rajeev\"\n",
    "path = \"s3a://temp.intellinum.co/\" + username + \"/ipCount.parquet\"\n",
    "(ipCountDF\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "username = \"rajeev\"\n",
    "writePath = \"s3a://temp.intellinum.co/\" + username + \"/ipCount.parquet\"\n",
    "\n",
    "ipCountDF2 = (spark\n",
    "  .read\n",
    "  .parquet(writePath)\n",
    "  .orderBy(desc(\"count\"))\n",
    ")\n",
    "ip1, count1 = ipCountDF2.first()\n",
    "cols = ipCountDF2.columns\n",
    "\n",
    "dfTest(\"ET1-P-02-02-01\", \"213.152.28.bhe\", ip1)\n",
    "dfTest(\"ET1-P-02-02-02\", True, count1 > 500000 and count1 < 550000)\n",
    "dfTest(\"ET1-P-02-02-03\", True, \"count\" in cols)\n",
    "dfTest(\"ET1-P-02-02-04\", True, \"ip\" in cols)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the load worked by using `aws s3 ls <path>`.  Parquet divides your data into a number of files.  If successful, you see a `_SUCCESS` file as well as the data split across a number of parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE ipCount.parquet/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {writePath.replace('s3a','s3')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What does ETL stand for and what are the stages of the process?  \n",
    "**Answer:** ETL stands for `extract-transform-load`\n",
    "0. *Extract* refers to ingesting data.  Spark easily connects to data in a number of different sources.\n",
    "0. *Transform* refers to applying structure, parsing fields, cleaning data, and/or computing statistics.\n",
    "0. *Load* refers to loading data to its final destination, usually a database or data warehouse.\n",
    "\n",
    "**Question:** How does the Spark approach to ETL deal with devops issues such as updating a software version?  \n",
    "**Answer:** By decoupling storage and compute, updating your Spark version is as easy as spinning up a new cluster.  Your old code will easily connect to S3, the Azure Blob, or other storage.  This also avoids the challenge of keeping a cluster always running, such as with Hadoop clusters.\n",
    "\n",
    "**Question:** How does the Spark approach to data applications differ from other solutions?  \n",
    "**Answer:** Spark offers a unified solution to use cases that would otherwise need individual tools. For instance, Spark combines machine learning, ETL, stream processing, and a number of other solutions all with one technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Topics & Resources\n",
    "\n",
    "**Q:** Where can I get more information on building ETL pipelines?  \n",
    "**A:** Check out the Spark Summit talk on <a href=\"https://databricks.com/session/building-robust-etl-pipelines-with-apache-spark\" target=\"_blank\">Building Robust ETL Pipelines with Apache Spark</a>\n",
    "\n",
    "**Q:** Where can I find out more information on moving from traditional ETL pipelines towards Spark?  \n",
    "**A:** Check out the Spark Summit talk <a href=\"https://databricks.com/session/get-rid-of-traditional-etl-move-to-spark\" target=\"_blank\">Get Rid of Traditional ETL, Move to Spark!</a>\n",
    "\n",
    "**Q:** What are the visualization options?  \n",
    "**A:** 3rd party visualization libraries, including <a href=\"https://d3js.org/\" target=\"_blank\">d3.js</a>, <a href=\"https://matplotlib.org/\" target=\"_blank\">matplotlib</a>, <a href=\"http://ggplot.yhathq.com/\" target=\"_blank\">ggplot</a>, and <a href=\"https://plot.ly/\" target=\"_blank\">plotly<a/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
