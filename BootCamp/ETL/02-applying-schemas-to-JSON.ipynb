{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Schemas to JSON Data\n",
    "\n",
    "Apache Spark&trade; provide a number of ways to project structure onto semi-structured data allowing for quick and easy access.\n",
    "## In this lesson you:\n",
    "* Infer the schema from JSON files\n",
    "* Create and use a user-defined schema with primitive data types\n",
    "* Use non-primitive data types such as `ArrayType` and `MapType` in a schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "Schemas are at the heart of data structures in Spark.\n",
    "**A schema describes the structure of your data by naming columns and declaring the type of data in that column.** \n",
    "Rigorously enforcing schemas leads to significant performance optimizations and reliability of code.\n",
    "\n",
    "Why is open source Spark so fast? While there are many reasons for these performance improvements, two key reasons are:<br><br>\n",
    "* First and foremost, Spark runs first in memory rather than reading and writing to disk. \n",
    "* Second, using DataFrames allows Spark to optimize the execution of your queries because it knows what your data looks like.\n",
    "\n",
    "Two pillars of computer science education are data structures, the organization and storage of data and algorithms, and the computational procedures on that data.  A rigorous understanding of computer science involves both of these domains. When you apply the most relevant data structures, the algorithms that carry out the computation become significantly more eloquent.\n",
    "\n",
    "In the road map for ETL, this is the **Apply Schema** step:\n",
    "\n",
    "<img src=\"../../resources/ETL-Process-2.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas with Semi-Structured JSON Data\n",
    "\n",
    "**Tabular data**, such as that found in CSV files or relational databases, has a formal structure where each observation, or row, of the data has a value (even if it's a NULL value) for each feature, or column, in the data set.  \n",
    "\n",
    "**Semi-structured data** does not need to conform to a formal data model. Instead, a given feature may appear zero, once, or many times for a given observation.  \n",
    "\n",
    "Semi-structured data storage works well with hierarchical data and with schemas that may evolve over time.  One of the most common forms of semi-structured data is JSON data, which consists of attribute-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped a SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://yuan.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_02_applying_schemas_to_json\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_02_applying_schemas_to_json\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://yuan.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first few lines of a JSON file holding ZIP Code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 10:09:11    3182409 zips.json\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://data.intellinum.co/bootcamp/common/zips.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Inference\n",
    "\n",
    "Import data as a DataFrame and view its schema with the `printSchema()` DataFrame method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- loc: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- pop: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zipsDF = spark.read.json(\"s3a://data.intellinum.co/bootcamp/common/zips.json\")\n",
    "zipsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the schema as an object by calling `.schema` on a DataFrame. Schemas consist of a `StructType`, which is a collection of `StructField`s.  Each `StructField` gives a name and a type for a given field in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.StructType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[StructField(_id,StringType,true),\n",
       " StructField(city,StringType,true),\n",
       " StructField(loc,ArrayType(DoubleType,true),true),\n",
       " StructField(pop,LongType,true),\n",
       " StructField(state,StringType,true)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipsSchema = zipsDF.schema\n",
    "print(type(zipsSchema))\n",
    "\n",
    "[field for field in zipsSchema]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Schemas\n",
    "\n",
    "Spark infers schemas from the data, as detailed in the example above.  Challenges with inferred schemas include:  \n",
    "<br>\n",
    "* Schema inference means Spark scans all of your data, creating an extra job, which can affect performance\n",
    "* Consider providing alternative data types (for example, change a `Long` to a `Integer`)\n",
    "* Consider throwing out certain fields in the data, to read only the data of interest\n",
    "\n",
    "To define schemas, build a `StructType` composed of `StructField`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary types from the `types` module. Build a `StructType`, which takes a list of `StructField`s.  Each `StructField` takes three arguments: the name of the field, the type of data in it, and a `Boolean` for whether this field can be `Null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "zipsSchema2 = StructType([\n",
    "  StructField(\"city\", StringType(), True), \n",
    "  StructField(\"pop\", IntegerType(), True) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the schema using the `.schema` method. This `read` returns only  the columns specified in the schema and changes the column `pop` from `LongType` (which was inferred above) to `IntegerType`.\n",
    "\n",
    "A `LongType` is an 8-byte integer ranging up to 9,223,372,036,854,775,807 while `IntegerType` is a 4-byte integer ranging up to 2,147,483,647.  Since no American city has over two billion people, `IntegerType` is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGAWAM</td>\n",
       "      <td>15338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUSHMAN</td>\n",
       "      <td>36963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BARRE</td>\n",
       "      <td>4546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BELCHERTOWN</td>\n",
       "      <td>10579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLANDFORD</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BRIMFIELD</td>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHESTER</td>\n",
       "      <td>1688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHESTERFIELD</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHICOPEE</td>\n",
       "      <td>23396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHICOPEE</td>\n",
       "      <td>31495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city    pop\n",
       "0        AGAWAM  15338\n",
       "1       CUSHMAN  36963\n",
       "2         BARRE   4546\n",
       "3   BELCHERTOWN  10579\n",
       "4     BLANDFORD   1240\n",
       "5     BRIMFIELD   3706\n",
       "6       CHESTER   1688\n",
       "7  CHESTERFIELD    177\n",
       "8      CHICOPEE  23396\n",
       "9      CHICOPEE  31495"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipsDF2 = (spark.read\n",
    "  .schema(zipsSchema2)\n",
    "  .json(\"s3a://data.intellinum.co/bootcamp/common/zips.json\")\n",
    ")\n",
    "\n",
    "display(zipsDF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primitive and Non-primitive Types\n",
    "\n",
    "The Spark [`types` package](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) provides the building blocks for constructing schemas.\n",
    "\n",
    "A primitive type contains the data itself.  The most common primitive types include:\n",
    "\n",
    "| Numeric | General | Time |\n",
    "|-----|-----|\n",
    "| `FloatType` | `StringType` | `TimestampType` | \n",
    "| `IntegerType` | `BooleanType` | `DateType` | \n",
    "| `DoubleType` | `NullType` | |\n",
    "| `LongType` | | |\n",
    "| `ShortType` |  | |\n",
    "\n",
    "Non-primitive types are sometimes called reference variables or composite types.  Technically, non-primitive types contain references to memory locations and not the data itself.  Non-primitive types are the composite of a number of primitive types such as an Array of the primitive type `Integer`.\n",
    "\n",
    "The two most common composite types are `ArrayType` and `MapType`. These types allow for a given field to contain an arbitrary number of elements in either an Array/List or Map/Dictionary form.\n",
    "\n",
    "See the [Spark documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types) for a complete picture of types in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ZIP Code dataset contains an array with the latitude and longitude of the cities.  Use an `ArrayType`, which takes the primitive type of its elements as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType\n",
    "\n",
    "zipsSchema3 = StructType([\n",
    "  StructField(\"city\", StringType(), True), \n",
    "  StructField(\"loc\", \n",
    "    ArrayType(FloatType(), True), True),\n",
    "  StructField(\"pop\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the schema using the `.schema()` method and observe the results.  Expand the array values in the column `loc` to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>loc</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGAWAM</td>\n",
       "      <td>[-72.62274169921875, 42.07020568847656]</td>\n",
       "      <td>15338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUSHMAN</td>\n",
       "      <td>[-72.5156478881836, 42.377017974853516]</td>\n",
       "      <td>36963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BARRE</td>\n",
       "      <td>[-72.10835266113281, 42.409698486328125]</td>\n",
       "      <td>4546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BELCHERTOWN</td>\n",
       "      <td>[-72.41094970703125, 42.27510452270508]</td>\n",
       "      <td>10579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLANDFORD</td>\n",
       "      <td>[-72.93611145019531, 42.18294906616211]</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BRIMFIELD</td>\n",
       "      <td>[-72.1884536743164, 42.11654281616211]</td>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHESTER</td>\n",
       "      <td>[-72.98876190185547, 42.279422760009766]</td>\n",
       "      <td>1688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHESTERFIELD</td>\n",
       "      <td>[-72.83330535888672, 42.38167190551758]</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHICOPEE</td>\n",
       "      <td>[-72.60796356201172, 42.162044525146484]</td>\n",
       "      <td>23396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHICOPEE</td>\n",
       "      <td>[-72.57614135742188, 42.17644119262695]</td>\n",
       "      <td>31495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city                                       loc    pop\n",
       "0        AGAWAM   [-72.62274169921875, 42.07020568847656]  15338\n",
       "1       CUSHMAN   [-72.5156478881836, 42.377017974853516]  36963\n",
       "2         BARRE  [-72.10835266113281, 42.409698486328125]   4546\n",
       "3   BELCHERTOWN   [-72.41094970703125, 42.27510452270508]  10579\n",
       "4     BLANDFORD   [-72.93611145019531, 42.18294906616211]   1240\n",
       "5     BRIMFIELD    [-72.1884536743164, 42.11654281616211]   3706\n",
       "6       CHESTER  [-72.98876190185547, 42.279422760009766]   1688\n",
       "7  CHESTERFIELD   [-72.83330535888672, 42.38167190551758]    177\n",
       "8      CHICOPEE  [-72.60796356201172, 42.162044525146484]  23396\n",
       "9      CHICOPEE   [-72.57614135742188, 42.17644119262695]  31495"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipsDF3 = (spark.read\n",
    "  .schema(zipsSchema3)\n",
    "  .json(\"s3a://data.intellinum.co/bootcamp/common/zips.json\")\n",
    ")\n",
    "display(zipsDF3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploring JSON Data\n",
    "\n",
    "<a href=\"https://archive.ics.uci.edu/ml/datasets/UbiqLog+(smartphone+lifelogging)\">Smartphone data from UCI Machine Learning Repository</a> is available under `s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI`. This is log data from the open source project [Ubiqlog](https://github.com/Rezar/Ubiqlog).\n",
    "\n",
    "Import this data and define your own schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the Data\n",
    "\n",
    "Import data from `s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*`. (This is the log files from a given user.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the head of one file from the data set using AWS CLI.  Use `s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log_1-6-2014.txt`.\n",
    "\n",
    "**Hint:** You can google `AWS S3 head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"AcceptRanges\": \"bytes\",\r\n",
      "    \"LastModified\": \"Wed, 05 Jun 2019 10:14:36 GMT\",\r\n",
      "    \"ContentLength\": 20044,\r\n",
      "    \"ETag\": \"\\\"fa1e90a04bdc00b9a2df2754444454b5\\\"\",\r\n",
      "    \"ContentType\": \"text/plain\",\r\n",
      "    \"Metadata\": {}\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "!aws s3api head-object --bucket data.intellinum.co --key bootcamp/common/UbiqLog4UCI/14_F/log_1-6-2014.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data and save it to `smartphoneDF`. Read the logs using a `*` in your path like `s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "smartphoneDF = spark.read.json(\"s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Application</th>\n",
       "      <th>Bluetooth</th>\n",
       "      <th>Call</th>\n",
       "      <th>Location</th>\n",
       "      <th>SMS</th>\n",
       "      <th>WiFi</th>\n",
       "      <th>_corrupt_record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(12-9-2013 21:30:02, com.android.settings, 12-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(12-9-2013 21:29:58, com.farsitel.bazaar, 12-9...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(12-9-2013 21:25:17, com.android.packageinstal...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(12-9-2013 21:25:11, com.farsitel.bazaar, 12-9...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(12-9-2013 21:21:00, com.android.browser, 12-9...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(12-9-2013 21:20:46, com.farsitel.bazaar, 12-9...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(12-9-2013 21:19:56, com.google.android.gms.ui...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(12-9-2013 21:19:40, com.google.process.locati...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(12-9-2013 21:19:30, com.farsitel.bazaar, 12-9...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(12-9-2013 21:17:50, com.android.browser, 12-9...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Application Bluetooth  Call Location  \\\n",
       "0  (12-9-2013 21:30:02, com.android.settings, 12-...      None  None     None   \n",
       "1  (12-9-2013 21:29:58, com.farsitel.bazaar, 12-9...      None  None     None   \n",
       "2  (12-9-2013 21:25:17, com.android.packageinstal...      None  None     None   \n",
       "3  (12-9-2013 21:25:11, com.farsitel.bazaar, 12-9...      None  None     None   \n",
       "4  (12-9-2013 21:21:00, com.android.browser, 12-9...      None  None     None   \n",
       "5  (12-9-2013 21:20:46, com.farsitel.bazaar, 12-9...      None  None     None   \n",
       "6  (12-9-2013 21:19:56, com.google.android.gms.ui...      None  None     None   \n",
       "7  (12-9-2013 21:19:40, com.google.process.locati...      None  None     None   \n",
       "8  (12-9-2013 21:19:30, com.farsitel.bazaar, 12-9...      None  None     None   \n",
       "9  (12-9-2013 21:17:50, com.android.browser, 12-9...      None  None     None   \n",
       "\n",
       "    SMS  WiFi _corrupt_record  \n",
       "0  None  None            None  \n",
       "1  None  None            None  \n",
       "2  None  None            None  \n",
       "3  None  None            None  \n",
       "4  None  None            None  \n",
       "5  None  None            None  \n",
       "6  None  None            None  \n",
       "7  None  None            None  \n",
       "8  None  None            None  \n",
       "9  None  None            None  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "display(smartphoneDF.orderBy(desc(\"Application\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "cols = set(smartphoneDF.columns)\n",
    "sample = smartphoneDF.orderBy(desc(\"Application\")).first()[0][0]\n",
    "\n",
    "dfTest(\"ET1-P-05-01-01\", 25372, smartphoneDF.count())\n",
    "dfTest(\"ET1-P-05-01-02\", '12-9-2013 21:30:02', sample)\n",
    "\n",
    "dfTest(\"ET1-P-05-01-03\", True, \"Location\" in cols)\n",
    "dfTest(\"ET1-P-05-01-04\", True, \"SMS\" in cols)\n",
    "dfTest(\"ET1-P-05-01-05\", True, \"WiFi\" in cols)\n",
    "dfTest(\"ET1-P-05-01-06\", True, \"_corrupt_record\" in cols)\n",
    "dfTest(\"ET1-P-05-01-07\", True, \"Application\" in cols)\n",
    "dfTest(\"ET1-P-05-01-08\", True, \"Call\" in cols)\n",
    "dfTest(\"ET1-P-05-01-09\", True, \"Bluetooth\" in cols)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore the Inferred Schema\n",
    "\n",
    "Print the schema to get a sense for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Application: struct (nullable = true)\n",
      " |    |-- End: string (nullable = true)\n",
      " |    |-- ProcessName: string (nullable = true)\n",
      " |    |-- Start: string (nullable = true)\n",
      " |-- Bluetooth: struct (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- bond status: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- Call: struct (nullable = true)\n",
      " |    |-- Duration: string (nullable = true)\n",
      " |    |-- Number: string (nullable = true)\n",
      " |    |-- Time: string (nullable = true)\n",
      " |    |-- Type: string (nullable = true)\n",
      " |-- Location: struct (nullable = true)\n",
      " |    |-- Accuracy: string (nullable = true)\n",
      " |    |-- Altitude: string (nullable = true)\n",
      " |    |-- Latitude: string (nullable = true)\n",
      " |    |-- Longtitude: string (nullable = true)\n",
      " |    |-- Provider: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- SMS: struct (nullable = true)\n",
      " |    |-- Address: string (nullable = true)\n",
      " |    |-- Type: string (nullable = true)\n",
      " |    |-- body: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- metadata: struct (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- WiFi: struct (nullable = true)\n",
      " |    |-- BSSID: string (nullable = true)\n",
      " |    |-- SSID: string (nullable = true)\n",
      " |    |-- capabilities: string (nullable = true)\n",
      " |    |-- frequency: string (nullable = true)\n",
      " |    |-- level: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "smartphoneDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.StructType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[StructField(Application,StructType(List(StructField(End,StringType,true),StructField(ProcessName,StringType,true),StructField(Start,StringType,true))),true),\n",
       " StructField(Bluetooth,StructType(List(StructField(address,StringType,true),StructField(bond status,StringType,true),StructField(name,StringType,true),StructField(time,StringType,true))),true),\n",
       " StructField(Call,StructType(List(StructField(Duration,StringType,true),StructField(Number,StringType,true),StructField(Time,StringType,true),StructField(Type,StringType,true))),true),\n",
       " StructField(Location,StructType(List(StructField(Accuracy,StringType,true),StructField(Altitude,StringType,true),StructField(Latitude,StringType,true),StructField(Longtitude,StringType,true),StructField(Provider,StringType,true),StructField(time,StringType,true))),true),\n",
       " StructField(SMS,StructType(List(StructField(Address,StringType,true),StructField(Type,StringType,true),StructField(body,StringType,true),StructField(date,StringType,true),StructField(metadata,StructType(List(StructField(name,StringType,true))),true),StructField(type,StringType,true))),true),\n",
       " StructField(WiFi,StructType(List(StructField(BSSID,StringType,true),StructField(SSID,StringType,true),StructField(capabilities,StringType,true),StructField(frequency,StringType,true),StructField(level,StringType,true),StructField(time,StringType,true))),true),\n",
       " StructField(_corrupt_record,StringType,true)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartphoneSchema = smartphoneDF.schema\n",
    "print(type(smartphoneSchema))\n",
    "\n",
    "[field for field in smartphoneSchema]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema shows:  \n",
    "\n",
    "* Six categories of tracked data \n",
    "* Nested data structures\n",
    "* A field showing corrupt records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating a User Defined Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Your workflow\n",
    "\n",
    "Often the hardest part of a coding challenge is setting up a workflow to get continuous feedback on what you develop.\n",
    "\n",
    "Start with the import statements you need, including functions from two main packages:\n",
    "\n",
    "| Package | Function |\n",
    "|---------|---------|\n",
    "| `pyspark.sql.types` | `StructType`, `StructField`, `StringType` |\n",
    "| `pyspark.sql.functions` | `col` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SMS** field needs to be parsed. Create a placeholder schema called `schema` that's a `StructType` with one `StructField` named **SMS** of type `StringType`. This imports the entire attribute (even though it contains nested entities) as a String.  \n",
    "\n",
    "This is a way to get a sense for what's in the data and make a progressively more complex schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "schema = StructType([\n",
    "    StructField(\"SMS\" , StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(SMS,StringType,true)]\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "fields = schema.fields\n",
    "print(fields)\n",
    "dfTest(\"ET1-P-05-02-01\", 1, len(fields))\n",
    "dfTest(\"ET1-P-05-02-02\", 'SMS', fields[0].name)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the schema to the data and save the result as `SMSDF`. This closes the loop on which to iterate and develop an increasingly complex schema. The path to the data is `s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*`. \n",
    "\n",
    "Include only records where the column `SMS` is not `Null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "schema = StructType([\n",
    "  StructField(\"SMS\", StringType(), False)\n",
    "])\n",
    "\n",
    "SMSDF = (spark\n",
    "         .read\n",
    "         .schema(schema)\n",
    "         .json('s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*')).filter(\"SMS != 'None' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SMS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SMSDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 SMS|\n",
      "+--------------------+\n",
      "|{\"Address\":\"+9821...|\n",
      "|{\"Address\":\"+9850...|\n",
      "|{\"Address\":\"+9821...|\n",
      "|{\"Address\":\"+9893...|\n",
      "|{\"Address\":\"+9821...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SMSDF.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "cols = SMSDF.columns\n",
    "\n",
    "dfTest(\"ET1-P-05-03-01\", 1147, SMSDF.count())\n",
    "dfTest(\"ET1-P-05-03-02\", ['SMS'], cols)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Full Schema for SMS\n",
    "\n",
    "Define the Schema for the following fields in the `StructType` `SMS` and name it `schema2`.  Apply it to a new DataFrame `SMSDF2`:  \n",
    "<br>\n",
    "* `Address`\n",
    "* `date`\n",
    "* `metadata`\n",
    " - `name`\n",
    " \n",
    "Note there's `Type` and `type`, which appears to be redundant data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, ArrayType\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"SMS\", StructType([\n",
    "        StructField(\"Address\" , StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"metadata\", StructType([\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(SMS,StructType(List(StructField(Address,StringType,true),StructField(date,DateType,true),StructField(metadata,StructType(List(StructField(name,StringType,true))),true))),true)]\n"
     ]
    }
   ],
   "source": [
    "print([field for field in schema2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "readSMSDF2 = (spark\n",
    "          .read\n",
    "          .schema(schema2)\n",
    "          .json('s3://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMSDF2 = readSMSDF2.filter(readSMSDF2.SMS.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SMS: struct (nullable = true)\n",
      " |    |-- Address: string (nullable = true)\n",
      " |    |-- date: date (nullable = true)\n",
      " |    |-- metadata: struct (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SMSDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(+98214428####, 0007-04-04, (,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(+985000406500####, 0007-04-04, (,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(+98214428####, 0007-04-04, (,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(+98939283####, 0007-03-05, (bahram,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(+98214428####, 0007-04-04, (,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(+98939283####, 0007-03-05, (bahram,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(+98935566####, 0007-04-04, (u Kh sevda,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(+98214428####, 0007-04-04, (,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(+981000721670####, 0007-04-04, (,))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(+98935566####, 0007-04-04, (u Kh sevda,))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          SMS\n",
       "0            (+98214428####, 0007-04-04, (,))\n",
       "1        (+985000406500####, 0007-04-04, (,))\n",
       "2            (+98214428####, 0007-04-04, (,))\n",
       "3      (+98939283####, 0007-03-05, (bahram,))\n",
       "4            (+98214428####, 0007-04-04, (,))\n",
       "5      (+98939283####, 0007-03-05, (bahram,))\n",
       "6  (+98935566####, 0007-04-04, (u Kh sevda,))\n",
       "7            (+98214428####, 0007-04-04, (,))\n",
       "8        (+981000721670####, 0007-04-04, (,))\n",
       "9  (+98935566####, 0007-04-04, (u Kh sevda,))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(SMSDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "cols = SMSDF2.columns\n",
    "schemaJson = SMSDF2.schema.json()\n",
    "\n",
    "dfTest(\"ET1-P-05-04-01\", 1147, SMSDF2.count())\n",
    "dfTest(\"ET1-P-05-04-02\", ['SMS'], cols)\n",
    "dfTest(\"ET1-P-05-04-03\", True, 'Address' in schemaJson and 'date' in schemaJson)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compare Solution Performance\n",
    "\n",
    "Compare the dafault schema inference to applying a user defined schema using the `%timeit` function.  Which completed faster?  Which triggered more jobs?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.41 s ± 334 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit SMSDF = spark.read.schema(schema2).json(\"s3a://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.66 s ± 167 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit SMSDF = spark.read.json(\"s3a://data.intellinum.co/bootcamp/common/UbiqLog4UCI/14_F/log*\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing a schema increases performance two to three times, depending on the size of the cluster used. Since Spark doesn't infer the schema, it doesn't have to read through all of the data. This is also why there are fewer jobs when a schema is provided: Spark doesn't need one job for each partition of the data to infer the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "**Question:** What are two ways to attain a schema from data?  \n",
    "**Answer:** Allow Spark to infer a schema from your data or provide a user defined schema. Schema inference is the recommended first step; however, you can customize this schema to your use case with a user defined schema.\n",
    "\n",
    "**Question:** Why should you define your own schema?  \n",
    "**Answer:** Benefits of user defined schemas include:\n",
    "* Avoiding the extra scan of your data needed to infer the schema\n",
    "* Providing alternative data types\n",
    "* Parsing only the fields you need\n",
    "\n",
    "**Question:** Why is JSON a common format in big data pipelines?  \n",
    "**Answer:** Semi-structured data works well with hierarchical data and where schemas need to evolve over time.  It also easily contains composite data types such as arrays and maps.\n",
    "\n",
    "**Question:** By default, how are corrupt records dealt with using `spark.read.json()`?  \n",
    "**Answer:** They appear in a column called `_corrupt_record`.  These are the records that Spark can't read (e.g. when characters are missing from a JSON string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
