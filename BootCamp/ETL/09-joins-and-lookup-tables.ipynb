{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 600px; height: 163px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins and Lookup Tables\n",
    "\n",
    "Apache Spark&trade; allows you to join new records to existing tables in an ETL job.\n",
    "\n",
    "## In this lesson you:\n",
    "* Join new records to a pre-existing lookup table\n",
    "* Employ table join best practices relevant to big data environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Broadcast Joins\n",
    "\n",
    "A common use case in ETL jobs involves joining new data to either lookup tables or historical data. You need different considerations to guide this process when working with distributed technologies such as Spark, rather than traditional databases that sit on a single machine.\n",
    "\n",
    "Traditional databases join tables by pairing values on a given column. When all the data sits in a single database, it often goes unnoticed how computationally expensive row-wise comparisons are.  When data is distributed across a cluster, the expense of joins becomes even more apparent.\n",
    "\n",
    "**A standard (or shuffle) join** moves all the data on the cluster for each table to a given node on the cluster. This is expensive not only because of the computation needed to perform row-wise comparisons, but also because data transfer across a network is often the biggest performance bottleneck of distributed systems.\n",
    "\n",
    "By contrast, **a broadcast join** remedies this situation when one DataFrame is sufficiently small. A broadcast join duplicates the smaller of the two DataFrames on each node of the cluster, avoiding the cost of shuffling the bigger DataFrame.\n",
    "\n",
    "<div><img src=\"../../resources/shuffle-and-broadcast-joins.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Tables\n",
    "\n",
    "Lookup tables are normally small, historical tables used to enrich new data passing through an ETL pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the lab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !aws s3 cp s3://devops.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 12\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_09-joins-and-lookup-tables\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_etl_09-joins-and-lookup-tables\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://devops.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a small table that will enrich new data coming into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow</th>\n",
       "      <th>longName</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>shortName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tue</td>\n",
       "      <td>Tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Wed</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Thr</td>\n",
       "      <td>Th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Fri</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Sa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Su</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow   longName abbreviated shortName\n",
       "0    1     Monday         Mon         M\n",
       "1    2    Tuesday         Tue        Tu\n",
       "2    3  Wednesday         Wed         W\n",
       "3    4   Thursday         Thr        Th\n",
       "4    5     Friday         Fri         F\n",
       "5    6   Saturday         Sat        Sa\n",
       "6    7     Sunday         Sun        Su"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsDF = spark.read.parquet(\"s3://data.intellinum.co/bootcamp/common/day-of-week\")\n",
    "\n",
    "display(labelsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a larger DataFrame that gives a column to combine back to the lookup table. In this case, use Wikipedia site requests data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "      <th>requests</th>\n",
       "      <th>dow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-16T00:09:55</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-16T00:10:39</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-16T00:19:39</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-16T00:38:11</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-16T00:42:40</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-03-16T00:52:24</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-03-16T00:54:16</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-03-16T01:18:11</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-03-16T01:30:32</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-03-16T01:32:24</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     site  requests dow\n",
       "0  2015-03-16T00:09:55   mobile      1595   1\n",
       "1  2015-03-16T00:10:39   mobile      1544   1\n",
       "2  2015-03-16T00:19:39  desktop      2460   1\n",
       "3  2015-03-16T00:38:11  desktop      2237   1\n",
       "4  2015-03-16T00:42:40   mobile      1656   1\n",
       "5  2015-03-16T00:52:24  desktop      2452   1\n",
       "6  2015-03-16T00:54:16   mobile      1654   1\n",
       "7  2015-03-16T01:18:11   mobile      1720   1\n",
       "8  2015-03-16T01:30:32  desktop      2288   1\n",
       "9  2015-03-16T01:32:24   mobile      1609   1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "pageviewsDF = (spark.read\n",
    "  .parquet(\"s3://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second.parquet/\")\n",
    "  .withColumn(\"dow\", date_format(col(\"timestamp\"), \"u\").alias(\"dow\"))\n",
    ")\n",
    "\n",
    "display(pageviewsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two DataFrames together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "      <th>requests</th>\n",
       "      <th>longName</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>shortName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:09:55</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1595</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:10:39</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1544</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:19:39</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2460</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:38:11</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2237</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:42:40</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1656</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:52:24</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2452</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T00:54:16</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1654</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T01:18:11</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1720</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T01:30:32</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2288</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-16T01:32:24</td>\n",
       "      <td>mobile</td>\n",
       "      <td>1609</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dow            timestamp     site  requests longName abbreviated shortName\n",
       "0   1  2015-03-16T00:09:55   mobile      1595   Monday         Mon         M\n",
       "1   1  2015-03-16T00:10:39   mobile      1544   Monday         Mon         M\n",
       "2   1  2015-03-16T00:19:39  desktop      2460   Monday         Mon         M\n",
       "3   1  2015-03-16T00:38:11  desktop      2237   Monday         Mon         M\n",
       "4   1  2015-03-16T00:42:40   mobile      1656   Monday         Mon         M\n",
       "5   1  2015-03-16T00:52:24  desktop      2452   Monday         Mon         M\n",
       "6   1  2015-03-16T00:54:16   mobile      1654   Monday         Mon         M\n",
       "7   1  2015-03-16T01:18:11   mobile      1720   Monday         Mon         M\n",
       "8   1  2015-03-16T01:30:32  desktop      2288   Monday         Mon         M\n",
       "9   1  2015-03-16T01:32:24   mobile      1609   Monday         Mon         M"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageviewsEnhancedDF = pageviewsDF.join(labelsDF, \"dow\")\n",
    "\n",
    "display(pageviewsEnhancedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now aggregate the results to see trends by day of the week.\n",
    "\n",
    ":NOTE: `pageviewsEnhancedDF` is a large DataFrame so it can take a while to process depending on the size of your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow</th>\n",
       "      <th>longName</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>shortName</th>\n",
       "      <th>Requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Mon</td>\n",
       "      <td>M</td>\n",
       "      <td>2356818845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tue</td>\n",
       "      <td>Tu</td>\n",
       "      <td>1995034884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Wed</td>\n",
       "      <td>W</td>\n",
       "      <td>1977615396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Thr</td>\n",
       "      <td>Th</td>\n",
       "      <td>1931508977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Fri</td>\n",
       "      <td>F</td>\n",
       "      <td>1842512718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Sa</td>\n",
       "      <td>1662762048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Su</td>\n",
       "      <td>1576726066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dow   longName abbreviated shortName    Requests\n",
       "0   1     Monday         Mon         M  2356818845\n",
       "1   2    Tuesday         Tue        Tu  1995034884\n",
       "2   3  Wednesday         Wed         W  1977615396\n",
       "3   4   Thursday         Thr        Th  1931508977\n",
       "4   5     Friday         Fri         F  1842512718\n",
       "5   6   Saturday         Sat        Sa  1662762048\n",
       "6   7     Sunday         Sun        Su  1576726066"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "aggregatedDowDF = (pageviewsEnhancedDF\n",
    "  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n",
    "  .sum(\"requests\")                                             \n",
    "  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n",
    "  .orderBy(col(\"dow\"))\n",
    ")\n",
    "\n",
    "display(aggregatedDowDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Broadcast Joins\n",
    "\n",
    "In joining these two DataFrames together, no type of join was specified.  In order to examine this, look at the physical plan used to return the query. This can be done with the `.explain()` DataFrame method. Look for **BroadcastHashJoin** and/or **BroadcastExchange**.\n",
    "\n",
    "<div><img src=\"../../resources/broadcasthashjoin.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [dow#15 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(dow#15 ASC NULLS FIRST, 5000)\n",
      "   +- *(3) HashAggregate(keys=[dow#15, longName#1, abbreviated#2, shortName#3], functions=[sum(cast(requests#10 as bigint))])\n",
      "      +- Exchange hashpartitioning(dow#15, longName#1, abbreviated#2, shortName#3, 5000)\n",
      "         +- *(2) HashAggregate(keys=[dow#15, longName#1, abbreviated#2, shortName#3], functions=[partial_sum(cast(requests#10 as bigint))])\n",
      "            +- *(2) Project [dow#15, requests#10, longName#1, abbreviated#2, shortName#3]\n",
      "               +- *(2) BroadcastHashJoin [cast(dow#15 as int)], [dow#0], Inner, BuildRight\n",
      "                  :- *(2) Project [requests#10, date_format(cast(timestamp#8 as timestamp), u, Some(Etc/Universal)) AS dow#15]\n",
      "                  :  +- *(2) Filter isnotnull(date_format(cast(timestamp#8 as timestamp), u, Some(Etc/Universal)))\n",
      "                  :     +- *(2) FileScan parquet [timestamp#8,requests#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:string,requests:int>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "                     +- *(1) Project [dow#0, longName#1, abbreviated#2, shortName#3]\n",
      "                        +- *(1) Filter isnotnull(dow#0)\n",
      "                           +- *(1) FileScan parquet [dow#0,longName#1,abbreviated#2,shortName#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://data.intellinum.co/bootcamp/common/day-of-week], PartitionFilters: [], PushedFilters: [IsNotNull(dow)], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>\n"
     ]
    }
   ],
   "source": [
    "aggregatedDowDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Spark did a broadcast join rather than a shuffle join.  In other words, it broadcast `labelsDF` to the larger `pageviewsDF`, replicating the smaller DataFrame on each node of our cluster.  This avoided having to move the larger DataFrame across the cluster.\n",
    "\n",
    "Take a look at the broadcast threshold by accessing the configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 10,485,760\n"
     ]
    }
   ],
   "source": [
    "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(\"Threshold: {0:,}\".format( int(threshold) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the maximize size in bytes for a table that broadcast to worker nodes.  Dropping it to `-1` disables broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice the lack of broadcast in the query physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [dow#15, timestamp#8, site#9, requests#10, longName#1, abbreviated#2, shortName#3]\n",
      "+- *(5) SortMergeJoin [cast(dow#15 as int)], [dow#0], Inner\n",
      "   :- *(2) Sort [cast(dow#15 as int) ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cast(dow#15 as int), 5000)\n",
      "   :     +- *(1) Project [timestamp#8, site#9, requests#10, date_format(cast(timestamp#8 as timestamp), u, Some(Etc/Universal)) AS dow#15]\n",
      "   :        +- *(1) Filter isnotnull(date_format(cast(timestamp#8 as timestamp), u, Some(Etc/Universal)))\n",
      "   :           +- *(1) FileScan parquet [timestamp#8,site#9,requests#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:string,site:string,requests:int>\n",
      "   +- *(4) Sort [dow#0 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(dow#0, 5000)\n",
      "         +- *(3) Project [dow#0, longName#1, abbreviated#2, shortName#3]\n",
      "            +- *(3) Filter isnotnull(dow#0)\n",
      "               +- *(3) FileScan parquet [dow#0,longName#1,abbreviated#2,shortName#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://data.intellinum.co/bootcamp/common/day-of-week], PartitionFilters: [], PushedFilters: [IsNotNull(dow)], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>\n"
     ]
    }
   ],
   "source": [
    "pageviewsDF.join(labelsDF, \"dow\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next reset the original threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicitly Broadcasting Tables\n",
    "\n",
    "There are two ways of telling Spark to explicitly broadcast tables. The first is to change the Spark configuration, which affects all operations. The second is to declare it using the `broadcast()` function in the `functions` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [dow#15, timestamp#8, site#9, requests#10, longName#1, abbreviated#2, shortName#3]\n",
      "+- *(2) BroadcastHashJoin [cast(dow#15 as int)], [dow#0], Inner, BuildRight\n",
      "   :- *(2) Project [timestamp#8, site#9, requests#10, date_format(cast(timestamp#8 as timestamp), u, Some(Etc/Universal)) AS dow#15]\n",
      "   :  +- *(2) Filter isnotnull(date_format(cast(timestamp#8 as timestamp), u, Some(Etc/Universal)))\n",
      "   :     +- *(2) FileScan parquet [timestamp#8,site#9,requests#10] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://data.intellinum.co/bootcamp/common/wikipedia/pageviews/pageviews_by_second..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:string,site:string,requests:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *(1) Project [dow#0, longName#1, abbreviated#2, shortName#3]\n",
      "         +- *(1) Filter isnotnull(dow#0)\n",
      "            +- *(1) FileScan parquet [dow#0,longName#1,abbreviated#2,shortName#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://data.intellinum.co/bootcamp/common/day-of-week], PartitionFilters: [], PushedFilters: [IsNotNull(dow)], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "pageviewsDF.join(broadcast(labelsDF), \"dow\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Join a Lookup Table\n",
    "\n",
    "Join a table that includes country name to a lookup table containing the full country name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the Data\n",
    "\n",
    "Create the following DataFrames:<br><br>\n",
    "\n",
    "- `countryLookupDF`: A lookup table with ISO country codes located at `s3a://data.intellinum.co/bootcamp/common/countries/ISOCountryCodes/ISOCountryLookup.parquet`\n",
    "- `logWithIPDF`: A server log including the results from an IPLookup table located at `s3a://data.intellinum.co/bootcamp/common/EDGAR-Log-20170329/enhanced/logDFwithIP.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "dfTest(\"ET2-P-05-01-01\", 249, countryLookupDF.count())\n",
    "dfTest(\"ET2-P-05-01-02\", 5000, logWithIPDF.count())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Broadcast the Lookup Table\n",
    "\n",
    "Complete the following:<br><br>\n",
    "\n",
    "- Create a new DataFrame `logWithIPEnhancedDF`\n",
    "- Get the full country name by performing a broadcast join that broadcasts the lookup table to the server log\n",
    "- Drop all columns other than `EnglishShortName`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "cols = set(logWithIPEnhancedDF.columns)\n",
    "\n",
    "dfTest(\"ET2-P-05-02-01\", True, \"EnglishShortName\" in cols and \"ip\" in cols)\n",
    "dfTest(\"ET2-P-05-02-02\", True, \"alpha2Code\" not in cols and \"ISO31662SubdivisionCode\" not in cols)\n",
    "dfTest(\"ET2-P-05-02-03\", 5000, logWithIPEnhancedDF.count())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** Why are joins expensive operations?  \n",
    "**Answer:** Joins perform a large number of row-wise comparisons, making the cost associated with joining tables grow with the size of the data in the tables.\n",
    "\n",
    "**Question:** What is the difference between a shuffle and broadcast join? How does Spark manage these differences?  \n",
    "**Answer:** A shuffle join shuffles data between nodes in a cluster. By contrast, a broadcast join moves the smaller of two DataFrames to where the larger DataFrame sits, minimizing the overall data transfer. By default, Spark performs a broadcast join if the total number of records is below a certain threshold. The threshold can be manually specified or you can manually specify that a broadcast join should take place. Since the automatic determination of whether a shuffle join should take place is by number of records, this could mean that really wide data would take up significantly more space per record and should therefore be specified manually.\n",
    "\n",
    "**Question:** What is a lookup table?  \n",
    "**Answer:** A lookup table is small table often used for referencing commonly used data such as mapping cities to countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
