{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 400px; height: 200px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying JSON & Hierarchical Data with DataFrames\n",
    "\n",
    "Apache Spark&trade; make it easy to work with hierarchical data, such as nested JSON records.\n",
    "\n",
    "## In this lesson you:\n",
    "* Use DataFrames to query JSON data.\n",
    "* Query nested structured data.\n",
    "* Query data containing array columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY} AWS_SECRET_ACCESS_KEY={AWS_SECRET_KEY} aws s3 cp s3://yuan.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 6\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_day03_querying_json\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_day03_querying_json\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://yuan.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Contents of a JSON file\n",
    "\n",
    "JSON is a common file format used in big data applications and in data lakes (or large stores of diverse data).  File formats such as JSON arise out of a number of data needs.  For instance, what if:\n",
    "<br>\n",
    "* Your schema, or the structure of your data, changes over time?\n",
    "* You need nested fields like an array with many values or an array of arrays?\n",
    "* You don't know how you're going use your data yet, so you don't want to spend time creating relational tables?\n",
    "\n",
    "The popularity of JSON is largely due to the fact that JSON allows for nested, flexible schemas.\n",
    "\n",
    "This lesson uses the `s3://data.intellinum.co/bootcamp/common/blog.json`. If you examine the raw file, notice it contains compact JSON data. There's a single JSON object on each line of the file; each object corresponds to a row in the table. Each row represents a blog post and the json file contains all blog posts through August 9, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY} AWS_SECRET_ACCESS_KEY={AWS_SECRET_KEY} aws s3 cp s3://data.intellinum.co/bootcamp/common/blog.json - | head -n 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame out of the syntax introduced in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").json(\"s3a://data.intellinum.co/bootcamp/common/blog.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the schema by invoking `printSchema` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query to view the contents of the table.\n",
    "\n",
    "Notice:\n",
    "* The `authors` column is an array containing one or more author names.\n",
    "* The `categories` column is an array of one or more blog post category names.\n",
    "* The `dates` column contains nested fields `createdOn`, `publishedOn` and `tz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(blogDF.select(\"authors\",\"categories\",\"dates\",\"content\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Data\n",
    "\n",
    "Think of nested data as columns within columns. \n",
    "\n",
    "For instance, look at the `dates` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesDF = blogDF.select(\"dates\")\n",
    "display(datesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out a specific subfield with `.` (object) notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(blogDF.select(\"dates.createdOn\", \"dates.publishedOn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame, `blog2DF` that contains the original columns plus the new `publishedOn` column obtained\n",
    "from flattening the dates column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "blog2DF = blogDF.withColumn(\"publishedOn\",col(\"dates.publishedOn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this temporary view, apply the printSchema method to check its schema and confirm the timestamp conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog2DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `createdOn` and `publishedOn` are stored as strings.\n",
    "\n",
    "Cast those values to SQL timestamps:\n",
    "\n",
    "In this case, use a single `select` method to:\n",
    "0. Cast `dates.publishedOn` to a `timestamp` data type\n",
    "0. \"Flatten\" the `dates.publishedOn` column to just `publishedOn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "display(blogDF.select(\"title\",date_format(\"dates.publishedOn\",\"yyyy-MM-dd\").alias(\"publishedOn\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another DataFrame, `blog2DF` that contains the original columns plus the new `publishedOn` column obtained\n",
    "from flattening the dates column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog2DF = blogDF.withColumn(\"publishedOn\", date_format(\"dates.publishedOn\",\"yyyy-MM-dd\")) \n",
    "display(blog2DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this temporary view, apply the `printSchema` method to check its schema and confirm the timestamp conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog2DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dates are represented by a `timestamp` data type, we need to convert to a data type that allows `<` and `>`-type comparison operations in order to query for articles within certain date ranges (such as a list of all articles published in 2013). This is accopmplished by using the `to_date` function in Scala or Python.\n",
    "\n",
    "See the Spark documentation on <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>, for a long list of date-specific functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, col\n",
    "          \n",
    "resultDF = (blog2DF.select(\"title\", to_date(col(\"publishedOn\"),\"MMM dd, yyyy\").alias('date'),\"link\") \n",
    "  .filter(year(col(\"publishedOn\")) == '2013') \n",
    "  .orderBy(col(\"publishedOn\"))\n",
    ")\n",
    "\n",
    "display(resultDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Data\n",
    "\n",
    "The DataFrame also contains array columns. \n",
    "\n",
    "Easily determine the size of each array using the built-in `size(..)` function with array columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "display(blogDF.select(size(\"authors\"),\"authors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the first element from the array `authors` using an array subscript operator.\n",
    "\n",
    "For example, in Scala, the 0th element of array `authors` is `authors(0)`\n",
    "whereas, in Python, the 0th element of `authors` is `authors[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(blogDF.select(col(\"authors\")[0].alias(\"primaryAuthor\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode\n",
    "\n",
    "The `explode` method allows you to split an array column into multiple rows, copying all the other columns into each new row. \n",
    "\n",
    "For example, split the column `authors` into the column `author`, with one author per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "display(blogDF.select(\"title\",\"authors\",explode(col(\"authors\")).alias(\"author\"), \"link\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more obvious to restrict the output to articles that have multiple authors, and then sort by the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog2DF = (blogDF \n",
    "  .select(\"title\",\"authors\",explode(col(\"authors\")).alias(\"author\"), \"link\") \n",
    "  .filter(size(col(\"authors\")) > 1) \n",
    "  .orderBy(\"title\")\n",
    ")\n",
    "\n",
    "display(blogDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Identify all the articles written or co-written by Michael Armbrust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with the `blogDF` DataFrame, create a DataFrame called `articlesByMichaelDF` where:\n",
    "0. Michael Armbrust is the author.\n",
    "0. The data set contains the column `title` (it may contain others).\n",
    "0. It contains only one record per article.\n",
    "\n",
    "**Hint:** See the Spark documentation on <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>.  \n",
    "\n",
    "**Hint:** Include the column `authors` in your view to help you debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "articlesByMichaelDF = ## FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "resultsCount = articlesByMichaelDF.count()\n",
    "dfTest(\"DF-L5-articlesByMichael-count\", 3, resultsCount)  \n",
    "\n",
    "results = articlesByMichaelDF.collect()\n",
    "\n",
    "dfTest(\"DF-L5-articlesByMichael-0\", Row(title=u'Spark SQL: Manipulating Structured Data Using Apache Spark'), results[0])\n",
    "dfTest(\"DF-L5-articlesByMichael-1\", Row(title=u'Exciting Performance Improvements on the Horizon for Spark SQL'), results[1])\n",
    "dfTest(\"DF-L5-articlesByMichael-2\", Row(title=u'Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform'), results[2])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Show the list of Michael Armbrust's articles in HTML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Identify the complete set of categories used in the blog articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with the `blogDF` DataFrame, create another DataFrame called `uniqueCategoriesDF` where:\n",
    "0. The data set contains the one column `category` (and no others).\n",
    "0. This list of categories should be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "resultsCount =  uniqueCategoriesDF.count()\n",
    "\n",
    "dfTest(\"DF-L5-uniqueCategories-count\", 12, resultsCount)\n",
    "\n",
    "results = uniqueCategoriesDF.collect()\n",
    "\n",
    "dfTest(\"DF-L5-uniqueCategories-0\", Row(category=u'Announcements'), results[0])\n",
    "dfTest(\"DF-L5-uniqueCategories-1\", Row(category=u'Apache Spark'), results[1])\n",
    "dfTest(\"DF-L5-uniqueCategories-2\", Row(category=u'Company Blog'), results[2])\n",
    "\n",
    "dfTest(\"DF-L5-uniqueCategories-9\", Row(category=u'Platform'), results[9])\n",
    "dfTest(\"DF-L5-uniqueCategories-10\", Row(category=u'Product'), results[10])\n",
    "dfTest(\"DF-L5-uniqueCategories-11\", Row(category=u'Streaming'), results[11])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Show the complete list of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Count how many times each category is referenced in the blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with the `blogDF` DataFrame, create another DataFrame called `totalArticlesByCategoryDF` where:\n",
    "0. The new DataFrame contains two columns, `category` and `total`.\n",
    "0. The `category` column is a single, distinct category (similar to the last exercise).\n",
    "0. The `total` column is the total number of articles in that category.\n",
    "0. Order by `category`.\n",
    "\n",
    "Because articles can be tagged with multiple categories, the sum of the totals adds up to more than the total number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "results = totalArticlesByCategoryDF.count()\n",
    "\n",
    "dfTest(\"DF-L5-articlesByCategory-count\", 12, results)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "results = totalArticlesByCategoryDF.collect()\n",
    "\n",
    "dfTest(\"DF-L5-articlesByCategory-0\", Row(category=u'Announcements', total=72), results[0])\n",
    "dfTest(\"DF-L5-articlesByCategory-1\", Row(category=u'Apache Spark', total=132), results[1])\n",
    "dfTest(\"DF-L5-articlesByCategory-2\", Row(category=u'Company Blog', total=224), results[2])\n",
    "\n",
    "dfTest(\"DF-L5-articlesByCategory-9\", Row(category=u'Platform', total=4), results[9])\n",
    "dfTest(\"DF-L5-articlesByCategory-10\", Row(category=u'Product', total=83), results[10])\n",
    "dfTest(\"DF-L5-articlesByCategory-11\", Row(category=u'Streaming', total=21), results[11])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Display the totals of each category in html format (should be ordered by `category`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Spark DataFrames allows you to query and manipulate structured and semi-structured data.\n",
    "* Spark DataFrames built-in functions provide powerful primitives for querying complex schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "**Q:** What is the syntax for accessing nested columns?  \n",
    "**A:** Use the dot notation:\n",
    "`select(\"dates.publishedOn\")`\n",
    "\n",
    "**Q:** What is the syntax for accessing the first element in an array?  \n",
    "**A:** Use the [subscript] notation: \n",
    "`select(\"col(authors)[0]\")`\n",
    "\n",
    "**Q:** What is the syntax for expanding an array into multiple rows?  \n",
    "**A:** Use the explode method:  `select(explode(col(\"authors\")).alias(\"Author\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Topics & Resources\n",
    "\n",
    "* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
