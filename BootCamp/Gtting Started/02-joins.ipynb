{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 400px; height: 200px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations and JOINs\n",
    "Apache Spark&trade; allow you to create on-the-fly data lakes.\n",
    "\n",
    "## In this lesson you:\n",
    "* Use basic aggregations.\n",
    "* Correlate two data sets with a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY} AWS_SECRET_ACCESS_KEY={AWS_SECRET_KEY} aws s3 cp s3://yuan.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"2G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 6\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_day02_joins\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_day02_joins\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://yuan.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Aggregations\n",
    "\n",
    "Using [built-in Spark functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$), you can aggregate data in various ways. \n",
    "\n",
    "Run the cell below to compute the average of all salaries in the people DataFrame.\n",
    "\n",
    " By default, you get a floating point value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/people-10m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    averageSalary|\n",
      "+-----------------+\n",
      "|72633.01037020104|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "avgSalaryDF = peopleDF.select(avg(\"salary\").alias(\"averageSalary\"))\n",
    "\n",
    "avgSalaryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert that value to an integer using the `round()` function. See\n",
    "[the documentation for round()](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|roundedAverageSalary|\n",
      "+--------------------+\n",
      "|             72633.0|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "roundedAvgSalaryDF = avgSalaryDF.select(round(\"averageSalary\").alias(\"roundedAverageSalary\"))\n",
    "\n",
    "roundedAvgSalaryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the average salary, what are the maximum and minimum salaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "|   max|   min|averageSalary|\n",
      "+------+------+-------------+\n",
      "|180841|-26884|      72633.0|\n",
      "+------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "salaryDF = peopleDF.select(max(\"salary\").alias(\"max\"), \n",
    "                           min(\"salary\").alias(\"min\"), \n",
    "                           round(avg(\"salary\")).alias(\"averageSalary\"))\n",
    "\n",
    "salaryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Two Data Sets\n",
    "\n",
    "Correlate the data in two data sets using a DataFrame join. \n",
    "\n",
    "The `people` data set has 10 million names in it. \n",
    "\n",
    "> How many of the first names appear in Social Security data files? \n",
    "\n",
    "To find out, use the Social Security data set with first name popularity data from the United States Social Security Administration. \n",
    "\n",
    "For every year from 1880 to 2014, `s3a://data.intellinum.co/bootcamp/common/names-1880-2016.parquet/` lists the first names of people born in that year, their gender, and the total number of people given that name. \n",
    "\n",
    "By joining the `people` data set with `names-1880-2016`, weed out the names that aren't represented in the Social Security data.\n",
    "\n",
    "(In a real application, you might use a join like this to filter out bad data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>total</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Racine</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clarisa</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commodore</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phil</td>\n",
       "      <td>M</td>\n",
       "      <td>264</td>\n",
       "      <td>1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paula</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>1891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenore</td>\n",
       "      <td>F</td>\n",
       "      <td>53</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>F</td>\n",
       "      <td>134</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blanche</td>\n",
       "      <td>F</td>\n",
       "      <td>227</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reynaldo</td>\n",
       "      <td>M</td>\n",
       "      <td>68</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zack</td>\n",
       "      <td>M</td>\n",
       "      <td>19</td>\n",
       "      <td>1889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   firstName gender  total  year\n",
       "0     Racine      F      5  1962\n",
       "1    Clarisa      F      5  1946\n",
       "2  Commodore      M      5  1897\n",
       "3       Phil      M    264  1935\n",
       "4      Paula      F     44  1891\n",
       "5     Lenore      F     53  1979\n",
       "6    Charlie      F    134  1940\n",
       "7    Blanche      F    227  1955\n",
       "8   Reynaldo      M     68  1927\n",
       "9       Zack      M     19  1889"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssaDF = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/names-1880-2016.parquet\")\n",
    "\n",
    "display(ssaDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, with a quick count of distinct names, get an idea of how many distinct names there are in each of the tables.\n",
    "\n",
    "DataFrames have a `distinct` method just for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDistinctNamesDF = peopleDF.select(\"firstName\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDistinctNamesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the join, let's rename the `firstName` column to `ssaFirstName` in the Social Security DataFrame.\n",
    "\n",
    "Question to ponder: why would we want to do this?\n",
    "\n",
    "Answer: In order that not confused by two firstNames in two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssaDistinctNamesDF = ssaDF.select(\"firstName\").withColumnRenamed(\"firstName\",'ssaFirstName').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many distinct names in the Social Security DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssaDistinctNamesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join the two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "joinedDF = peopleDistinctNamesDF.join(ssaDistinctNamesDF, col(\"firstName\") == col(\"ssaFirstName\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In the tables above, some of the salaries in the `peopleDF` DataFrame are negative. \n",
    "\n",
    "These salaries represent bad data. \n",
    "\n",
    "Your job is to convert all the negative salaries to positive ones, and then sort the top 20 people by their salary.\n",
    "\n",
    "**Hint:** See the Apache Spark documentation, [built-in Spark functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Create a DataFrame`PeopleWithFixedSalariesDF`, where all the negative salaries have been converted to positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary\n",
       "0   74468\n",
       "1   55688\n",
       "2   92511\n",
       "3   48451\n",
       "4   90330\n",
       "5   84103\n",
       "6   54632\n",
       "7   63966\n",
       "8   97736\n",
       "9   95509"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "from pyspark.sql.functions import abs\n",
    "peopleWithFixedSalariesDF = peopleDF.select(abs(col(\"salary\")).alias(\"salary\"))\n",
    "display(peopleWithFixedSalariesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "belowZero = peopleWithFixedSalariesDF.filter(peopleWithFixedSalariesDF[\"salary\"] < 0).count()\n",
    "dfTest(\"DF-L3-belowZero\", 0, belowZero)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Starting with the `peopleWithFixedSalariesDF` DataFrame, create another DataFrame called `PeopleWithFixedSalariesSortedDF` where:\n",
    "0. The data set has been reduced to the first 20 records.\n",
    "0. The records are sorted by the column `salary` in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary\n",
       "0       2\n",
       "1       3\n",
       "2       4\n",
       "3       5\n",
       "4       6\n",
       "5       8\n",
       "6      15\n",
       "7      16\n",
       "8      16\n",
       "9      17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "from pyspark.sql.functions import asc\n",
    "peopleWithFixedSalariesSortedDF = peopleWithFixedSalariesDF.orderBy(asc(\"salary\")).limit(20)\n",
    "display(peopleWithFixedSalariesSortedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "resultsDF = peopleWithFixedSalariesSortedDF.select(\"salary\")\n",
    "dfTest(\"DF-L3-count\", 20, resultsDF.count())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "results = resultsDF.collect()\n",
    "\n",
    "dfTest(\"DF-L3-fixedSalaries-0\", Row(salary=2), results[0])\n",
    "dfTest(\"DF-L3-fixedSalaries-1\", Row(salary=3), results[1])\n",
    "dfTest(\"DF-L3-fixedSalaries-2\", Row(salary=4), results[2])\n",
    "\n",
    "dfTest(\"DF-L3-fixedSalaries-10\", Row(salary=19), results[10])\n",
    "dfTest(\"DF-L3-fixedSalaries-11\", Row(salary=19), results[11])\n",
    "dfTest(\"DF-L3-fixedSalaries-12\", Row(salary=20), results[12])\n",
    "\n",
    "dfTest(\"DF-L3-fixedSalaries-17\", Row(salary=28), results[17])\n",
    "dfTest(\"DF-L3-fixedSalaries-18\", Row(salary=30), results[18]) \n",
    "dfTest(\"DF-L3-fixedSalaries-19\", Row(salary=31), results[19]) \n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "As a refinement, assume all salaries under $20,000 represent bad rows and filter them out.\n",
    "\n",
    "Additionally, categorize each person's salary into $10K groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    " Starting with the `peopleWithFixedSalariesDF` DataFrame, create a DataFrame called `peopleWithFixedSalaries20KDF` where:\n",
    "0. The data set excludes all records where salaries are below $20K.\n",
    "0. The data set includes a new column called `salary10k`, that should be the salary in groups of 10,000. For example:\n",
    "  * A salary of 23,000 should report a value of \"2\".\n",
    "  * A salary of 57,400 should report a value of \"6\".\n",
    "  * A salary of 1,231,375 should report a value of \"123\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>salary10k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74468</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55688</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92511</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48451</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90330</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84103</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54632</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63966</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97736</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95509</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary  salary10k\n",
       "0   74468        7.0\n",
       "1   55688        6.0\n",
       "2   92511        9.0\n",
       "3   48451        5.0\n",
       "4   90330        9.0\n",
       "5   84103        8.0\n",
       "6   54632        5.0\n",
       "7   63966        6.0\n",
       "8   97736       10.0\n",
       "9   95509       10.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "peopleWithFixedSalaries20KDF = (peopleWithFixedSalariesDF\n",
    "                                .filter(col(\"salary\")>=20000)\n",
    "                                .withColumn(\"salary10k\", round(col(\"salary\")/10000)))\n",
    "display(peopleWithFixedSalaries20KDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "below20K = peopleWithFixedSalaries20KDF.filter(\"salary < 20000\").count()\n",
    " \n",
    "dfTest(\"DF-L3-count-salaries\", 0, below20K)  \n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "results = (peopleWithFixedSalaries20KDF \n",
    "  .select(\"salary10k\") \n",
    "  .groupBy(\"salary10k\") \n",
    "  .agg(count(\"*\").alias(\"total\")) \n",
    "  .orderBy(\"salary10k\") \n",
    "  .limit(5) \n",
    "  .collect()\n",
    ")\n",
    "\n",
    "dfTest(\"DF-L3-countSalaries-0\", Row(salary10k=2.0, total=43792), results[0])\n",
    "dfTest(\"DF-L3-countSalaries-1\", Row(salary10k=3.0, total=212630), results[1])\n",
    "dfTest(\"DF-L3-countSalaries-2\", Row(salary10k=4.0, total=536535), results[2])\n",
    "dfTest(\"DF-L3-countSalaries-3\", Row(salary10k=5.0, total=1055261), results[3])\n",
    "dfTest(\"DF-L3-countSalaries-4\", Row(salary10k=6.0, total=1623248), results[4])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Using the `peopleDF` DataFrame, count the number of females named Caren who were born before March 1980. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with `peopleDF`, create a DataFrame called `carensDF` where:\n",
    "0. The result set has a single record.\n",
    "0. The data set has a single column named `total`.\n",
    "0. The result counts only \n",
    "  * Females (`gender`)\n",
    "  * First Name is \"Caren\" (`firstName`)\n",
    "  * Born before March 1980 (`birthDate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total\n",
       "0    750"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "carensDF = (peopleDF.filter(\"gender = 'F'\")\n",
    "            .filter(\"firstName='Caren'\")\n",
    "            .filter(\"birthDate < '1980-03'\")\n",
    "            .agg(count(\"*\").alias(\"total\")))\n",
    "\n",
    "display(carensDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "rows = carensDF.collect()\n",
    "\n",
    "dfTest(\"DF-L3-carens-len\", 1, len(rows))\n",
    "dfTest(\"DF-L3-carens-total\", Row(total=750), rows[0])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "**Q:** What is the DataFrame equivalent of the SQL statement `SELECT count(*) AS total`  \n",
    "**A:** ```.agg(count(\"*\").alias(\"total\"))```\n",
    "\n",
    "**Q:** What is the DataFrame equivalent of the SQL statement \n",
    "```SELECT firstName FROM PeopleDistinctNames INNER JOIN SSADistinctNames ON firstName = ssaFirstName```  \n",
    "**A:** \n",
    "`peopleDistinctNamesDF.join(ssaDistinctNamesDF, peopleDistinctNamesDF(col(\"firstName\")) == col(\"ssaFirstName\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise 3\n",
    "\n",
    "Starting with the `s3a://data.intellinum.co/bootcamp/common/names-1880-2016.parquet/` file, find the most popular first name for girls in 1885, 1915, 1945, 1975, and 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Create a temporary view called `HistoricNames` where:\n",
    "0. You may need to create temporary DataFrames to generate the DataFrame listing the names.\n",
    "0. The result has three columns:\n",
    "  * `firstName`\n",
    "  * `year`\n",
    "  * `total`\n",
    "\n",
    "**Hint:** This is an example of a nested `SELECT` if you were to use SQL syntax. Using DataFrames, you will need to craft two queries and perform a `join`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Load `s3a://data.intellinum.co/bootcamp/common/names-1880-2016.parquet/` into a DataFrame called `ssaDF` and display the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "ssaDF = spark.read.parquet(\"s3a://data.intellinum.co/bootcamp/common/names-1880-2016.parquet/\")\n",
    "\n",
    "ssaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891894"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssaDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Craft a DataFrame query that solves the problem described above.\n",
    "\n",
    "Display the output of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+----+\n",
      "|total|firstName|gender|year|\n",
      "+-----+---------+------+----+\n",
      "| 9128|     Mary|     F|1885|\n",
      "|58187|     Mary|     F|1915|\n",
      "|59288|     Mary|     F|1945|\n",
      "|58186| Jennifer|     F|1975|\n",
      "|23934|    Emily|     F|2005|\n",
      "+-----+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# find the most popular first name for girls in 1885, 1915, 1945, 1975, and 2005.\n",
    "\n",
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "maxNamesDF = (ssaDF.filter(col(\"year\").isin('1885','1915','1945','1975','2005'))\n",
    "              .filter(\"gender = 'F'\")\n",
    "              .groupBy(\"year\")\n",
    "              .agg(max(\"total\").alias(\"total\"))\n",
    "              .withColumnRenamed(\"year\",\"maxyear\"))\n",
    "\n",
    "outerQueryDF = maxNamesDF.join(ssaDF, \"total\",\"right_outer\")\n",
    "\n",
    "joinedQueryDF = (outerQueryDF.filter(\"maxyear >0\")\n",
    "                 .filter(\"maxyear == year\")\n",
    "                 .drop(\"maxyear\")\n",
    "                 .orderBy(asc(\"year\")))\n",
    "\n",
    "joinedQueryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "resultsDF = joinedQueryDF.select(\"firstName\", \"year\", \"total\")\n",
    "results = [ (r[0]+\" \"+str(r[1])+\": \"+str(r[2])) for r in resultsDF.collect()]\n",
    "\n",
    "dfTest(\"DF-L3-Opt-historicNames-0\", u'Mary 1885: 9128', results[0])\n",
    "dfTest(\"DF-L3-Opt-historicNames-1\", u'Mary 1915: 58187', results[1])\n",
    "dfTest(\"DF-L3-Opt-historicNames-2\", u'Mary 1945: 59288', results[2])\n",
    "dfTest(\"DF-L3-Opt-historicNames-3\", u'Jennifer 1975: 58186', results[3])\n",
    "dfTest(\"DF-L3-Opt-historicNames-4\", u'Emily 2005: 23934', results[4])\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
