{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"../../resources/logo.png\" alt=\"Intellinum Bootcamp\" style=\"width: 400px; height: 200px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying JSON & Hierarchical Data with DataFrames\n",
    "\n",
    "Apache Spark&trade; make it easy to work with hierarchical data, such as nested JSON records.\n",
    "\n",
    "## In this lesson you:\n",
    "* Use DataFrames to query JSON data.\n",
    "* Query nested structured data.\n",
    "* Query data containing array columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing SparkSession detected\n",
      "Creating a new SparkSession\n"
     ]
    }
   ],
   "source": [
    "#MODE = \"LOCAL\"\n",
    "MODE = \"CLUSTER\"\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0,'../../src')\n",
    "from settings import *\n",
    "\n",
    "try:\n",
    "    fh = open('../../libs/pyspark24_py36.zip', 'r')\n",
    "except FileNotFoundError:\n",
    "    !AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY} AWS_SECRET_ACCESS_KEY={AWS_SECRET_KEY} aws s3 cp s3://yuan.intellinum.co/bins/pyspark24_py36.zip ../../libs/pyspark24_py36.zip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped a SparkSession\")\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession detected\")\n",
    "    print(\"Creating a new SparkSession\")\n",
    "\n",
    "SPARK_DRIVER_MEMORY= \"1G\"\n",
    "SPARK_DRIVER_CORE = \"1\"\n",
    "SPARK_EXECUTOR_MEMORY= \"1G\"\n",
    "SPARK_EXECUTOR_CORE = \"1\"\n",
    "SPARK_EXECUTOR_INSTANCES = 6\n",
    "\n",
    "\n",
    "\n",
    "conf = None\n",
    "if MODE == \"LOCAL\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/home/yuan/anaconda3/envs/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_day03_querying_json\").\\\n",
    "            setMaster('local[*]').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', '../../libs/mysql-connector-java-5.1.45-bin.jar').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"./MN/pyspark24_py36/bin/python\"\n",
    "    conf = SparkConf().\\\n",
    "            setAppName(\"pyspark_day03_querying_json\").\\\n",
    "            setMaster('yarn-client').\\\n",
    "            set('spark.executor.cores', SPARK_EXECUTOR_CORE).\\\n",
    "            set('spark.executor.memory', SPARK_EXECUTOR_MEMORY).\\\n",
    "            set('spark.driver.cores', SPARK_DRIVER_CORE).\\\n",
    "            set('spark.driver.memory', SPARK_DRIVER_MEMORY).\\\n",
    "            set(\"spark.executor.instances\", SPARK_EXECUTOR_INSTANCES).\\\n",
    "            set('spark.sql.files.ignoreCorruptFiles', 'true').\\\n",
    "            set('spark.yarn.dist.archives', '../../libs/pyspark24_py36.zip#MN').\\\n",
    "            set('spark.sql.shuffle.partitions', '5000').\\\n",
    "            set('spark.default.parallelism', '5000').\\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars.packages','net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'). \\\n",
    "            set('spark.driver.maxResultSize', '0').\\\n",
    "            set('spark.jars', 's3://yuan.intellinum.co/bins/mysql-connector-java-5.1.45-bin.jar')\n",
    "        \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf).\\\n",
    "    getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('../../src/settings.py')\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "def display(df, limit=10):\n",
    "    return df.limit(limit).toPandas()\n",
    "\n",
    "def dfTest(id, expected, result):\n",
    "    assert str(expected) == str(result), \"{} does not equal expected {}\".format(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Contents of a JSON file\n",
    "\n",
    "JSON is a common file format used in big data applications and in data lakes (or large stores of diverse data).  File formats such as JSON arise out of a number of data needs.  For instance, what if:\n",
    "<br>\n",
    "* Your schema, or the structure of your data, changes over time?\n",
    "* You need nested fields like an array with many values or an array of arrays?\n",
    "* You don't know how you're going use your data yet, so you don't want to spend time creating relational tables?\n",
    "\n",
    "The popularity of JSON is largely due to the fact that JSON allows for nested, flexible schemas.\n",
    "\n",
    "This lesson uses the `s3://data.intellinum.co/bootcamp/common/blog.json`. If you examine the raw file, notice it contains compact JSON data. There's a single JSON object on each line of the file; each object corresponds to a row in the table. Each row represents a blog post and the json file contains all blog posts through August 9, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\": \"publish\", \"description\": null, \"creator\": \"roy\", \"link\": \"https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html\", \"authors\": [\"Tomer Shiran (VP of Product Management at MapR)\"], \"id\": 33, \"categories\": [\"Company Blog\", \"Partners\"], \"dates\": {\"publishedOn\": \"2014-04-10\", \"tz\": \"UTC\", \"createdOn\": \"2014-04-10\"}, \"title\": \"MapR Integrates the Complete Apache Spark Stack\", \"slug\": \"mapr-integrates-spark-stack\", \"content\": \"<div class=\\\"post-meta\\\">This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.</div>\\n\\n<hr />\\n\\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world\\u2019s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an <a href=\\\"http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\\\" target=\\\"_blank\\\">interview</a> with Stefan Groschupf, CEO of Datameer.\\n\\nToday, I am happy to <a href=\\\"http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\\\" target=\\\"_blank\\\">announce</a> and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\\n<h2 id=\\\"why-spark\\\">Why Spark?</h2>\\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\\n\\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\\n<h2 id=\\\"why-databricks\\\">Why Databricks?</h2>\\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\\n\\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\\n\\n<a href=\\\"http://w.on24.com/r.htm?e=780379&amp;s=1&amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;partnerref=MapR\\\" target=\\\"_blank\\\">Register for an upcoming joint webinar</a> to learn more about the benefits of the complete Spark stack on MapR.\"}\n",
      "{\"status\": \"publish\", \"description\": null, \"creator\": \"tdas\", \"link\": \"https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html\", \"authors\": [\"Tathagata Das\"], \"id\": 35, \"categories\": [\"Apache Spark\", \"Engineering Blog\", \"Machine Learning\"], \"dates\": {\"publishedOn\": \"2014-04-10\", \"tz\": \"UTC\", \"createdOn\": \"2014-04-10\"}, \"title\": \"Apache Spark 0.9.1 Released\", \"slug\": \"spark-0_9_1-released\", \"content\": \"We are happy to announce the availability of <a href=\\\"http://spark.apache.org/releases/spark-release-0-9-1.html\\\" target=\\\"_blank\\\">Apache Spark 0.9.1</a>! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\\n\\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\\n\\nVisit the <a href=\\\"http://spark.apache.org/releases/spark-release-0-9-1.html\\\" target=\\\"_blank\\\">release notes</a> for more information about all the improvements and bug fixes. <a href=\\\"http://spark.apache.org/downloads.html\\\" target=\\\"_blank\\\">Download</a> it and try it out!\"}\n",
      "download failed: s3://data.intellinum.co/bootcamp/common/blog.json to - [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY} AWS_SECRET_ACCESS_KEY={AWS_SECRET_KEY} aws s3 cp s3://data.intellinum.co/bootcamp/common/blog.json - | head -n 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame out of the syntax introduced in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").json(\"s3a://data.intellinum.co/bootcamp/common/blog.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the schema by invoking `printSchema` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- dates: struct (nullable = true)\n",
      " |    |-- createdOn: string (nullable = true)\n",
      " |    |-- publishedOn: string (nullable = true)\n",
      " |    |-- tz: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- slug: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query to view the contents of the table.\n",
    "\n",
    "Notice:\n",
    "* The `authors` column is an array containing one or more author names.\n",
    "* The `categories` column is an array of one or more blog post category names.\n",
    "* The `dates` column contains nested fields `createdOn`, `publishedOn` and `tz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>dates</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Tomer Shiran (VP of Product Management at MapR)]</td>\n",
       "      <td>[Company Blog, Partners]</td>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;This post is guest auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Tathagata Das]</td>\n",
       "      <td>[Apache Spark, Engineering Blog, Machine Learn...</td>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "      <td>We are happy to announce the availability of &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Steven Hillion]</td>\n",
       "      <td>[Company Blog, Partners]</td>\n",
       "      <td>(2014-04-01, 2014-04-01, UTC)</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;This post is guest auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Michael Armbrust, Reynold Xin]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>(2014-03-27, 2014-03-27, UTC)</td>\n",
       "      <td>Building a unified platform for big data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Patrick Wendell]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>(2014-02-04, 2014-02-04, UTC)</td>\n",
       "      <td>Our goal with Apache Spark is very simple: pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Ali Ghodsi, Ahir Reddy]</td>\n",
       "      <td>[Apache Spark, Ecosystem, Engineering Blog]</td>\n",
       "      <td>(2014-01-02, 2014-01-02, UTC)</td>\n",
       "      <td>Apache Hadoop integration has always been a ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Russell Cardullo (Data Infrastructure Enginee...</td>\n",
       "      <td>[Company Blog, Customers]</td>\n",
       "      <td>(2014-03-26, 2014-03-26, UTC)</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;We're very happy to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Jai Ranganathan, Matei Zaharia]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>(2014-03-21, 2014-03-21, UTC)</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;\\n\\nThis article was cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Databricks Press Office]</td>\n",
       "      <td>[Announcements, Company Blog]</td>\n",
       "      <td>(2014-03-19, 2014-03-19, UTC)</td>\n",
       "      <td>&lt;strong&gt;BERKELEY, Calif. – March 18, 2014 –&lt;/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Ion Stoica]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>(2014-03-03, 2014-03-03, UTC)</td>\n",
       "      <td>&lt;div class=\"blogContent\"&gt;\\n\\nWe are delighted ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  [Tomer Shiran (VP of Product Management at MapR)]   \n",
       "1                                    [Tathagata Das]   \n",
       "2                                   [Steven Hillion]   \n",
       "3                    [Michael Armbrust, Reynold Xin]   \n",
       "4                                  [Patrick Wendell]   \n",
       "5                           [Ali Ghodsi, Ahir Reddy]   \n",
       "6  [Russell Cardullo (Data Infrastructure Enginee...   \n",
       "7                   [Jai Ranganathan, Matei Zaharia]   \n",
       "8                          [Databricks Press Office]   \n",
       "9                                       [Ion Stoica]   \n",
       "\n",
       "                                          categories  \\\n",
       "0                           [Company Blog, Partners]   \n",
       "1  [Apache Spark, Engineering Blog, Machine Learn...   \n",
       "2                           [Company Blog, Partners]   \n",
       "3                   [Apache Spark, Engineering Blog]   \n",
       "4                   [Apache Spark, Engineering Blog]   \n",
       "5        [Apache Spark, Ecosystem, Engineering Blog]   \n",
       "6                          [Company Blog, Customers]   \n",
       "7                   [Apache Spark, Engineering Blog]   \n",
       "8                      [Announcements, Company Blog]   \n",
       "9                   [Apache Spark, Engineering Blog]   \n",
       "\n",
       "                           dates  \\\n",
       "0  (2014-04-10, 2014-04-10, UTC)   \n",
       "1  (2014-04-10, 2014-04-10, UTC)   \n",
       "2  (2014-04-01, 2014-04-01, UTC)   \n",
       "3  (2014-03-27, 2014-03-27, UTC)   \n",
       "4  (2014-02-04, 2014-02-04, UTC)   \n",
       "5  (2014-01-02, 2014-01-02, UTC)   \n",
       "6  (2014-03-26, 2014-03-26, UTC)   \n",
       "7  (2014-03-21, 2014-03-21, UTC)   \n",
       "8  (2014-03-19, 2014-03-19, UTC)   \n",
       "9  (2014-03-03, 2014-03-03, UTC)   \n",
       "\n",
       "                                             content  \n",
       "0  <div class=\"post-meta\">This post is guest auth...  \n",
       "1  We are happy to announce the availability of <...  \n",
       "2  <div class=\"post-meta\">This post is guest auth...  \n",
       "3  Building a unified platform for big data analy...  \n",
       "4  Our goal with Apache Spark is very simple: pro...  \n",
       "5  Apache Hadoop integration has always been a ke...  \n",
       "6  <div class=\"post-meta\">We're very happy to see...  \n",
       "7  <div class=\"post-meta\">\\n\\nThis article was cr...  \n",
       "8  <strong>BERKELEY, Calif. – March 18, 2014 –</s...  \n",
       "9  <div class=\"blogContent\">\\n\\nWe are delighted ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(blogDF.select(\"authors\",\"categories\",\"dates\",\"content\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Data\n",
    "\n",
    "Think of nested data as columns within columns. \n",
    "\n",
    "For instance, look at the `dates` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2014-04-01, 2014-04-01, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(2014-03-27, 2014-03-27, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(2014-02-04, 2014-02-04, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(2014-01-02, 2014-01-02, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(2014-03-26, 2014-03-26, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(2014-03-21, 2014-03-21, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(2014-03-19, 2014-03-19, UTC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2014-03-03, 2014-03-03, UTC)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           dates\n",
       "0  (2014-04-10, 2014-04-10, UTC)\n",
       "1  (2014-04-10, 2014-04-10, UTC)\n",
       "2  (2014-04-01, 2014-04-01, UTC)\n",
       "3  (2014-03-27, 2014-03-27, UTC)\n",
       "4  (2014-02-04, 2014-02-04, UTC)\n",
       "5  (2014-01-02, 2014-01-02, UTC)\n",
       "6  (2014-03-26, 2014-03-26, UTC)\n",
       "7  (2014-03-21, 2014-03-21, UTC)\n",
       "8  (2014-03-19, 2014-03-19, UTC)\n",
       "9  (2014-03-03, 2014-03-03, UTC)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datesDF = blogDF.select(\"dates\")\n",
    "display(datesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out a specific subfield with `.` (object) notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdOn</th>\n",
       "      <th>publishedOn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-04-10</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-04-10</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-03-27</td>\n",
       "      <td>2014-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-02-04</td>\n",
       "      <td>2014-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>2014-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014-03-26</td>\n",
       "      <td>2014-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2014-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014-03-19</td>\n",
       "      <td>2014-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-03-03</td>\n",
       "      <td>2014-03-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    createdOn publishedOn\n",
       "0  2014-04-10  2014-04-10\n",
       "1  2014-04-10  2014-04-10\n",
       "2  2014-04-01  2014-04-01\n",
       "3  2014-03-27  2014-03-27\n",
       "4  2014-02-04  2014-02-04\n",
       "5  2014-01-02  2014-01-02\n",
       "6  2014-03-26  2014-03-26\n",
       "7  2014-03-21  2014-03-21\n",
       "8  2014-03-19  2014-03-19\n",
       "9  2014-03-03  2014-03-03"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(blogDF.select(\"dates.createdOn\", \"dates.publishedOn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame, `blog2DF` that contains the original columns plus the new `publishedOn` column obtained\n",
    "from flattening the dates column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "blog2DF = blogDF.withColumn(\"publishedOn\",col(\"dates.publishedOn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this temporary view, apply the printSchema method to check its schema and confirm the timestamp conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- dates: struct (nullable = true)\n",
      " |    |-- createdOn: string (nullable = true)\n",
      " |    |-- publishedOn: string (nullable = true)\n",
      " |    |-- tz: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- slug: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedOn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blog2DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `createdOn` and `publishedOn` are stored as strings.\n",
    "\n",
    "Cast those values to SQL timestamps:\n",
    "\n",
    "In this case, use a single `select` method to:\n",
    "0. Cast `dates.publishedOn` to a `timestamp` data type\n",
    "0. \"Flatten\" the `dates.publishedOn` column to just `publishedOn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publishedOn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MapR Integrates the Complete Apache Spark Stack</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apache Spark 0.9.1 Released</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Application Spotlight: Alpine Data Labs</td>\n",
       "      <td>2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spark SQL: Manipulating Structured Data Using ...</td>\n",
       "      <td>2014-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apache Spark 0.9.0 Released</td>\n",
       "      <td>2014-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apache Spark In MapReduce (SIMR)</td>\n",
       "      <td>2014-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sharethrough Uses Apache Spark Streaming to Op...</td>\n",
       "      <td>2014-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apache Spark: A Delight for Developers</td>\n",
       "      <td>2014-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Databricks announces \"Certified on Apache Spar...</td>\n",
       "      <td>2014-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apache Spark Now a Top-level Apache Project</td>\n",
       "      <td>2014-03-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title publishedOn\n",
       "0    MapR Integrates the Complete Apache Spark Stack  2014-04-10\n",
       "1                        Apache Spark 0.9.1 Released  2014-04-10\n",
       "2            Application Spotlight: Alpine Data Labs  2014-04-01\n",
       "3  Spark SQL: Manipulating Structured Data Using ...  2014-03-27\n",
       "4                        Apache Spark 0.9.0 Released  2014-02-04\n",
       "5                   Apache Spark In MapReduce (SIMR)  2014-01-02\n",
       "6  Sharethrough Uses Apache Spark Streaming to Op...  2014-03-26\n",
       "7             Apache Spark: A Delight for Developers  2014-03-21\n",
       "8  Databricks announces \"Certified on Apache Spar...  2014-03-19\n",
       "9        Apache Spark Now a Top-level Apache Project  2014-03-03"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "display(blogDF.select(\"title\",date_format(\"dates.publishedOn\",\"yyyy-MM-dd\").alias(\"publishedOn\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another DataFrame, `blog2DF` that contains the original columns plus the new `publishedOn` column obtained\n",
    "from flattening the dates column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>content</th>\n",
       "      <th>creator</th>\n",
       "      <th>dates</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>slug</th>\n",
       "      <th>status</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedOn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Tomer Shiran (VP of Product Management at MapR)]</td>\n",
       "      <td>[Company Blog, Partners]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;This post is guest auth...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>33</td>\n",
       "      <td>https://databricks.com/blog/2014/04/10/mapr-in...</td>\n",
       "      <td>mapr-integrates-spark-stack</td>\n",
       "      <td>publish</td>\n",
       "      <td>MapR Integrates the Complete Apache Spark Stack</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Tathagata Das]</td>\n",
       "      <td>[Apache Spark, Engineering Blog, Machine Learn...</td>\n",
       "      <td>We are happy to announce the availability of &lt;...</td>\n",
       "      <td>tdas</td>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>35</td>\n",
       "      <td>https://databricks.com/blog/2014/04/09/spark-0...</td>\n",
       "      <td>spark-0_9_1-released</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark 0.9.1 Released</td>\n",
       "      <td>2014-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Steven Hillion]</td>\n",
       "      <td>[Company Blog, Partners]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;This post is guest auth...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-04-01, 2014-04-01, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>37</td>\n",
       "      <td>https://databricks.com/blog/2014/03/31/applica...</td>\n",
       "      <td>application-spotlight-alpine</td>\n",
       "      <td>publish</td>\n",
       "      <td>Application Spotlight: Alpine Data Labs</td>\n",
       "      <td>2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Michael Armbrust, Reynold Xin]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>Building a unified platform for big data analy...</td>\n",
       "      <td>michael</td>\n",
       "      <td>(2014-03-27, 2014-03-27, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>https://databricks.com/blog/2014/03/26/spark-s...</td>\n",
       "      <td>spark-sql-manipulating-structured-data-using-s...</td>\n",
       "      <td>publish</td>\n",
       "      <td>Spark SQL: Manipulating Structured Data Using ...</td>\n",
       "      <td>2014-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Patrick Wendell]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>Our goal with Apache Spark is very simple: pro...</td>\n",
       "      <td>patrick</td>\n",
       "      <td>(2014-02-04, 2014-02-04, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>58</td>\n",
       "      <td>https://databricks.com/blog/2014/02/03/release...</td>\n",
       "      <td>release-0_9_0</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark 0.9.0 Released</td>\n",
       "      <td>2014-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Ali Ghodsi, Ahir Reddy]</td>\n",
       "      <td>[Apache Spark, Ecosystem, Engineering Blog]</td>\n",
       "      <td>Apache Hadoop integration has always been a ke...</td>\n",
       "      <td>ali</td>\n",
       "      <td>(2014-01-02, 2014-01-02, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>https://databricks.com/blog/2014/01/01/simr.html</td>\n",
       "      <td>simr</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark In MapReduce (SIMR)</td>\n",
       "      <td>2014-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Russell Cardullo (Data Infrastructure Enginee...</td>\n",
       "      <td>[Company Blog, Customers]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;We're very happy to see...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-03-26, 2014-03-26, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2409</td>\n",
       "      <td>https://databricks.com/blog/2014/03/25/shareth...</td>\n",
       "      <td>sharethrough-and-spark-streaming</td>\n",
       "      <td>publish</td>\n",
       "      <td>Sharethrough Uses Apache Spark Streaming to Op...</td>\n",
       "      <td>2014-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Jai Ranganathan, Matei Zaharia]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;\\n\\nThis article was cr...</td>\n",
       "      <td>matei</td>\n",
       "      <td>(2014-03-21, 2014-03-21, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2410</td>\n",
       "      <td>https://databricks.com/blog/2014/03/20/apache-...</td>\n",
       "      <td>apache-spark-a-delight-for-developers</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark: A Delight for Developers</td>\n",
       "      <td>2014-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Databricks Press Office]</td>\n",
       "      <td>[Announcements, Company Blog]</td>\n",
       "      <td>&lt;strong&gt;BERKELEY, Calif. – March 18, 2014 –&lt;/s...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-03-19, 2014-03-19, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2411</td>\n",
       "      <td>https://databricks.com/blog/2014/03/18/spark-c...</td>\n",
       "      <td>spark-certification</td>\n",
       "      <td>publish</td>\n",
       "      <td>Databricks announces \"Certified on Apache Spar...</td>\n",
       "      <td>2014-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Ion Stoica]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>&lt;div class=\"blogContent\"&gt;\\n\\nWe are delighted ...</td>\n",
       "      <td>ion</td>\n",
       "      <td>(2014-03-03, 2014-03-03, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2412</td>\n",
       "      <td>https://databricks.com/blog/2014/03/02/spark-a...</td>\n",
       "      <td>spark-apache-top-level-project</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark Now a Top-level Apache Project</td>\n",
       "      <td>2014-03-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  [Tomer Shiran (VP of Product Management at MapR)]   \n",
       "1                                    [Tathagata Das]   \n",
       "2                                   [Steven Hillion]   \n",
       "3                    [Michael Armbrust, Reynold Xin]   \n",
       "4                                  [Patrick Wendell]   \n",
       "5                           [Ali Ghodsi, Ahir Reddy]   \n",
       "6  [Russell Cardullo (Data Infrastructure Enginee...   \n",
       "7                   [Jai Ranganathan, Matei Zaharia]   \n",
       "8                          [Databricks Press Office]   \n",
       "9                                       [Ion Stoica]   \n",
       "\n",
       "                                          categories  \\\n",
       "0                           [Company Blog, Partners]   \n",
       "1  [Apache Spark, Engineering Blog, Machine Learn...   \n",
       "2                           [Company Blog, Partners]   \n",
       "3                   [Apache Spark, Engineering Blog]   \n",
       "4                   [Apache Spark, Engineering Blog]   \n",
       "5        [Apache Spark, Ecosystem, Engineering Blog]   \n",
       "6                          [Company Blog, Customers]   \n",
       "7                   [Apache Spark, Engineering Blog]   \n",
       "8                      [Announcements, Company Blog]   \n",
       "9                   [Apache Spark, Engineering Blog]   \n",
       "\n",
       "                                             content  creator  \\\n",
       "0  <div class=\"post-meta\">This post is guest auth...      roy   \n",
       "1  We are happy to announce the availability of <...     tdas   \n",
       "2  <div class=\"post-meta\">This post is guest auth...      roy   \n",
       "3  Building a unified platform for big data analy...  michael   \n",
       "4  Our goal with Apache Spark is very simple: pro...  patrick   \n",
       "5  Apache Hadoop integration has always been a ke...      ali   \n",
       "6  <div class=\"post-meta\">We're very happy to see...      roy   \n",
       "7  <div class=\"post-meta\">\\n\\nThis article was cr...    matei   \n",
       "8  <strong>BERKELEY, Calif. – March 18, 2014 –</s...      roy   \n",
       "9  <div class=\"blogContent\">\\n\\nWe are delighted ...      ion   \n",
       "\n",
       "                           dates description    id  \\\n",
       "0  (2014-04-10, 2014-04-10, UTC)        None    33   \n",
       "1  (2014-04-10, 2014-04-10, UTC)        None    35   \n",
       "2  (2014-04-01, 2014-04-01, UTC)        None    37   \n",
       "3  (2014-03-27, 2014-03-27, UTC)        None    42   \n",
       "4  (2014-02-04, 2014-02-04, UTC)        None    58   \n",
       "5  (2014-01-02, 2014-01-02, UTC)        None    65   \n",
       "6  (2014-03-26, 2014-03-26, UTC)        None  2409   \n",
       "7  (2014-03-21, 2014-03-21, UTC)        None  2410   \n",
       "8  (2014-03-19, 2014-03-19, UTC)        None  2411   \n",
       "9  (2014-03-03, 2014-03-03, UTC)        None  2412   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://databricks.com/blog/2014/04/10/mapr-in...   \n",
       "1  https://databricks.com/blog/2014/04/09/spark-0...   \n",
       "2  https://databricks.com/blog/2014/03/31/applica...   \n",
       "3  https://databricks.com/blog/2014/03/26/spark-s...   \n",
       "4  https://databricks.com/blog/2014/02/03/release...   \n",
       "5   https://databricks.com/blog/2014/01/01/simr.html   \n",
       "6  https://databricks.com/blog/2014/03/25/shareth...   \n",
       "7  https://databricks.com/blog/2014/03/20/apache-...   \n",
       "8  https://databricks.com/blog/2014/03/18/spark-c...   \n",
       "9  https://databricks.com/blog/2014/03/02/spark-a...   \n",
       "\n",
       "                                                slug   status  \\\n",
       "0                        mapr-integrates-spark-stack  publish   \n",
       "1                               spark-0_9_1-released  publish   \n",
       "2                       application-spotlight-alpine  publish   \n",
       "3  spark-sql-manipulating-structured-data-using-s...  publish   \n",
       "4                                      release-0_9_0  publish   \n",
       "5                                               simr  publish   \n",
       "6                   sharethrough-and-spark-streaming  publish   \n",
       "7              apache-spark-a-delight-for-developers  publish   \n",
       "8                                spark-certification  publish   \n",
       "9                     spark-apache-top-level-project  publish   \n",
       "\n",
       "                                               title publishedOn  \n",
       "0    MapR Integrates the Complete Apache Spark Stack  2014-04-10  \n",
       "1                        Apache Spark 0.9.1 Released  2014-04-10  \n",
       "2            Application Spotlight: Alpine Data Labs  2014-04-01  \n",
       "3  Spark SQL: Manipulating Structured Data Using ...  2014-03-27  \n",
       "4                        Apache Spark 0.9.0 Released  2014-02-04  \n",
       "5                   Apache Spark In MapReduce (SIMR)  2014-01-02  \n",
       "6  Sharethrough Uses Apache Spark Streaming to Op...  2014-03-26  \n",
       "7             Apache Spark: A Delight for Developers  2014-03-21  \n",
       "8  Databricks announces \"Certified on Apache Spar...  2014-03-19  \n",
       "9        Apache Spark Now a Top-level Apache Project  2014-03-03  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog2DF = blogDF.withColumn(\"publishedOn\", date_format(\"dates.publishedOn\",\"yyyy-MM-dd\")) \n",
    "display(blog2DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this temporary view, apply the `printSchema` method to check its schema and confirm the timestamp conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- dates: struct (nullable = true)\n",
      " |    |-- createdOn: string (nullable = true)\n",
      " |    |-- publishedOn: string (nullable = true)\n",
      " |    |-- tz: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- slug: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedOn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blog2DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dates are represented by a `timestamp` data type, we need to convert to a data type that allows `<` and `>`-type comparison operations in order to query for articles within certain date ranges (such as a list of all articles published in 2013). This is accopmplished by using the `to_date` function in Scala or Python.\n",
    "\n",
    "See the Spark documentation on <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>, for a long list of date-specific functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Databricks and the Apache Spark Platform</td>\n",
       "      <td>None</td>\n",
       "      <td>https://databricks.com/blog/2013/10/27/databri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Growing Apache Spark Community</td>\n",
       "      <td>None</td>\n",
       "      <td>https://databricks.com/blog/2013/10/27/the-gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Databricks and Cloudera Partner to Support Apa...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://databricks.com/blog/2013/10/28/databri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Putting Apache Spark to Use: Fast In-Memory Co...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://databricks.com/blog/2013/11/21/putting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Highlights From Spark Summit 2013</td>\n",
       "      <td>None</td>\n",
       "      <td>https://databricks.com/blog/2013/12/18/spark-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apache Spark 0.8.1 Released</td>\n",
       "      <td>None</td>\n",
       "      <td>https://databricks.com/blog/2013/12/19/release...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  date  \\\n",
       "0           Databricks and the Apache Spark Platform  None   \n",
       "1                 The Growing Apache Spark Community  None   \n",
       "2  Databricks and Cloudera Partner to Support Apa...  None   \n",
       "3  Putting Apache Spark to Use: Fast In-Memory Co...  None   \n",
       "4                  Highlights From Spark Summit 2013  None   \n",
       "5                        Apache Spark 0.8.1 Released  None   \n",
       "\n",
       "                                                link  \n",
       "0  https://databricks.com/blog/2013/10/27/databri...  \n",
       "1  https://databricks.com/blog/2013/10/27/the-gro...  \n",
       "2  https://databricks.com/blog/2013/10/28/databri...  \n",
       "3  https://databricks.com/blog/2013/11/21/putting...  \n",
       "4  https://databricks.com/blog/2013/12/18/spark-s...  \n",
       "5  https://databricks.com/blog/2013/12/19/release...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year, col\n",
    "          \n",
    "resultDF = (blog2DF.select(\"title\", to_date(col(\"publishedOn\"),\"MMM dd, yyyy\").alias('date'),\"link\")\n",
    "            .filter(year(col(\"publishedOn\")) == '2013')\n",
    "            .orderBy(col(\"publishedOn\")))\n",
    "\n",
    "display(resultDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Data\n",
    "\n",
    "The DataFrame also contains array columns. \n",
    "\n",
    "Easily determine the size of each array using the built-in `size(..)` function with array columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size(authors)</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Tomer Shiran (VP of Product Management at MapR)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Tathagata Das]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Steven Hillion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[Michael Armbrust, Reynold Xin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[Patrick Wendell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[Ali Ghodsi, Ahir Reddy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[Russell Cardullo (Data Infrastructure Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[Jai Ranganathan, Matei Zaharia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[Databricks Press Office]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ion Stoica]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size(authors)                                            authors\n",
       "0              1  [Tomer Shiran (VP of Product Management at MapR)]\n",
       "1              1                                    [Tathagata Das]\n",
       "2              1                                   [Steven Hillion]\n",
       "3              2                    [Michael Armbrust, Reynold Xin]\n",
       "4              1                                  [Patrick Wendell]\n",
       "5              2                           [Ali Ghodsi, Ahir Reddy]\n",
       "6              2  [Russell Cardullo (Data Infrastructure Enginee...\n",
       "7              2                   [Jai Ranganathan, Matei Zaharia]\n",
       "8              1                          [Databricks Press Office]\n",
       "9              1                                       [Ion Stoica]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "display(blogDF.select(size(\"authors\"),\"authors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the first element from the array `authors` using an array subscript operator.\n",
    "\n",
    "For example, in Scala, the 0th element of array `authors` is `authors(0)`\n",
    "whereas, in Python, the 0th element of `authors` is `authors[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primaryAuthor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tomer Shiran (VP of Product Management at MapR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tathagata Das</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven Hillion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Armbrust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patrick Wendell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ali Ghodsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russell Cardullo (Data Infrastructure Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jai Ranganathan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Databricks Press Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ion Stoica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       primaryAuthor\n",
       "0    Tomer Shiran (VP of Product Management at MapR)\n",
       "1                                      Tathagata Das\n",
       "2                                     Steven Hillion\n",
       "3                                   Michael Armbrust\n",
       "4                                    Patrick Wendell\n",
       "5                                         Ali Ghodsi\n",
       "6  Russell Cardullo (Data Infrastructure Engineer...\n",
       "7                                    Jai Ranganathan\n",
       "8                            Databricks Press Office\n",
       "9                                         Ion Stoica"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(blogDF.select(col(\"authors\")[0].alias(\"primaryAuthor\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode\n",
    "\n",
    "The `explode` method allows you to split an array column into multiple rows, copying all the other columns into each new row. \n",
    "\n",
    "For example, split the column `authors` into the column `author`, with one author per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MapR Integrates the Complete Apache Spark Stack</td>\n",
       "      <td>[Tomer Shiran (VP of Product Management at MapR)]</td>\n",
       "      <td>Tomer Shiran (VP of Product Management at MapR)</td>\n",
       "      <td>https://databricks.com/blog/2014/04/10/mapr-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apache Spark 0.9.1 Released</td>\n",
       "      <td>[Tathagata Das]</td>\n",
       "      <td>Tathagata Das</td>\n",
       "      <td>https://databricks.com/blog/2014/04/09/spark-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Application Spotlight: Alpine Data Labs</td>\n",
       "      <td>[Steven Hillion]</td>\n",
       "      <td>Steven Hillion</td>\n",
       "      <td>https://databricks.com/blog/2014/03/31/applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spark SQL: Manipulating Structured Data Using ...</td>\n",
       "      <td>[Michael Armbrust, Reynold Xin]</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>https://databricks.com/blog/2014/03/26/spark-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spark SQL: Manipulating Structured Data Using ...</td>\n",
       "      <td>[Michael Armbrust, Reynold Xin]</td>\n",
       "      <td>Reynold Xin</td>\n",
       "      <td>https://databricks.com/blog/2014/03/26/spark-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apache Spark 0.9.0 Released</td>\n",
       "      <td>[Patrick Wendell]</td>\n",
       "      <td>Patrick Wendell</td>\n",
       "      <td>https://databricks.com/blog/2014/02/03/release...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apache Spark In MapReduce (SIMR)</td>\n",
       "      <td>[Ali Ghodsi, Ahir Reddy]</td>\n",
       "      <td>Ali Ghodsi</td>\n",
       "      <td>https://databricks.com/blog/2014/01/01/simr.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apache Spark In MapReduce (SIMR)</td>\n",
       "      <td>[Ali Ghodsi, Ahir Reddy]</td>\n",
       "      <td>Ahir Reddy</td>\n",
       "      <td>https://databricks.com/blog/2014/01/01/simr.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sharethrough Uses Apache Spark Streaming to Op...</td>\n",
       "      <td>[Russell Cardullo (Data Infrastructure Enginee...</td>\n",
       "      <td>Russell Cardullo (Data Infrastructure Engineer...</td>\n",
       "      <td>https://databricks.com/blog/2014/03/25/shareth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sharethrough Uses Apache Spark Streaming to Op...</td>\n",
       "      <td>[Russell Cardullo (Data Infrastructure Enginee...</td>\n",
       "      <td>Michael Ruggiero (Data Infrastructure Engineer...</td>\n",
       "      <td>https://databricks.com/blog/2014/03/25/shareth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    MapR Integrates the Complete Apache Spark Stack   \n",
       "1                        Apache Spark 0.9.1 Released   \n",
       "2            Application Spotlight: Alpine Data Labs   \n",
       "3  Spark SQL: Manipulating Structured Data Using ...   \n",
       "4  Spark SQL: Manipulating Structured Data Using ...   \n",
       "5                        Apache Spark 0.9.0 Released   \n",
       "6                   Apache Spark In MapReduce (SIMR)   \n",
       "7                   Apache Spark In MapReduce (SIMR)   \n",
       "8  Sharethrough Uses Apache Spark Streaming to Op...   \n",
       "9  Sharethrough Uses Apache Spark Streaming to Op...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Tomer Shiran (VP of Product Management at MapR)]   \n",
       "1                                    [Tathagata Das]   \n",
       "2                                   [Steven Hillion]   \n",
       "3                    [Michael Armbrust, Reynold Xin]   \n",
       "4                    [Michael Armbrust, Reynold Xin]   \n",
       "5                                  [Patrick Wendell]   \n",
       "6                           [Ali Ghodsi, Ahir Reddy]   \n",
       "7                           [Ali Ghodsi, Ahir Reddy]   \n",
       "8  [Russell Cardullo (Data Infrastructure Enginee...   \n",
       "9  [Russell Cardullo (Data Infrastructure Enginee...   \n",
       "\n",
       "                                              author  \\\n",
       "0    Tomer Shiran (VP of Product Management at MapR)   \n",
       "1                                      Tathagata Das   \n",
       "2                                     Steven Hillion   \n",
       "3                                   Michael Armbrust   \n",
       "4                                        Reynold Xin   \n",
       "5                                    Patrick Wendell   \n",
       "6                                         Ali Ghodsi   \n",
       "7                                         Ahir Reddy   \n",
       "8  Russell Cardullo (Data Infrastructure Engineer...   \n",
       "9  Michael Ruggiero (Data Infrastructure Engineer...   \n",
       "\n",
       "                                                link  \n",
       "0  https://databricks.com/blog/2014/04/10/mapr-in...  \n",
       "1  https://databricks.com/blog/2014/04/09/spark-0...  \n",
       "2  https://databricks.com/blog/2014/03/31/applica...  \n",
       "3  https://databricks.com/blog/2014/03/26/spark-s...  \n",
       "4  https://databricks.com/blog/2014/03/26/spark-s...  \n",
       "5  https://databricks.com/blog/2014/02/03/release...  \n",
       "6   https://databricks.com/blog/2014/01/01/simr.html  \n",
       "7   https://databricks.com/blog/2014/01/01/simr.html  \n",
       "8  https://databricks.com/blog/2014/03/25/shareth...  \n",
       "9  https://databricks.com/blog/2014/03/25/shareth...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "display(blogDF.select(\"title\",\"authors\",explode(col(\"authors\")).alias(\"author\"), \"link\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more obvious to restrict the output to articles that have multiple authors, and then sort by the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>content</th>\n",
       "      <th>creator</th>\n",
       "      <th>dates</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>slug</th>\n",
       "      <th>status</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Tomer Shiran (VP of Product Management at MapR)]</td>\n",
       "      <td>[Company Blog, Partners]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;This post is guest auth...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>33</td>\n",
       "      <td>https://databricks.com/blog/2014/04/10/mapr-in...</td>\n",
       "      <td>mapr-integrates-spark-stack</td>\n",
       "      <td>publish</td>\n",
       "      <td>MapR Integrates the Complete Apache Spark Stack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Tathagata Das]</td>\n",
       "      <td>[Apache Spark, Engineering Blog, Machine Learn...</td>\n",
       "      <td>We are happy to announce the availability of &lt;...</td>\n",
       "      <td>tdas</td>\n",
       "      <td>(2014-04-10, 2014-04-10, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>35</td>\n",
       "      <td>https://databricks.com/blog/2014/04/09/spark-0...</td>\n",
       "      <td>spark-0_9_1-released</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark 0.9.1 Released</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Steven Hillion]</td>\n",
       "      <td>[Company Blog, Partners]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;This post is guest auth...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-04-01, 2014-04-01, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>37</td>\n",
       "      <td>https://databricks.com/blog/2014/03/31/applica...</td>\n",
       "      <td>application-spotlight-alpine</td>\n",
       "      <td>publish</td>\n",
       "      <td>Application Spotlight: Alpine Data Labs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Michael Armbrust, Reynold Xin]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>Building a unified platform for big data analy...</td>\n",
       "      <td>michael</td>\n",
       "      <td>(2014-03-27, 2014-03-27, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>https://databricks.com/blog/2014/03/26/spark-s...</td>\n",
       "      <td>spark-sql-manipulating-structured-data-using-s...</td>\n",
       "      <td>publish</td>\n",
       "      <td>Spark SQL: Manipulating Structured Data Using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Patrick Wendell]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>Our goal with Apache Spark is very simple: pro...</td>\n",
       "      <td>patrick</td>\n",
       "      <td>(2014-02-04, 2014-02-04, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>58</td>\n",
       "      <td>https://databricks.com/blog/2014/02/03/release...</td>\n",
       "      <td>release-0_9_0</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark 0.9.0 Released</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Ali Ghodsi, Ahir Reddy]</td>\n",
       "      <td>[Apache Spark, Ecosystem, Engineering Blog]</td>\n",
       "      <td>Apache Hadoop integration has always been a ke...</td>\n",
       "      <td>ali</td>\n",
       "      <td>(2014-01-02, 2014-01-02, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>https://databricks.com/blog/2014/01/01/simr.html</td>\n",
       "      <td>simr</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark In MapReduce (SIMR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Russell Cardullo (Data Infrastructure Enginee...</td>\n",
       "      <td>[Company Blog, Customers]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;We're very happy to see...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-03-26, 2014-03-26, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2409</td>\n",
       "      <td>https://databricks.com/blog/2014/03/25/shareth...</td>\n",
       "      <td>sharethrough-and-spark-streaming</td>\n",
       "      <td>publish</td>\n",
       "      <td>Sharethrough Uses Apache Spark Streaming to Op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Jai Ranganathan, Matei Zaharia]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>&lt;div class=\"post-meta\"&gt;\\n\\nThis article was cr...</td>\n",
       "      <td>matei</td>\n",
       "      <td>(2014-03-21, 2014-03-21, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2410</td>\n",
       "      <td>https://databricks.com/blog/2014/03/20/apache-...</td>\n",
       "      <td>apache-spark-a-delight-for-developers</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark: A Delight for Developers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Databricks Press Office]</td>\n",
       "      <td>[Announcements, Company Blog]</td>\n",
       "      <td>&lt;strong&gt;BERKELEY, Calif. – March 18, 2014 –&lt;/s...</td>\n",
       "      <td>roy</td>\n",
       "      <td>(2014-03-19, 2014-03-19, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2411</td>\n",
       "      <td>https://databricks.com/blog/2014/03/18/spark-c...</td>\n",
       "      <td>spark-certification</td>\n",
       "      <td>publish</td>\n",
       "      <td>Databricks announces \"Certified on Apache Spar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Ion Stoica]</td>\n",
       "      <td>[Apache Spark, Engineering Blog]</td>\n",
       "      <td>&lt;div class=\"blogContent\"&gt;\\n\\nWe are delighted ...</td>\n",
       "      <td>ion</td>\n",
       "      <td>(2014-03-03, 2014-03-03, UTC)</td>\n",
       "      <td>None</td>\n",
       "      <td>2412</td>\n",
       "      <td>https://databricks.com/blog/2014/03/02/spark-a...</td>\n",
       "      <td>spark-apache-top-level-project</td>\n",
       "      <td>publish</td>\n",
       "      <td>Apache Spark Now a Top-level Apache Project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  [Tomer Shiran (VP of Product Management at MapR)]   \n",
       "1                                    [Tathagata Das]   \n",
       "2                                   [Steven Hillion]   \n",
       "3                    [Michael Armbrust, Reynold Xin]   \n",
       "4                                  [Patrick Wendell]   \n",
       "5                           [Ali Ghodsi, Ahir Reddy]   \n",
       "6  [Russell Cardullo (Data Infrastructure Enginee...   \n",
       "7                   [Jai Ranganathan, Matei Zaharia]   \n",
       "8                          [Databricks Press Office]   \n",
       "9                                       [Ion Stoica]   \n",
       "\n",
       "                                          categories  \\\n",
       "0                           [Company Blog, Partners]   \n",
       "1  [Apache Spark, Engineering Blog, Machine Learn...   \n",
       "2                           [Company Blog, Partners]   \n",
       "3                   [Apache Spark, Engineering Blog]   \n",
       "4                   [Apache Spark, Engineering Blog]   \n",
       "5        [Apache Spark, Ecosystem, Engineering Blog]   \n",
       "6                          [Company Blog, Customers]   \n",
       "7                   [Apache Spark, Engineering Blog]   \n",
       "8                      [Announcements, Company Blog]   \n",
       "9                   [Apache Spark, Engineering Blog]   \n",
       "\n",
       "                                             content  creator  \\\n",
       "0  <div class=\"post-meta\">This post is guest auth...      roy   \n",
       "1  We are happy to announce the availability of <...     tdas   \n",
       "2  <div class=\"post-meta\">This post is guest auth...      roy   \n",
       "3  Building a unified platform for big data analy...  michael   \n",
       "4  Our goal with Apache Spark is very simple: pro...  patrick   \n",
       "5  Apache Hadoop integration has always been a ke...      ali   \n",
       "6  <div class=\"post-meta\">We're very happy to see...      roy   \n",
       "7  <div class=\"post-meta\">\\n\\nThis article was cr...    matei   \n",
       "8  <strong>BERKELEY, Calif. – March 18, 2014 –</s...      roy   \n",
       "9  <div class=\"blogContent\">\\n\\nWe are delighted ...      ion   \n",
       "\n",
       "                           dates description    id  \\\n",
       "0  (2014-04-10, 2014-04-10, UTC)        None    33   \n",
       "1  (2014-04-10, 2014-04-10, UTC)        None    35   \n",
       "2  (2014-04-01, 2014-04-01, UTC)        None    37   \n",
       "3  (2014-03-27, 2014-03-27, UTC)        None    42   \n",
       "4  (2014-02-04, 2014-02-04, UTC)        None    58   \n",
       "5  (2014-01-02, 2014-01-02, UTC)        None    65   \n",
       "6  (2014-03-26, 2014-03-26, UTC)        None  2409   \n",
       "7  (2014-03-21, 2014-03-21, UTC)        None  2410   \n",
       "8  (2014-03-19, 2014-03-19, UTC)        None  2411   \n",
       "9  (2014-03-03, 2014-03-03, UTC)        None  2412   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://databricks.com/blog/2014/04/10/mapr-in...   \n",
       "1  https://databricks.com/blog/2014/04/09/spark-0...   \n",
       "2  https://databricks.com/blog/2014/03/31/applica...   \n",
       "3  https://databricks.com/blog/2014/03/26/spark-s...   \n",
       "4  https://databricks.com/blog/2014/02/03/release...   \n",
       "5   https://databricks.com/blog/2014/01/01/simr.html   \n",
       "6  https://databricks.com/blog/2014/03/25/shareth...   \n",
       "7  https://databricks.com/blog/2014/03/20/apache-...   \n",
       "8  https://databricks.com/blog/2014/03/18/spark-c...   \n",
       "9  https://databricks.com/blog/2014/03/02/spark-a...   \n",
       "\n",
       "                                                slug   status  \\\n",
       "0                        mapr-integrates-spark-stack  publish   \n",
       "1                               spark-0_9_1-released  publish   \n",
       "2                       application-spotlight-alpine  publish   \n",
       "3  spark-sql-manipulating-structured-data-using-s...  publish   \n",
       "4                                      release-0_9_0  publish   \n",
       "5                                               simr  publish   \n",
       "6                   sharethrough-and-spark-streaming  publish   \n",
       "7              apache-spark-a-delight-for-developers  publish   \n",
       "8                                spark-certification  publish   \n",
       "9                     spark-apache-top-level-project  publish   \n",
       "\n",
       "                                               title  \n",
       "0    MapR Integrates the Complete Apache Spark Stack  \n",
       "1                        Apache Spark 0.9.1 Released  \n",
       "2            Application Spotlight: Alpine Data Labs  \n",
       "3  Spark SQL: Manipulating Structured Data Using ...  \n",
       "4                        Apache Spark 0.9.0 Released  \n",
       "5                   Apache Spark In MapReduce (SIMR)  \n",
       "6  Sharethrough Uses Apache Spark Streaming to Op...  \n",
       "7             Apache Spark: A Delight for Developers  \n",
       "8  Databricks announces \"Certified on Apache Spar...  \n",
       "9        Apache Spark Now a Top-level Apache Project  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog2DF = (blogDF \n",
    "  .select(\"title\",\"authors\",explode(col(\"authors\")).alias(\"author\"), \"link\") \n",
    "  .filter(size(col(\"authors\")) > 1) \n",
    "  .orderBy(\"title\")\n",
    ")\n",
    "\n",
    "display(blogDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Identify all the articles written or co-written by Michael Armbrust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with the `blogDF` DataFrame, create a DataFrame called `articlesByMichaelDF` where:\n",
    "0. Michael Armbrust is the author.\n",
    "0. The data set contains the column `title` (it may contain others).\n",
    "0. It contains only one record per article.\n",
    "\n",
    "**Hint:** See the Spark documentation on <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>.  \n",
    "\n",
    "**Hint:** Include the column `authors` in your view to help you debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "articlesByMichaelDF = (blogDF.select(\"title\", explode(col(\"authors\")).alias(\"author\"))\n",
    "                       .filter(\"author == 'Michael Armbrust'\")\n",
    "                       .drop(\"author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "resultsCount = articlesByMichaelDF.count()\n",
    "dfTest(\"DF-L5-articlesByMichael-count\", 3, resultsCount)  \n",
    "\n",
    "results = articlesByMichaelDF.collect()\n",
    "\n",
    "dfTest(\"DF-L5-articlesByMichael-0\", Row(title=u'Spark SQL: Manipulating Structured Data Using Apache Spark'), results[0])\n",
    "dfTest(\"DF-L5-articlesByMichael-1\", Row(title=u'Exciting Performance Improvements on the Horizon for Spark SQL'), results[1])\n",
    "dfTest(\"DF-L5-articlesByMichael-2\", Row(title=u'Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform'), results[2])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Show the list of Michael Armbrust's articles in HTML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spark SQL: Manipulating Structured Data Using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exciting Performance Improvements on the Horiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spark SQL Data Sources API: Unified Data Acces...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  Spark SQL: Manipulating Structured Data Using ...\n",
       "1  Exciting Performance Improvements on the Horiz...\n",
       "2  Spark SQL Data Sources API: Unified Data Acces..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "display(articlesByMichaelDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Identify the complete set of categories used in the blog articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with the `blogDF` DataFrame, create another DataFrame called `uniqueCategoriesDF` where:\n",
    "0. The data set contains the one column `category` (and no others).\n",
    "0. This list of categories should be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "uniqueCategoriesDF = (blogDF.select(explode(col(\"categories\")).alias(\"category\")).distinct().orderBy(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "resultsCount =  uniqueCategoriesDF.count()\n",
    "\n",
    "dfTest(\"DF-L5-uniqueCategories-count\", 12, resultsCount)\n",
    "\n",
    "results = uniqueCategoriesDF.collect()\n",
    "\n",
    "dfTest(\"DF-L5-uniqueCategories-0\", Row(category=u'Announcements'), results[0])\n",
    "dfTest(\"DF-L5-uniqueCategories-1\", Row(category=u'Apache Spark'), results[1])\n",
    "dfTest(\"DF-L5-uniqueCategories-2\", Row(category=u'Company Blog'), results[2])\n",
    "\n",
    "dfTest(\"DF-L5-uniqueCategories-9\", Row(category=u'Platform'), results[9])\n",
    "dfTest(\"DF-L5-uniqueCategories-10\", Row(category=u'Product'), results[10])\n",
    "dfTest(\"DF-L5-uniqueCategories-11\", Row(category=u'Streaming'), results[11])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Show the complete list of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Announcements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apache Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Company Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ecosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Engineering Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Partners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Platform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category\n",
       "0     Announcements\n",
       "1      Apache Spark\n",
       "2      Company Blog\n",
       "3         Customers\n",
       "4         Ecosystem\n",
       "5  Engineering Blog\n",
       "6            Events\n",
       "7  Machine Learning\n",
       "8          Partners\n",
       "9          Platform"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "display(uniqueCategoriesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Count how many times each category is referenced in the blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Starting with the `blogDF` DataFrame, create another DataFrame called `totalArticlesByCategoryDF` where:\n",
    "0. The new DataFrame contains two columns, `category` and `total`.\n",
    "0. The `category` column is a single, distinct category (similar to the last exercise).\n",
    "0. The `total` column is the total number of articles in that category.\n",
    "0. Order by `category`.\n",
    "\n",
    "Because articles can be tagged with multiple categories, the sum of the totals adds up to more than the total number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.sql.functions import count\n",
    "totalArticlesByCategoryDF = (blogDF.select(explode(col(\"categories\")).alias(\"category\"))\n",
    "                             .groupBy(\"category\")\n",
    "                             .agg(count(\"*\").alias(\"total\"))\n",
    "                             .orderBy(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "results = totalArticlesByCategoryDF.count()\n",
    "\n",
    "dfTest(\"DF-L5-articlesByCategory-count\", 12, results)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "\n",
    "results = totalArticlesByCategoryDF.collect()\n",
    "\n",
    "dfTest(\"DF-L5-articlesByCategory-0\", Row(category=u'Announcements', total=72), results[0])\n",
    "dfTest(\"DF-L5-articlesByCategory-1\", Row(category=u'Apache Spark', total=132), results[1])\n",
    "dfTest(\"DF-L5-articlesByCategory-2\", Row(category=u'Company Blog', total=224), results[2])\n",
    "\n",
    "dfTest(\"DF-L5-articlesByCategory-9\", Row(category=u'Platform', total=4), results[9])\n",
    "dfTest(\"DF-L5-articlesByCategory-10\", Row(category=u'Product', total=83), results[10])\n",
    "dfTest(\"DF-L5-articlesByCategory-11\", Row(category=u'Streaming', total=21), results[11])\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Display the totals of each category in html format (should be ordered by `category`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Announcements</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apache Spark</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Company Blog</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Customers</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ecosystem</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Engineering Blog</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Events</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Partners</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Platform</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category  total\n",
       "0     Announcements     72\n",
       "1      Apache Spark    132\n",
       "2      Company Blog    224\n",
       "3         Customers     34\n",
       "4         Ecosystem     21\n",
       "5  Engineering Blog    141\n",
       "6            Events     52\n",
       "7  Machine Learning     38\n",
       "8          Partners     50\n",
       "9          Platform      4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "display(totalArticlesByCategoryDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Spark DataFrames allows you to query and manipulate structured and semi-structured data.\n",
    "* Spark DataFrames built-in functions provide powerful primitives for querying complex schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "**Q:** What is the syntax for accessing nested columns?  \n",
    "**A:** Use the dot notation:\n",
    "`select(\"dates.publishedOn\")`\n",
    "\n",
    "**Q:** What is the syntax for accessing the first element in an array?  \n",
    "**A:** Use the [subscript] notation: \n",
    "`select(\"col(authors)[0]\")`\n",
    "\n",
    "**Q:** What is the syntax for expanding an array into multiple rows?  \n",
    "**A:** Use the explode method:  `select(explode(col(\"authors\")).alias(\"Author\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Topics & Resources\n",
    "\n",
    "* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 [Intellinum Analytics, Inc](http://www.intellinum.co). All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_py36]",
   "language": "python",
   "name": "conda-env-pyspark24_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
